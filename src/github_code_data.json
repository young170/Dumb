[
    {
        "file_name": "sort.py",
        "language": "python",
        "source_code": "#!/usr/bin/env python\n# coding: utf-8\n\n\"\"\"\n    The approach taken is explained below. I decided to do it simply.\n    Initially I was considering parsing the data into some sort of\n    structure and then generating an appropriate README. I am still\n    considering doing it - but for now this should work. The only issue\n    I see is that it only sorts the entries at the lowest level, and that\n    the order of the top-level contents do not match the order of the actual\n    entries.\n\n    This could be extended by having nested blocks, sorting them recursively\n    and flattening the end structure into a list of lines. Revision 2 maybe ^.^.\n\"\"\"\n\ndef sort_blocks():\n    # First, we load the current README into memory\n    with open('README.md', 'r') as read_me_file:\n        read_me = read_me_file.read()\n\n    # Separating the 'table of contents' from the contents (blocks)\n    table_of_contents = ''.join(read_me.split('- - -')[0])\n    blocks = ''.join(read_me.split('- - -')[1]).split('\\n# ')\n    for i in range(len(blocks)):\n        if i == 0:\n            blocks[i] = blocks[i] + '\\n'\n        else:\n            blocks[i] = '# ' + blocks[i] + '\\n'\n\n    # Sorting the libraries\n    inner_blocks = sorted(blocks[0].split('##'))\n    for i in range(1, len(inner_blocks)):\n        if inner_blocks[i][0] != '#':\n            inner_blocks[i] = '##' + inner_blocks[i]\n    inner_blocks = ''.join(inner_blocks)\n\n    # Replacing the non-sorted libraries by the sorted ones and gathering all at the final_README file\n    blocks[0] = inner_blocks\n    final_README = table_of_contents + '- - -' + ''.join(blocks)\n\n    with open('README.md', 'w+') as sorted_file:\n        sorted_file.write(final_README)\n\ndef main():\n    # First, we load the current README into memory as an array of lines\n    with open('README.md', 'r') as read_me_file:\n        read_me = read_me_file.readlines()\n\n    # Then we cluster the lines together as blocks\n    # Each block represents a collection of lines that should be sorted\n    # This was done by assuming only links ([...](...)) are meant to be sorted\n    # Clustering is done by indentation\n    blocks = []\n    last_indent = None\n    for line in read_me:\n        s_line = line.lstrip()\n        indent = len(line) - len(s_line)\n\n        if any([s_line.startswith(s) for s in ['* [', '- [']]):\n            if indent == last_indent:\n                blocks[-1].append(line)\n            else:\n                blocks.append([line])\n            last_indent = indent\n        else:\n            blocks.append([line])\n            last_indent = None\n\n    with open('README.md', 'w+') as sorted_file:\n        # Then all of the blocks are sorted individually\n        blocks = [\n            ''.join(sorted(block, key=str.lower)) for block in blocks\n        ]\n        # And the result is written back to README.md\n        sorted_file.write(''.join(blocks))\n\n    # Then we call the sorting method\n    sort_blocks()\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "imports": [
            "the"
        ],
        "functions": [
            "sort_blocks",
            "main"
        ],
        "variables": [
            "final_README",
            "blocks",
            "last_indent",
            "inner_blocks",
            "indent",
            "__name__",
            "s_line",
            "key",
            "read_me",
            "table_of_contents",
            "i"
        ]
    },
    {
        "file_name": "launch.py",
        "language": "python",
        "source_code": "from modules import launch_utils\r\n\r\nargs = launch_utils.args\r\npython = launch_utils.python\r\ngit = launch_utils.git\r\nindex_url = launch_utils.index_url\r\ndir_repos = launch_utils.dir_repos\r\n\r\ncommit_hash = launch_utils.commit_hash\r\ngit_tag = launch_utils.git_tag\r\n\r\nrun = launch_utils.run\r\nis_installed = launch_utils.is_installed\r\nrepo_dir = launch_utils.repo_dir\r\n\r\nrun_pip = launch_utils.run_pip\r\ncheck_run_python = launch_utils.check_run_python\r\ngit_clone = launch_utils.git_clone\r\ngit_pull_recursive = launch_utils.git_pull_recursive\r\nlist_extensions = launch_utils.list_extensions\r\nrun_extension_installer = launch_utils.run_extension_installer\r\nprepare_environment = launch_utils.prepare_environment\r\nconfigure_for_tests = launch_utils.configure_for_tests\r\nstart = launch_utils.start\r\n\r\n\r\ndef main():\r\n    if args.dump_sysinfo:\r\n        filename = launch_utils.dump_sysinfo()\r\n\r\n        print(f\"Sysinfo saved as {filename}. Exiting...\")\r\n\r\n        exit(0)\r\n\r\n    launch_utils.startup_timer.record(\"initial startup\")\r\n\r\n    with launch_utils.startup_timer.subcategory(\"prepare environment\"):\r\n        if not args.skip_prepare_environment:\r\n            prepare_environment()\r\n\r\n    if args.test_server:\r\n        configure_for_tests()\r\n\r\n    start()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
        "imports": [
            "modules",
            "launch_utils"
        ],
        "functions": [
            "main"
        ],
        "variables": [
            "commit_hash",
            "dir_repos",
            "git_clone",
            "run_extension_installer",
            "start",
            "git_tag",
            "repo_dir",
            "git",
            "filename",
            "is_installed",
            "prepare_environment",
            "python",
            "__name__",
            "configure_for_tests",
            "list_extensions",
            "git_pull_recursive",
            "index_url",
            "args",
            "run_pip",
            "run",
            "check_run_python"
        ]
    },
    {
        "file_name": "conftest.py",
        "language": "python",
        "source_code": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# tests directory-specific settings - this file is run automatically\n# by pytest before any tests are run\n\nimport doctest\nimport sys\nimport warnings\nfrom os.path import abspath, dirname, join\n\nimport _pytest\nimport pytest\n\nfrom transformers.testing_utils import HfDoctestModule, HfDocTestParser\n\n\nNOT_DEVICE_TESTS = {\n    \"test_tokenization\",\n    \"test_processor\",\n    \"test_processing\",\n    \"test_beam_constraints\",\n    \"test_configuration_utils\",\n    \"test_data_collator\",\n    \"test_trainer_callback\",\n    \"test_trainer_utils\",\n    \"test_feature_extraction\",\n    \"test_image_processing\",\n    \"test_image_processor\",\n    \"test_image_transforms\",\n    \"test_optimization\",\n    \"test_retrieval\",\n    \"test_config\",\n    \"test_from_pretrained_no_checkpoint\",\n    \"test_keep_in_fp32_modules\",\n    \"test_gradient_checkpointing_backward_compatibility\",\n    \"test_gradient_checkpointing_enable_disable\",\n    \"test_save_load_fast_init_from_base\",\n    \"test_fast_init_context_manager\",\n    \"test_fast_init_tied_embeddings\",\n    \"test_save_load_fast_init_to_base\",\n    \"test_torch_save_load\",\n    \"test_initialization\",\n    \"test_forward_signature\",\n    \"test_model_get_set_embeddings\",\n    \"test_model_main_input_name\",\n    \"test_correct_missing_keys\",\n    \"test_tie_model_weights\",\n    \"test_can_use_safetensors\",\n    \"test_load_save_without_tied_weights\",\n    \"test_tied_weights_keys\",\n    \"test_model_weights_reload_no_missing_tied_weights\",\n    \"test_pt_tf_model_equivalence\",\n    \"test_mismatched_shapes_have_properly_initialized_weights\",\n    \"test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist\",\n    \"test_model_is_small\",\n    \"test_tf_from_pt_safetensors\",\n    \"test_flax_from_pt_safetensors\",\n    \"ModelTest::test_pipeline_\",  # None of the pipeline tests from PipelineTesterMixin (of which XxxModelTest inherits from) are running on device\n    \"ModelTester::test_pipeline_\",\n    \"/repo_utils/\",\n    \"/utils/\",\n    \"/agents/\",\n}\n\n# allow having multiple repository checkouts and not needing to remember to rerun\n# `pip install -e '.[dev]'` when switching between checkouts and running tests.\ngit_repo_path = abspath(join(dirname(__file__), \"src\"))\nsys.path.insert(1, git_repo_path)\n\n# silence FutureWarning warnings in tests since often we can't act on them until\n# they become normal warnings - i.e. the tests still need to test the current functionality\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(\n        \"markers\", \"is_pt_tf_cross_test: mark test to run only when PT and TF interactions are tested\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"is_pt_flax_cross_test: mark test to run only when PT and FLAX interactions are tested\"\n    )\n    config.addinivalue_line(\"markers\", \"is_pipeline_test: mark test to run only when pipelines are tested\")\n    config.addinivalue_line(\"markers\", \"is_staging_test: mark test to run only in the staging environment\")\n    config.addinivalue_line(\"markers\", \"accelerate_tests: mark test that require accelerate\")\n    config.addinivalue_line(\"markers\", \"agent_tests: mark the agent tests that are run on their specific schedule\")\n    config.addinivalue_line(\"markers\", \"not_device_test: mark the tests always running on cpu\")\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if any(test_name in item.nodeid for test_name in NOT_DEVICE_TESTS):\n            item.add_marker(pytest.mark.not_device_test)\n\n\ndef pytest_addoption(parser):\n    from transformers.testing_utils import pytest_addoption_shared\n\n    pytest_addoption_shared(parser)\n\n\ndef pytest_terminal_summary(terminalreporter):\n    from transformers.testing_utils import pytest_terminal_summary_main\n\n    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n    if make_reports:\n        pytest_terminal_summary_main(terminalreporter, id=make_reports)\n\n\ndef pytest_sessionfinish(session, exitstatus):\n    # If no tests are collected, pytest exists with code 5, which makes the CI fail.\n    if exitstatus == 5:\n        session.exitstatus = 0\n\n\n# Doctest custom flag to ignore output.\nIGNORE_RESULT = doctest.register_optionflag(\"IGNORE_RESULT\")\n\nOutputChecker = doctest.OutputChecker\n\n\nclass CustomOutputChecker(OutputChecker):\n    def check_output(self, want, got, optionflags):\n        if IGNORE_RESULT & optionflags:\n            return True\n        return OutputChecker.check_output(self, want, got, optionflags)\n\n\ndoctest.OutputChecker = CustomOutputChecker\n_pytest.doctest.DoctestModule = HfDoctestModule\ndoctest.DocTestParser = HfDocTestParser\n",
        "imports": [
            "warnings",
            "abspath",
            "doctest",
            "sys",
            "PipelineTesterMixin",
            "os.path",
            "_pytest",
            "HfDoctestModule",
            "pytest",
            "transformers.testing_utils",
            "pytest_terminal_summary_main",
            "pytest_addoption_shared"
        ],
        "functions": [
            "pytest_terminal_summary",
            "pytest_addoption",
            "check_output",
            "pytest_sessionfinish",
            "pytest_configure",
            "pytest_collection_modifyitems"
        ],
        "variables": [
            "DocTestParser",
            "DoctestModule",
            "id",
            "NOT_DEVICE_TESTS",
            "action",
            "OutputChecker",
            "make_reports",
            "git_repo_path",
            "IGNORE_RESULT",
            "exitstatus"
        ]
    },
    {
        "file_name": "hubconf.py",
        "language": "python",
        "source_code": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\n\nSRC_DIR = os.path.join(os.path.dirname(__file__), \"src\")\nsys.path.append(SRC_DIR)\n\n\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoModelForMaskedLM,\n    AutoModelForQuestionAnswering,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    add_start_docstrings,\n)\n\n\ndependencies = [\"torch\", \"numpy\", \"tokenizers\", \"filelock\", \"requests\", \"tqdm\", \"regex\", \"sentencepiece\", \"sacremoses\", \"importlib_metadata\", \"huggingface_hub\"]\n\n\n@add_start_docstrings(AutoConfig.__doc__)\ndef config(*args, **kwargs):\n    r\"\"\"\n                # Using torch.hub !\n                import torch\n\n                config = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased')  # Download configuration from huggingface.co and cache.\n                config = torch.hub.load('huggingface/transformers', 'config', './test/bert_saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n                config = torch.hub.load('huggingface/transformers', 'config', './test/bert_saved_model/my_configuration.json')\n                config = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased', output_attentions=True, foo=False)\n                assert config.output_attentions == True\n                config, unused_kwargs = torch.hub.load('huggingface/transformers', 'config', 'google-bert/bert-base-uncased', output_attentions=True, foo=False, return_unused_kwargs=True)\n                assert config.output_attentions == True\n                assert unused_kwargs == {'foo': False}\n\n            \"\"\"\n\n    return AutoConfig.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoTokenizer.__doc__)\ndef tokenizer(*args, **kwargs):\n    r\"\"\"\n        # Using torch.hub !\n        import torch\n\n        tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', 'google-bert/bert-base-uncased')    # Download vocabulary from huggingface.co and cache.\n        tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', './test/bert_saved_model/')  # E.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`\n\n    \"\"\"\n\n    return AutoTokenizer.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModel.__doc__)\ndef model(*args, **kwargs):\n    r\"\"\"\n            # Using torch.hub !\n            import torch\n\n            model = torch.hub.load('huggingface/transformers', 'model', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n            model = torch.hub.load('huggingface/transformers', 'model', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = torch.hub.load('huggingface/transformers', 'model', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n            assert model.config.output_attentions == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n            model = torch.hub.load('huggingface/transformers', 'model', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"\n\n    return AutoModel.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModelForCausalLM.__doc__)\ndef modelForCausalLM(*args, **kwargs):\n    r\"\"\"\n        # Using torch.hub !\n        import torch\n\n        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'openai-community/gpt2')    # Download model and configuration from huggingface.co and cache.\n        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', './test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', 'openai-community/gpt2', output_attentions=True)  # Update configuration during loading\n        assert model.config.output_attentions == True\n        # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n        config = AutoConfig.from_pretrained('./tf_model/gpt_tf_model_config.json')\n        model = torch.hub.load('huggingface/transformers', 'modelForCausalLM', './tf_model/gpt_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n    \"\"\"\n    return AutoModelForCausalLM.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModelForMaskedLM.__doc__)\ndef modelForMaskedLM(*args, **kwargs):\n    r\"\"\"\n            # Using torch.hub !\n            import torch\n\n            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n            assert model.config.output_attentions == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n            model = torch.hub.load('huggingface/transformers', 'modelForMaskedLM', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"\n\n    return AutoModelForMaskedLM.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModelForSequenceClassification.__doc__)\ndef modelForSequenceClassification(*args, **kwargs):\n    r\"\"\"\n            # Using torch.hub !\n            import torch\n\n            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n            assert model.config.output_attentions == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n            model = torch.hub.load('huggingface/transformers', 'modelForSequenceClassification', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"\n\n    return AutoModelForSequenceClassification.from_pretrained(*args, **kwargs)\n\n\n@add_start_docstrings(AutoModelForQuestionAnswering.__doc__)\ndef modelForQuestionAnswering(*args, **kwargs):\n    r\"\"\"\n        # Using torch.hub !\n        import torch\n\n        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', 'google-bert/bert-base-uncased')    # Download model and configuration from huggingface.co and cache.\n        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', 'google-bert/bert-base-uncased', output_attentions=True)  # Update configuration during loading\n        assert model.config.output_attentions == True\n        # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n        config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')\n        model = torch.hub.load('huggingface/transformers', 'modelForQuestionAnswering', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n    \"\"\"\n    return AutoModelForQuestionAnswering.from_pretrained(*args, **kwargs)\n",
        "imports": [
            "huggingface.co",
            "sys",
            "a",
            "os",
            "transformers",
            "torch"
        ],
        "functions": [
            "model",
            "modelForMaskedLM",
            "tokenizer",
            "config",
            "modelForCausalLM",
            "modelForSequenceClassification",
            "modelForQuestionAnswering"
        ],
        "variables": [
            "output_attentions",
            "dependencies",
            "model",
            "tokenizer",
            "SRC_DIR",
            "unused_kwargs",
            "config"
        ]
    },
    {
        "file_name": "setup.py",
        "language": "python",
        "source_code": "# Copyright 2021 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nSimple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/main/setup.py\n\nTo create the package for pypi.\n\n1. Create the release branch named: v<RELEASE>-release, for example v4.19-release. For a patch release checkout the\n   current release branch.\n\n   If releasing on a special branch, copy the updated README.md on the main branch for the commit you will make\n   for the post-release and run `make fix-copies` on the main branch as well.\n\n2. Run `make pre-release` (or `make pre-patch` for a patch release) and commit these changes with the message:\n   \"Release: <VERSION>\" and push.\n\n3. Go back to the main branch and run `make post-release` then `make fix-copies`. Commit these changes with the\n   message \"v<NEXT_VERSION>.dev.0\" and push to main.\n\n# If you were just cutting the branch in preparation for a release, you can stop here for now.\n\n4. Wait for the tests on the release branch to be completed and be green (otherwise revert and fix bugs)\n\n5. On the release branch, add a tag in git to mark the release: \"git tag v<VERSION> -m 'Adds tag v<VERSION> for pypi' \"\n   Push the tag to git: git push --tags origin v<RELEASE>-release\n\n6. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n\n   Run `make build-release`. This will build the release and do some sanity checks for you. If this ends with an error\n   message, you need to fix things before going further.\n\n   You should now have a /dist directory with both .whl and .tar.gz source versions.\n\n7. Check that everything looks correct by uploading the package to the pypi test server:\n\n   twine upload dist/* -r testpypi\n   (pypi suggest using twine as other methods upload files via plaintext.)\n   You may have to specify the repository url, use the following command then:\n   twine upload dist/* -r testpypi --repository-url=https://test.pypi.org/legacy/\n\n   Check that you can install it in a virtualenv by running:\n   pip install -i https://testpypi.python.org/pypi transformers\n\n   Check you can run the following commands:\n   python -c \"from transformers import pipeline; classifier = pipeline('text-classification'); print(classifier('What a nice release'))\"\n   python -c \"from transformers import *\"\n   python utils/check_build.py --check_lib\n\n   If making a patch release, double check the bug you are patching is indeed resolved.\n\n8. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n\n9. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.\n\"\"\"\n\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\nfrom setuptools import Command, find_packages, setup\n\n\n# Remove stale transformers.egg-info directory to avoid https://github.com/pypa/pip/issues/5466\nstale_egg_info = Path(__file__).parent / \"transformers.egg-info\"\nif stale_egg_info.exists():\n    print(\n        (\n            \"Warning: {} exists.\\n\\n\"\n            \"If you recently updated transformers to 3.0 or later, this is expected,\\n\"\n            \"but it may prevent transformers from installing in editable mode.\\n\\n\"\n            \"This directory is automatically generated by Python's packaging tools.\\n\"\n            \"I will remove it now.\\n\\n\"\n            \"See https://github.com/pypa/pip/issues/5466 for details.\\n\"\n        ).format(stale_egg_info)\n    )\n    shutil.rmtree(stale_egg_info)\n\n\n# IMPORTANT:\n# 1. all dependencies should be listed here with their version requirements if any\n# 2. once modified, run: `make deps_table_update` to update src/transformers/dependency_versions_table.py\n_deps = [\n    \"Pillow>=10.0.1,<=15.0\",\n    \"accelerate>=0.26.0\",\n    \"av\",\n    \"beautifulsoup4\",\n    \"blobfile\",\n    \"codecarbon>=2.8.1\",\n    \"cookiecutter==1.7.3\",\n    \"dataclasses\",\n    \"datasets!=2.5.0\",\n    \"deepspeed>=0.9.3\",\n    \"diffusers\",\n    \"dill<0.3.5\",\n    \"evaluate>=0.2.0\",\n    \"faiss-cpu\",\n    \"fastapi\",\n    \"filelock\",\n    \"flax>=0.4.1,<=0.7.0\",\n    \"fsspec<2023.10.0\",\n    \"ftfy\",\n    \"fugashi>=1.0\",\n    \"GitPython<3.1.19\",\n    \"hf-doc-builder>=0.3.0\",\n    \"huggingface-hub>=0.24.0,<1.0\",\n    \"importlib_metadata\",\n    \"ipadic>=1.0.0,<2.0\",\n    \"isort>=5.5.4\",\n    \"jax>=0.4.1,<=0.4.13\",\n    \"jaxlib>=0.4.1,<=0.4.13\",\n    \"jieba\",\n    \"jinja2>=3.1.0\",\n    \"kenlm\",\n    # Keras pin - this is to make sure Keras 3 doesn't destroy us. Remove or change when we have proper support.\n    \"keras>2.9,<2.16\",\n    \"keras-nlp>=0.3.1,<0.14.0\",  # keras-nlp 0.14 doesn't support keras 2, see pin on keras.\n    \"librosa\",\n    \"nltk<=3.8.1\",\n    \"natten>=0.14.6,<0.15.0\",\n    \"numpy>=1.17\",\n    \"onnxconverter-common\",\n    \"onnxruntime-tools>=1.4.2\",\n    \"onnxruntime>=1.4.0\",\n    \"opencv-python\",\n    \"optimum-benchmark>=0.3.0\",\n    \"optuna\",\n    \"optax>=0.0.8,<=0.1.4\",\n    \"packaging>=20.0\",\n    \"parameterized\",\n    \"phonemizer\",\n    \"protobuf\",\n    \"psutil\",\n    \"pyyaml>=5.1\",\n    \"pydantic\",\n    \"pytest>=7.2.0,<8.0.0\",\n    \"pytest-asyncio\",\n    \"pytest-timeout\",\n    \"pytest-xdist\",\n    \"python>=3.9.0\",\n    \"ray[tune]>=2.7.0\",\n    \"regex!=2019.12.17\",\n    \"requests\",\n    \"rhoknp>=1.1.0,<1.3.1\",\n    \"rjieba\",\n    \"rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1\",\n    \"ruff==0.5.1\",\n    \"sacrebleu>=1.4.12,<2.0.0\",\n    \"sacremoses\",\n    \"safetensors>=0.4.1\",\n    \"sagemaker>=2.31.0\",\n    \"schedulefree>=1.2.6\",\n    \"scikit-learn\",\n    \"scipy<1.13.0\",  # SciPy >= 1.13.0 is not supported with the current jax pin (`jax>=0.4.1,<=0.4.13`)\n    \"sentencepiece>=0.1.91,!=0.1.92\",\n    \"sigopt\",\n    \"starlette\",\n    \"sudachipy>=0.6.6\",\n    \"sudachidict_core>=20220729\",\n    \"tensorboard\",\n    # TensorFlow pin. When changing this value, update examples/tensorflow/_tests_requirements.txt accordingly\n    \"tensorflow-cpu>2.9,<2.16\",\n    \"tensorflow>2.9,<2.16\",\n    \"tensorflow-text<2.16\",\n    \"tensorflow-probability<0.24\",\n    \"tf2onnx\",\n    \"timeout-decorator\",\n    \"tiktoken\",\n    \"timm<=1.0.11\",\n    \"tokenizers>=0.21,<0.22\",\n    \"torch>=2.0\",\n    \"torchaudio\",\n    \"torchvision\",\n    \"pyctcdecode>=0.4.0\",\n    \"tqdm>=4.27\",\n    \"unidic>=1.0.2\",\n    \"unidic_lite>=1.0.7\",\n    \"urllib3<2.0.0\",\n    \"uvicorn\",\n    \"pytest-rich\",\n    \"libcst\",\n    \"rich\",\n]\n\n\n# this is a lookup table with items like:\n#\n# tokenizers: \"tokenizers==0.9.4\"\n# packaging: \"packaging\"\n#\n# some of the values are versioned whereas others aren't.\ndeps = {b: a for a, b in (re.findall(r\"^(([^!=<>~ ]+)(?:[!=<>~ ].*)?$)\", x)[0] for x in _deps)}\n\n# since we save this data in src/transformers/dependency_versions_table.py it can be easily accessed from\n# anywhere. If you need to quickly access the data from this table in a shell, you can do so easily with:\n#\n# python -c 'import sys; from transformers.dependency_versions_table import deps; \\\n# print(\" \".join([ deps[x] for x in sys.argv[1:]]))' tokenizers datasets\n#\n# Just pass the desired package names to that script as it's shown with 2 packages above.\n#\n# If transformers is not yet installed and the work is done from the cloned repo remember to add `PYTHONPATH=src` to the script above\n#\n# You can then feed this for example to `pip`:\n#\n# pip install -U $(python -c 'import sys; from transformers.dependency_versions_table import deps; \\\n# print(\" \".join([deps[x] for x in sys.argv[1:]]))' tokenizers datasets)\n#\n\n\ndef deps_list(*pkgs):\n    return [deps[pkg] for pkg in pkgs]\n\n\nclass DepsTableUpdateCommand(Command):\n    \"\"\"\n    A custom distutils command that updates the dependency table.\n    usage: python setup.py deps_table_update\n    \"\"\"\n\n    description = \"build runtime dependency table\"\n    user_options = [\n        # format: (long option, short option, description).\n        (\"dep-table-update\", None, \"updates src/transformers/dependency_versions_table.py\"),\n    ]\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        entries = \"\\n\".join([f'    \"{k}\": \"{v}\",' for k, v in deps.items()])\n        content = [\n            \"# THIS FILE HAS BEEN AUTOGENERATED. To update:\",\n            \"# 1. modify the `_deps` dict in setup.py\",\n            \"# 2. run `make deps_table_update``\",\n            \"deps = {\",\n            entries,\n            \"}\",\n            \"\",\n        ]\n        target = \"src/transformers/dependency_versions_table.py\"\n        print(f\"updating {target}\")\n        with open(target, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n            f.write(\"\\n\".join(content))\n\n\nextras = {}\n\nextras[\"ja\"] = deps_list(\"fugashi\", \"ipadic\", \"unidic_lite\", \"unidic\", \"sudachipy\", \"sudachidict_core\", \"rhoknp\")\nextras[\"sklearn\"] = deps_list(\"scikit-learn\")\n\nextras[\"tf\"] = deps_list(\"tensorflow\", \"onnxconverter-common\", \"tf2onnx\", \"tensorflow-text\", \"keras-nlp\")\nextras[\"tf-cpu\"] = deps_list(\n    \"keras\",\n    \"tensorflow-cpu\",\n    \"onnxconverter-common\",\n    \"tf2onnx\",\n    \"tensorflow-text\",\n    \"keras-nlp\",\n    \"tensorflow-probability\",\n)\n\nextras[\"torch\"] = deps_list(\"torch\", \"accelerate\")\nextras[\"accelerate\"] = deps_list(\"accelerate\")\n\nif os.name == \"nt\":  # windows\n    extras[\"retrieval\"] = deps_list(\"datasets\")  # faiss is not supported on windows\n    extras[\"flax\"] = []  # jax is not supported on windows\nelse:\n    extras[\"retrieval\"] = deps_list(\"faiss-cpu\", \"datasets\")\n    extras[\"flax\"] = deps_list(\"jax\", \"jaxlib\", \"flax\", \"optax\", \"scipy\")\n\nextras[\"tokenizers\"] = deps_list(\"tokenizers\")\nextras[\"ftfy\"] = deps_list(\"ftfy\")\nextras[\"onnxruntime\"] = deps_list(\"onnxruntime\", \"onnxruntime-tools\")\nextras[\"onnx\"] = deps_list(\"onnxconverter-common\", \"tf2onnx\") + extras[\"onnxruntime\"]\nextras[\"modelcreation\"] = deps_list(\"cookiecutter\")\n\nextras[\"sagemaker\"] = deps_list(\"sagemaker\")\nextras[\"deepspeed\"] = deps_list(\"deepspeed\") + extras[\"accelerate\"]\nextras[\"optuna\"] = deps_list(\"optuna\")\nextras[\"ray\"] = deps_list(\"ray[tune]\")\nextras[\"sigopt\"] = deps_list(\"sigopt\")\n\nextras[\"integrations\"] = extras[\"optuna\"] + extras[\"ray\"] + extras[\"sigopt\"]\n\nextras[\"serving\"] = deps_list(\"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\")\nextras[\"audio\"] = deps_list(\"librosa\", \"pyctcdecode\", \"phonemizer\", \"kenlm\")\n# `pip install \".[speech]\"` is deprecated and `pip install \".[torch-speech]\"` should be used instead\nextras[\"speech\"] = deps_list(\"torchaudio\") + extras[\"audio\"]\nextras[\"torch-speech\"] = deps_list(\"torchaudio\") + extras[\"audio\"]\nextras[\"tf-speech\"] = extras[\"audio\"]\nextras[\"flax-speech\"] = extras[\"audio\"]\nextras[\"vision\"] = deps_list(\"Pillow\")\nextras[\"timm\"] = deps_list(\"timm\")\nextras[\"torch-vision\"] = deps_list(\"torchvision\") + extras[\"vision\"]\nextras[\"natten\"] = deps_list(\"natten\")\nextras[\"codecarbon\"] = deps_list(\"codecarbon\")\nextras[\"video\"] = deps_list(\"av\")\n\nextras[\"sentencepiece\"] = deps_list(\"sentencepiece\", \"protobuf\")\nextras[\"tiktoken\"] = deps_list(\"tiktoken\", \"blobfile\")\nextras[\"testing\"] = (\n    deps_list(\n        \"pytest\",\n        \"pytest-asyncio\",\n        \"pytest-rich\",\n        \"pytest-xdist\",\n        \"timeout-decorator\",\n        \"parameterized\",\n        \"psutil\",\n        \"datasets\",\n        \"dill\",\n        \"evaluate\",\n        \"pytest-timeout\",\n        \"ruff\",\n        \"sacrebleu\",\n        \"rouge-score\",\n        \"nltk\",\n        \"GitPython\",\n        \"sacremoses\",\n        \"rjieba\",\n        \"beautifulsoup4\",\n        \"tensorboard\",\n        \"pydantic\",\n        \"sentencepiece\",\n    )\n    + extras[\"retrieval\"]\n    + extras[\"modelcreation\"]\n)\n\nextras[\"deepspeed-testing\"] = extras[\"deepspeed\"] + extras[\"testing\"] + extras[\"optuna\"] + extras[\"sentencepiece\"]\nextras[\"ruff\"] = deps_list(\"ruff\")\nextras[\"quality\"] = deps_list(\"datasets\", \"isort\", \"ruff\", \"GitPython\", \"urllib3\", \"libcst\", \"rich\")\n\nextras[\"all\"] = (\n    extras[\"tf\"]\n    + extras[\"torch\"]\n    + extras[\"flax\"]\n    + extras[\"sentencepiece\"]\n    + extras[\"tokenizers\"]\n    + extras[\"torch-speech\"]\n    + extras[\"vision\"]\n    + extras[\"integrations\"]\n    + extras[\"timm\"]\n    + extras[\"torch-vision\"]\n    + extras[\"codecarbon\"]\n    + extras[\"accelerate\"]\n    + extras[\"video\"]\n)\n\n\nextras[\"dev-torch\"] = (\n    extras[\"testing\"]\n    + extras[\"torch\"]\n    + extras[\"sentencepiece\"]\n    + extras[\"tokenizers\"]\n    + extras[\"torch-speech\"]\n    + extras[\"vision\"]\n    + extras[\"integrations\"]\n    + extras[\"timm\"]\n    + extras[\"torch-vision\"]\n    + extras[\"codecarbon\"]\n    + extras[\"quality\"]\n    + extras[\"ja\"]\n    + extras[\"sklearn\"]\n    + extras[\"modelcreation\"]\n    + extras[\"onnxruntime\"]\n)\nextras[\"dev-tensorflow\"] = (\n    extras[\"testing\"]\n    + extras[\"tf\"]\n    + extras[\"sentencepiece\"]\n    + extras[\"tokenizers\"]\n    + extras[\"vision\"]\n    + extras[\"quality\"]\n    + extras[\"sklearn\"]\n    + extras[\"modelcreation\"]\n    + extras[\"onnx\"]\n    + extras[\"tf-speech\"]\n)\nextras[\"dev\"] = (\n    extras[\"all\"] + extras[\"testing\"] + extras[\"quality\"] + extras[\"ja\"] + extras[\"sklearn\"] + extras[\"modelcreation\"]\n)\n\nextras[\"torchhub\"] = deps_list(\n    \"filelock\",\n    \"huggingface-hub\",\n    \"importlib_metadata\",\n    \"numpy\",\n    \"packaging\",\n    \"protobuf\",\n    \"regex\",\n    \"requests\",\n    \"sentencepiece\",\n    \"torch\",\n    \"tokenizers\",\n    \"tqdm\",\n)\n\nextras[\"agents\"] = deps_list(\n    \"diffusers\", \"accelerate\", \"datasets\", \"torch\", \"sentencepiece\", \"opencv-python\", \"Pillow\"\n)\n\nextras[\"benchmark\"] = deps_list(\"optimum-benchmark\")\n\n# when modifying the following list, make sure to update src/transformers/dependency_versions_check.py\ninstall_requires = [\n    deps[\"filelock\"],  # filesystem locks, e.g., to prevent parallel downloads\n    deps[\"huggingface-hub\"],\n    deps[\"numpy\"],\n    deps[\"packaging\"],  # utilities from PyPA to e.g., compare versions\n    deps[\"pyyaml\"],  # used for the model cards metadata\n    deps[\"regex\"],  # for OpenAI GPT\n    deps[\"requests\"],  # for downloading models over HTTPS\n    deps[\"tokenizers\"],\n    deps[\"safetensors\"],\n    deps[\"tqdm\"],  # progress bars in model download and training scripts\n]\n\nsetup(\n    name=\"transformers\",\n    version=\"4.49.0.dev0\",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)\n    author=\"The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\",\n    author_email=\"transformers@huggingface.co\",\n    description=\"State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\",\n    long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    keywords=\"NLP vision speech deep learning transformer pytorch tensorflow jax BERT GPT-2 Wav2Vec2 ViT\",\n    license=\"Apache 2.0 License\",\n    url=\"https://github.com/huggingface/transformers\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    include_package_data=True,\n    package_data={\"\": [\"**/*.cu\", \"**/*.cpp\", \"**/*.cuh\", \"**/*.h\", \"**/*.pyx\"]},\n    zip_safe=False,\n    extras_require=extras,\n    entry_points={\"console_scripts\": [\"transformers-cli=transformers.commands.transformers_cli:main\"]},\n    python_requires=\">=3.9.0\",\n    install_requires=list(install_requires),\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    cmdclass={\"deps_table_update\": DepsTableUpdateCommand},\n)\n\nextras[\"tests_torch\"] = deps_list()\nextras[\"tests_tf\"] = deps_list()\nextras[\"tests_flax\"] = deps_list()\nextras[\"tests_torch_and_tf\"] = deps_list()\nextras[\"tests_torch_and_flax\"] = deps_list()\nextras[\"tests_hub\"] = deps_list()\nextras[\"tests_pipelines_torch\"] = deps_list()\nextras[\"tests_pipelines_tf\"] = deps_list()\nextras[\"tests_onnx\"] = deps_list()\nextras[\"tests_examples_torch\"] = deps_list()\nextras[\"tests_examples_tf\"] = deps_list()\nextras[\"tests_custom_tokenizers\"] = deps_list()\nextras[\"tests_exotic_models\"] = deps_list()\nextras[\"consistency\"] = deps_list()\n",
        "imports": [
            "this",
            "PyPA",
            "deps",
            "Path",
            "RELEASE.md",
            "AllenNLP",
            "Command",
            "sys",
            "transformers.dependency_versions_table",
            "setuptools",
            "re",
            "the",
            "os",
            "transformers",
            "installing",
            "pipeline",
            "pathlib",
            "shutil"
        ],
        "functions": [
            "finalize_options",
            "run",
            "initialize_options",
            "deps_list"
        ],
        "variables": [
            "classifier",
            "package_dir",
            "author",
            "entries",
            "extras_require",
            "entry_points",
            "PYTHONPATH",
            "name",
            "deps",
            "keywords",
            "extras",
            "version",
            "install_requires",
            "classifiers",
            "package_data",
            "cookiecutter",
            "packages",
            "cmdclass",
            "url",
            "long_description",
            "_deps",
            "tokenizers",
            "python_requires",
            "zip_safe",
            "description",
            "user_options",
            "target",
            "ruff",
            "author_email",
            "content",
            "long_description_content_type",
            "encoding",
            "license",
            "stale_egg_info",
            "include_package_data"
        ]
    },
    {
        "file_name": "setup.py",
        "language": "python",
        "source_code": "#!/usr/bin/env python\n# coding: utf-8\n\nfrom __future__ import print_function\n\nimport os.path\nimport warnings\nimport sys\n\ntry:\n    from setuptools import setup, Command\n    setuptools_available = True\nexcept ImportError:\n    from distutils.core import setup, Command\n    setuptools_available = False\nfrom distutils.spawn import spawn\n\ntry:\n    # This will create an exe that needs Microsoft Visual C++ 2008\n    # Redistributable Package\n    import py2exe\nexcept ImportError:\n    if len(sys.argv) >= 2 and sys.argv[1] == 'py2exe':\n        print('Cannot import py2exe', file=sys.stderr)\n        exit(1)\n\npy2exe_options = {\n    'bundle_files': 1,\n    'compressed': 1,\n    'optimize': 2,\n    'dist_dir': '.',\n    'dll_excludes': ['w9xpopen.exe', 'crypt32.dll'],\n}\n\n# Get the version from youtube_dl/version.py without importing the package\nexec(compile(open('youtube_dl/version.py').read(),\n             'youtube_dl/version.py', 'exec'))\n\nDESCRIPTION = 'YouTube video downloader'\nLONG_DESCRIPTION = 'Command-line program to download videos from YouTube.com and other video sites'\n\npy2exe_console = [{\n    'script': './youtube_dl/__main__.py',\n    'dest_base': 'youtube-dl',\n    'version': __version__,\n    'description': DESCRIPTION,\n    'comments': LONG_DESCRIPTION,\n    'product_name': 'youtube-dl',\n    'product_version': __version__,\n}]\n\npy2exe_params = {\n    'console': py2exe_console,\n    'options': {'py2exe': py2exe_options},\n    'zipfile': None\n}\n\nif len(sys.argv) >= 2 and sys.argv[1] == 'py2exe':\n    params = py2exe_params\nelse:\n    files_spec = [\n        ('etc/bash_completion.d', ['youtube-dl.bash-completion']),\n        ('etc/fish/completions', ['youtube-dl.fish']),\n        ('share/doc/youtube_dl', ['README.txt']),\n        ('share/man/man1', ['youtube-dl.1'])\n    ]\n    root = os.path.dirname(os.path.abspath(__file__))\n    data_files = []\n    for dirname, files in files_spec:\n        resfiles = []\n        for fn in files:\n            if not os.path.exists(fn):\n                warnings.warn('Skipping file %s since it is not present. Type  make  to build all automatically generated files.' % fn)\n            else:\n                resfiles.append(fn)\n        data_files.append((dirname, resfiles))\n\n    params = {\n        'data_files': data_files,\n    }\n    if setuptools_available:\n        params['entry_points'] = {'console_scripts': ['youtube-dl = youtube_dl:main']}\n    else:\n        params['scripts'] = ['bin/youtube-dl']\n\nclass build_lazy_extractors(Command):\n    description = 'Build the extractor lazy loading module'\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        spawn(\n            [sys.executable, 'devscripts/make_lazy_extractors.py', 'youtube_dl/extractor/lazy_extractors.py'],\n            dry_run=self.dry_run,\n        )\n\nsetup(\n    name='youtube_dl',\n    version=__version__,\n    description=DESCRIPTION,\n    long_description=LONG_DESCRIPTION,\n    url='https://github.com/ytdl-org/youtube-dl',\n    author='Ricardo Garcia',\n    author_email='ytdl@yt-dl.org',\n    maintainer='Sergey M.',\n    maintainer_email='dstftw@gmail.com',\n    license='Unlicense',\n    packages=[\n        'youtube_dl',\n        'youtube_dl.extractor', 'youtube_dl.downloader',\n        'youtube_dl.postprocessor'],\n\n    # Provokes warning on most systems (why?!)\n    # test_suite = 'nose.collector',\n    # test_requires = ['nosetest'],\n\n    classifiers=[\n        'Topic :: Multimedia :: Video',\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Console',\n        'License :: Public Domain',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: Implementation',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: IronPython',\n        'Programming Language :: Python :: Implementation :: Jython',\n        'Programming Language :: Python :: Implementation :: PyPy',\n    ],\n\n    cmdclass={'build_lazy_extractors': build_lazy_extractors},\n    **params\n)\n",
        "imports": [
            "warnings",
            "setup",
            "spawn",
            "YouTube.com",
            "__future__",
            "sys",
            "setuptools",
            "os.path",
            "distutils.core",
            "py2exe",
            "youtube_dl",
            "print_function",
            "distutils.spawn"
        ],
        "functions": [
            "finalize_options",
            "run",
            "initialize_options"
        ],
        "variables": [
            "author",
            "dl",
            "test_suite",
            "resfiles",
            "test_requires",
            "name",
            "file",
            "LONG_DESCRIPTION",
            "DESCRIPTION",
            "py2exe_console",
            "maintainer_email",
            "py2exe_params",
            "maintainer",
            "data_files",
            "version",
            "classifiers",
            "packages",
            "cmdclass",
            "url",
            "py2exe_options",
            "dry_run",
            "files_spec",
            "long_description",
            "description",
            "user_options",
            "setuptools_available",
            "author_email",
            "params",
            "root",
            "license"
        ]
    },
    {
        "file_name": "configure.py",
        "language": "python",
        "source_code": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"configure script to get build parameters from user.\"\"\"\n\nimport argparse\nimport errno\nimport json\nimport os\nimport platform\nimport re\nimport shutil\nimport subprocess\nimport sys\n\n\n_DEFAULT_CUDA_COMPUTE_CAPABILITIES = '3.5,7.0'\n\n_SUPPORTED_ANDROID_NDK_VERSIONS = [19, 20, 21, 25]\n\n_DEFAULT_PROMPT_ASK_ATTEMPTS = 10\n\n_TF_BAZELRC_FILENAME = '.tf_configure.bazelrc'\n_TF_WORKSPACE_ROOT = ''\n_TF_BAZELRC = ''\n_TF_CURRENT_BAZEL_VERSION = None\n\nNCCL_LIB_PATHS = [\n    'lib64/', 'lib/powerpc64le-linux-gnu/', 'lib/x86_64-linux-gnu/', ''\n]\n\n# List of files to configure when building Bazel on Apple platforms.\nAPPLE_BAZEL_FILES = [\n    'tensorflow/lite/ios/BUILD', 'tensorflow/lite/objc/BUILD',\n    'tensorflow/lite/swift/BUILD',\n    'tensorflow/lite/tools/benchmark/experimental/ios/BUILD'\n]\n\n# List of files to move when building for iOS.\nIOS_FILES = [\n    'tensorflow/lite/objc/TensorFlowLiteObjC.podspec',\n    'tensorflow/lite/swift/TensorFlowLiteSwift.podspec',\n]\n\n\nclass UserInputError(Exception):\n  pass\n\n\ndef is_windows():\n  return platform.system() == 'Windows'\n\n\ndef is_linux():\n  return platform.system() == 'Linux'\n\n\ndef is_macos():\n  return platform.system() == 'Darwin'\n\n\ndef is_ppc64le():\n  return platform.machine() == 'ppc64le'\n\n\ndef is_s390x():\n  return platform.machine() == 's390x'\n\n\ndef is_cygwin():\n  return platform.system().startswith('CYGWIN_NT')\n\n\ndef get_input(question):\n  try:\n    try:\n      answer = raw_input(question)\n    except NameError:\n      answer = input(question)  # pylint: disable=bad-builtin\n  except EOFError:\n    answer = ''\n  return answer\n\n\ndef symlink_force(target, link_name):\n  \"\"\"Force symlink, equivalent of 'ln -sf'.\n\n  Args:\n    target: items to link to.\n    link_name: name of the link.\n  \"\"\"\n  try:\n    os.symlink(target, link_name)\n  except OSError as e:\n    if e.errno == errno.EEXIST:\n      os.remove(link_name)\n      os.symlink(target, link_name)\n    else:\n      raise e\n\n\ndef write_to_bazelrc(line):\n  with open(_TF_BAZELRC, 'a') as f:\n    f.write(line + '\\n')\n\n\ndef write_action_env_to_bazelrc(var_name, var):\n  write_to_bazelrc('build --action_env {}=\"{}\"'.format(var_name, str(var)))\n\n\ndef write_repo_env_to_bazelrc(config_name, var_name, var):\n  write_to_bazelrc(\n      'build:{} --repo_env {}=\"{}\"'.format(config_name, var_name, str(var))\n  )\n\n\ndef run_shell(cmd, allow_non_zero=False, stderr=None):\n  if stderr is None:\n    stderr = sys.stdout\n  if allow_non_zero:\n    try:\n      output = subprocess.check_output(cmd, stderr=stderr)\n    except subprocess.CalledProcessError as e:\n      output = e.output\n  else:\n    output = subprocess.check_output(cmd, stderr=stderr)\n  return output.decode('UTF-8').strip()\n\n\ndef cygpath(path):\n  \"\"\"Convert path from posix to windows.\"\"\"\n  return os.path.abspath(path).replace('\\\\', '/')\n\n\ndef get_python_path(environ_cp, python_bin_path):\n  \"\"\"Get the python site package paths.\"\"\"\n  python_paths = []\n  if environ_cp.get('PYTHONPATH'):\n    python_paths = environ_cp.get('PYTHONPATH').split(':')\n  try:\n    stderr = open(os.devnull, 'wb')\n    library_paths = run_shell([\n        python_bin_path, '-c',\n        'import site; print(\"\\\\n\".join(site.getsitepackages()))'\n    ],\n                              stderr=stderr).split('\\n')\n  except subprocess.CalledProcessError:\n    library_paths = [\n        run_shell([\n            python_bin_path,\n            '-c',\n            'import sysconfig;print(sysconfig.get_path(\"purelib\")',\n        ])\n    ]\n\n  all_paths = set(python_paths + library_paths)\n  # Sort set so order is deterministic\n  all_paths = sorted(all_paths)\n\n  paths = []\n  for path in all_paths:\n    if os.path.isdir(path):\n      paths.append(path)\n  return paths\n\n\ndef get_python_major_version(python_bin_path):\n  \"\"\"Get the python major version.\"\"\"\n  return run_shell([python_bin_path, '-c', 'import sys; print(sys.version[0])'])\n\n\ndef setup_python(environ_cp):\n  \"\"\"Setup python related env variables.\"\"\"\n  # Get PYTHON_BIN_PATH, default is the current running python.\n  default_python_bin_path = sys.executable\n  ask_python_bin_path = ('Please specify the location of python. [Default is '\n                         '{}]: ').format(default_python_bin_path)\n  while True:\n    python_bin_path = get_from_env_or_user_or_default(environ_cp,\n                                                      'PYTHON_BIN_PATH',\n                                                      ask_python_bin_path,\n                                                      default_python_bin_path)\n    # Check if the path is valid\n    if os.path.isfile(python_bin_path) and os.access(python_bin_path, os.X_OK):\n      break\n    elif not os.path.exists(python_bin_path):\n      print('Invalid python path: {} cannot be found.'.format(python_bin_path))\n    else:\n      print('{} is not executable.  Is it the python binary?'.format(\n          python_bin_path))\n    environ_cp['PYTHON_BIN_PATH'] = ''\n\n  # Convert python path to Windows style before checking lib and version\n  if is_windows() or is_cygwin():\n    python_bin_path = cygpath(python_bin_path)\n\n  # Get PYTHON_LIB_PATH\n  python_lib_path = environ_cp.get('PYTHON_LIB_PATH')\n  if not python_lib_path:\n    python_lib_paths = get_python_path(environ_cp, python_bin_path)\n    if environ_cp.get('USE_DEFAULT_PYTHON_LIB_PATH') == '1':\n      python_lib_path = python_lib_paths[0]\n    else:\n      print('Found possible Python library paths:\\n  %s' %\n            '\\n  '.join(python_lib_paths))\n      default_python_lib_path = python_lib_paths[0]\n      python_lib_path = get_input(\n          'Please input the desired Python library path to use.  '\n          'Default is [{}]\\n'.format(python_lib_paths[0]))\n      if not python_lib_path:\n        python_lib_path = default_python_lib_path\n    environ_cp['PYTHON_LIB_PATH'] = python_lib_path\n\n  python_major_version = get_python_major_version(python_bin_path)\n  if python_major_version == '2':\n    write_to_bazelrc('build --host_force_python=PY2')\n\n  # Convert python path to Windows style before writing into bazel.rc\n  if is_windows() or is_cygwin():\n    python_lib_path = cygpath(python_lib_path)\n\n  # Set-up env variables used by python_configure.bzl\n  write_action_env_to_bazelrc('PYTHON_BIN_PATH', python_bin_path)\n  write_action_env_to_bazelrc('PYTHON_LIB_PATH', python_lib_path)\n  write_to_bazelrc('build --python_path=\\\"{}\"'.format(python_bin_path))\n  environ_cp['PYTHON_BIN_PATH'] = python_bin_path\n\n  # If chosen python_lib_path is from a path specified in the PYTHONPATH\n  # variable, need to tell bazel to include PYTHONPATH\n  if environ_cp.get('PYTHONPATH'):\n    python_paths = environ_cp.get('PYTHONPATH').split(':')\n    if python_lib_path in python_paths:\n      write_action_env_to_bazelrc('PYTHONPATH', environ_cp.get('PYTHONPATH'))\n\n  # Write tools/python_bin_path.sh\n  with open(\n      os.path.join(_TF_WORKSPACE_ROOT, 'tools', 'python_bin_path.sh'),\n      'w') as f:\n    f.write('export PYTHON_BIN_PATH=\"{}\"'.format(python_bin_path))\n\n\ndef reset_tf_configure_bazelrc():\n  \"\"\"Reset file that contains customized config settings.\"\"\"\n  open(_TF_BAZELRC, 'w').close()\n\n\ndef cleanup_makefile():\n  \"\"\"Delete any leftover BUILD files from the Makefile build.\n\n  These files could interfere with Bazel parsing.\n  \"\"\"\n  makefile_download_dir = os.path.join(_TF_WORKSPACE_ROOT, 'tensorflow',\n                                       'contrib', 'makefile', 'downloads')\n  if os.path.isdir(makefile_download_dir):\n    for root, _, filenames in os.walk(makefile_download_dir):\n      for f in filenames:\n        if f.endswith('BUILD'):\n          os.remove(os.path.join(root, f))\n\n\ndef get_var(environ_cp,\n            var_name,\n            query_item,\n            enabled_by_default,\n            question=None,\n            yes_reply=None,\n            no_reply=None):\n  \"\"\"Get boolean input from user.\n\n  If var_name is not set in env, ask user to enable query_item or not. If the\n  response is empty, use the default.\n\n  Args:\n    environ_cp: copy of the os.environ.\n    var_name: string for name of environment variable, e.g. \"TF_NEED_CUDA\".\n    query_item: string for feature related to the variable, e.g. \"CUDA for\n      Nvidia GPUs\".\n    enabled_by_default: boolean for default behavior.\n    question: optional string for how to ask for user input.\n    yes_reply: optional string for reply when feature is enabled.\n    no_reply: optional string for reply when feature is disabled.\n\n  Returns:\n    boolean value of the variable.\n\n  Raises:\n    UserInputError: if an environment variable is set, but it cannot be\n      interpreted as a boolean indicator, assume that the user has made a\n      scripting error, and will continue to provide invalid input.\n      Raise the error to avoid infinitely looping.\n  \"\"\"\n  if not question:\n    question = 'Do you wish to build TensorFlow with {} support?'.format(\n        query_item)\n  if not yes_reply:\n    yes_reply = '{} support will be enabled for TensorFlow.'.format(query_item)\n  if not no_reply:\n    no_reply = 'No {}'.format(yes_reply)\n\n  yes_reply += '\\n'\n  no_reply += '\\n'\n\n  if enabled_by_default:\n    question += ' [Y/n]: '\n  else:\n    question += ' [y/N]: '\n\n  var = environ_cp.get(var_name)\n  if var is not None:\n    var_content = var.strip().lower()\n    true_strings = ('1', 't', 'true', 'y', 'yes')\n    false_strings = ('0', 'f', 'false', 'n', 'no')\n    if var_content in true_strings:\n      var = True\n    elif var_content in false_strings:\n      var = False\n    else:\n      raise UserInputError(\n          'Environment variable %s must be set as a boolean indicator.\\n'\n          'The following are accepted as TRUE : %s.\\n'\n          'The following are accepted as FALSE: %s.\\n'\n          'Current value is %s.' %\n          (var_name, ', '.join(true_strings), ', '.join(false_strings), var))\n\n  while var is None:\n    user_input_origin = get_input(question)\n    user_input = user_input_origin.strip().lower()\n    if user_input == 'y':\n      print(yes_reply)\n      var = True\n    elif user_input == 'n':\n      print(no_reply)\n      var = False\n    elif not user_input:\n      if enabled_by_default:\n        print(yes_reply)\n        var = True\n      else:\n        print(no_reply)\n        var = False\n    else:\n      print('Invalid selection: {}'.format(user_input_origin))\n  return var\n\n\ndef set_action_env_var(environ_cp,\n                       var_name,\n                       query_item,\n                       enabled_by_default,\n                       question=None,\n                       yes_reply=None,\n                       no_reply=None,\n                       bazel_config_name=None):\n  \"\"\"Set boolean action_env variable.\n\n  Ask user if query_item will be enabled. Default is used if no input is given.\n  Set environment variable and write to .bazelrc.\n\n  Args:\n    environ_cp: copy of the os.environ.\n    var_name: string for name of environment variable, e.g. \"TF_NEED_CUDA\".\n    query_item: string for feature related to the variable, e.g. \"CUDA for\n      Nvidia GPUs\".\n    enabled_by_default: boolean for default behavior.\n    question: optional string for how to ask for user input.\n    yes_reply: optional string for reply when feature is enabled.\n    no_reply: optional string for reply when feature is disabled.\n    bazel_config_name: adding config to .bazelrc instead of action_env.\n  \"\"\"\n  var = int(\n      get_var(environ_cp, var_name, query_item, enabled_by_default, question,\n              yes_reply, no_reply))\n\n  if not bazel_config_name:\n    write_action_env_to_bazelrc(var_name, var)\n  elif var:\n    write_to_bazelrc('build --config=%s' % bazel_config_name)\n  environ_cp[var_name] = str(var)\n\n\ndef convert_version_to_int(version):\n  \"\"\"Convert a version number to a integer that can be used to compare.\n\n  Version strings of the form X.YZ and X.Y.Z-xxxxx are supported. The\n  'xxxxx' part, for instance 'homebrew' on OS/X, is ignored.\n\n  Args:\n    version: a version to be converted\n\n  Returns:\n    An integer if converted successfully, otherwise return None.\n  \"\"\"\n  version = version.split('-')[0]\n  version_segments = version.split('.')\n  # Treat \"0.24\" as \"0.24.0\"\n  if len(version_segments) == 2:\n    version_segments.append('0')\n  for seg in version_segments:\n    if not seg.isdigit():\n      return None\n\n  version_str = ''.join(['%03d' % int(seg) for seg in version_segments])\n  return int(version_str)\n\n\ndef retrieve_bazel_version():\n  \"\"\"Retrieve installed bazel version (or bazelisk).\n\n  Returns:\n    The bazel version detected.\n  \"\"\"\n  bazel_executable = shutil.which('bazel')\n  if bazel_executable is None:\n    bazel_executable = shutil.which('bazelisk')\n    if bazel_executable is None:\n      print('Cannot find bazel. Please install bazel/bazelisk.')\n      sys.exit(1)\n\n  stderr = open(os.devnull, 'wb')\n  curr_version = run_shell([bazel_executable, '--version'],\n                           allow_non_zero=True,\n                           stderr=stderr)\n  if curr_version.startswith('bazel '):\n    curr_version = curr_version.split('bazel ')[1]\n\n  curr_version_int = convert_version_to_int(curr_version)\n\n  # Check if current bazel version can be detected properly.\n  if not curr_version_int:\n    print('WARNING: current bazel installation is not a release version.')\n    return curr_version\n\n  print('You have bazel %s installed.' % curr_version)\n  return curr_version\n\n\ndef set_cc_opt_flags(environ_cp):\n  \"\"\"Set up architecture-dependent optimization flags.\n\n  Also append CC optimization flags to bazel.rc..\n\n  Args:\n    environ_cp: copy of the os.environ.\n  \"\"\"\n  if is_ppc64le():\n    # gcc on ppc64le does not support -march, use mcpu instead\n    default_cc_opt_flags = '-mcpu=native'\n  elif is_windows():\n    default_cc_opt_flags = '/arch:AVX'\n  else:\n    # On all other platforms, no longer use `-march=native` as this can result\n    # in instructions that are too modern being generated. Users that want\n    # maximum performance should compile TF in their environment and can pass\n    # `-march=native` there.\n    # See https://github.com/tensorflow/tensorflow/issues/45744 and duplicates\n    default_cc_opt_flags = '-Wno-sign-compare'\n  question = ('Please specify optimization flags to use during compilation when'\n              ' bazel option \"--config=opt\" is specified [Default is %s]: '\n             ) % default_cc_opt_flags\n  cc_opt_flags = get_from_env_or_user_or_default(environ_cp, 'CC_OPT_FLAGS',\n                                                 question, default_cc_opt_flags)\n  for opt in cc_opt_flags.split():\n    write_to_bazelrc('build:opt --copt=%s' % opt)\n    write_to_bazelrc('build:opt --host_copt=%s' % opt)\n\n\ndef set_tf_cuda_clang(environ_cp):\n  \"\"\"set TF_CUDA_CLANG action_env.\n\n  Args:\n    environ_cp: copy of the os.environ.\n  \"\"\"\n  question = 'Do you want to use clang as CUDA compiler?'\n  yes_reply = 'Clang will be used as CUDA compiler.'\n  no_reply = 'nvcc will be used as CUDA compiler.'\n  set_action_env_var(\n      environ_cp,\n      'TF_CUDA_CLANG',\n      None,\n      True,\n      question=question,\n      yes_reply=yes_reply,\n      no_reply=no_reply,\n      bazel_config_name='cuda_clang',\n  )\n\n\ndef set_tf_download_clang(environ_cp):\n  \"\"\"Set TF_DOWNLOAD_CLANG action_env.\"\"\"\n  question = 'Do you wish to download a fresh release of clang? (Experimental)'\n  yes_reply = 'Clang will be downloaded and used to compile tensorflow.'\n  no_reply = 'Clang will not be downloaded.'\n  set_action_env_var(\n      environ_cp,\n      'TF_DOWNLOAD_CLANG',\n      None,\n      False,\n      question=question,\n      yes_reply=yes_reply,\n      no_reply=no_reply,\n      bazel_config_name='download_clang')\n\n\ndef get_from_env_or_user_or_default(environ_cp, var_name, ask_for_var,\n                                    var_default):\n  \"\"\"Get var_name either from env, or user or default.\n\n  If var_name has been set as environment variable, use the preset value, else\n  ask for user input. If no input is provided, the default is used.\n\n  Args:\n    environ_cp: copy of the os.environ.\n    var_name: string for name of environment variable, e.g. \"TF_NEED_CUDA\".\n    ask_for_var: string for how to ask for user input.\n    var_default: default value string.\n\n  Returns:\n    string value for var_name\n  \"\"\"\n  var = environ_cp.get(var_name)\n  if not var:\n    var = get_input(ask_for_var)\n    print('\\n')\n  if not var:\n    var = var_default\n  return var\n\n\ndef prompt_loop_or_load_from_env(environ_cp,\n                                 var_name,\n                                 var_default,\n                                 ask_for_var,\n                                 check_success,\n                                 error_msg,\n                                 suppress_default_error=False,\n                                 resolve_symlinks=False,\n                                 n_ask_attempts=_DEFAULT_PROMPT_ASK_ATTEMPTS):\n  \"\"\"Loop over user prompts for an ENV param until receiving a valid response.\n\n  For the env param var_name, read from the environment or verify user input\n  until receiving valid input. When done, set var_name in the environ_cp to its\n  new value.\n\n  Args:\n    environ_cp: (Dict) copy of the os.environ.\n    var_name: (String) string for name of environment variable, e.g. \"TF_MYVAR\".\n    var_default: (String) default value string.\n    ask_for_var: (String) string for how to ask for user input.\n    check_success: (Function) function that takes one argument and returns a\n      boolean. Should return True if the value provided is considered valid. May\n      contain a complex error message if error_msg does not provide enough\n      information. In that case, set suppress_default_error to True.\n    error_msg: (String) String with one and only one '%s'. Formatted with each\n      invalid response upon check_success(input) failure.\n    suppress_default_error: (Bool) Suppress the above error message in favor of\n      one from the check_success function.\n    resolve_symlinks: (Bool) Translate symbolic links into the real filepath.\n    n_ask_attempts: (Integer) Number of times to query for valid input before\n      raising an error and quitting.\n\n  Returns:\n    [String] The value of var_name after querying for input.\n\n  Raises:\n    UserInputError: if a query has been attempted n_ask_attempts times without\n      success, assume that the user has made a scripting error, and will\n      continue to provide invalid input. Raise the error to avoid infinitely\n      looping.\n  \"\"\"\n  default = environ_cp.get(var_name) or var_default\n  full_query = '%s [Default is %s]: ' % (\n      ask_for_var,\n      default,\n  )\n\n  for _ in range(n_ask_attempts):\n    val = get_from_env_or_user_or_default(environ_cp, var_name, full_query,\n                                          default)\n    if check_success(val):\n      break\n    if not suppress_default_error:\n      print(error_msg % val)\n    environ_cp[var_name] = ''\n  else:\n    raise UserInputError('Invalid %s setting was provided %d times in a row. '\n                         'Assuming to be a scripting mistake.' %\n                         (var_name, n_ask_attempts))\n\n  if resolve_symlinks:\n    val = os.path.realpath(val)\n  environ_cp[var_name] = val\n  return val\n\n\ndef set_clang_cuda_compiler_path(environ_cp):\n  \"\"\"Set CLANG_CUDA_COMPILER_PATH.\"\"\"\n  # Upon clang 19 drop the check for 16\n  default_clang_path = '/usr/lib/llvm-18/bin/clang'\n  if not os.path.exists(default_clang_path):\n    default_clang_path = '/usr/lib/llvm-17/bin/clang'\n    if not os.path.exists(default_clang_path):\n      default_clang_path = '/usr/lib/llvm-16/bin/clang'\n    if not os.path.exists(default_clang_path):\n      default_clang_path = shutil.which('clang') or ''\n\n  clang_cuda_compiler_path = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='CLANG_CUDA_COMPILER_PATH',\n      var_default=default_clang_path,\n      ask_for_var='Please specify clang path that to be used as host compiler.',\n      check_success=os.path.exists,\n      resolve_symlinks=True,\n      error_msg='Invalid clang path. %s cannot be found.',\n  )\n\n  # Set CLANG_CUDA_COMPILER_PATH\n  environ_cp['CLANG_CUDA_COMPILER_PATH'] = clang_cuda_compiler_path\n  write_action_env_to_bazelrc('CLANG_CUDA_COMPILER_PATH',\n                              clang_cuda_compiler_path)\n  return clang_cuda_compiler_path\n\n\ndef create_android_ndk_rule(environ_cp):\n  \"\"\"Set ANDROID_NDK_HOME and write Android NDK WORKSPACE rule.\"\"\"\n  if is_windows() or is_cygwin():\n    default_ndk_path = cygpath('%s/Android/Sdk/ndk-bundle' %\n                               environ_cp['APPDATA'])\n  elif is_macos():\n    default_ndk_path = '%s/library/Android/Sdk/ndk-bundle' % environ_cp['HOME']\n  else:\n    default_ndk_path = '%s/Android/Sdk/ndk-bundle' % environ_cp['HOME']\n\n  def valid_ndk_path(path):\n    return (os.path.exists(path) and\n            os.path.exists(os.path.join(path, 'source.properties')))\n\n  android_ndk_home_path = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='ANDROID_NDK_HOME',\n      var_default=default_ndk_path,\n      ask_for_var='Please specify the home path of the Android NDK to use.',\n      check_success=valid_ndk_path,\n      error_msg=('The path %s or its child file \"source.properties\" '\n                 'does not exist.'))\n  write_action_env_to_bazelrc('ANDROID_NDK_HOME', android_ndk_home_path)\n  write_action_env_to_bazelrc(\n      'ANDROID_NDK_API_LEVEL',\n      get_ndk_api_level(environ_cp, android_ndk_home_path))\n\n\ndef create_android_sdk_rule(environ_cp):\n  \"\"\"Set Android variables and write Android SDK WORKSPACE rule.\"\"\"\n  if is_windows() or is_cygwin():\n    default_sdk_path = cygpath('%s/Android/Sdk' % environ_cp['APPDATA'])\n  elif is_macos():\n    default_sdk_path = '%s/library/Android/Sdk' % environ_cp['HOME']\n  else:\n    default_sdk_path = '%s/Android/Sdk' % environ_cp['HOME']\n\n  def valid_sdk_path(path):\n    return (os.path.exists(path) and\n            os.path.exists(os.path.join(path, 'platforms')) and\n            os.path.exists(os.path.join(path, 'build-tools')))\n\n  android_sdk_home_path = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='ANDROID_SDK_HOME',\n      var_default=default_sdk_path,\n      ask_for_var='Please specify the home path of the Android SDK to use.',\n      check_success=valid_sdk_path,\n      error_msg=('Either %s does not exist, or it does not contain the '\n                 'subdirectories \"platforms\" and \"build-tools\".'))\n\n  platforms = os.path.join(android_sdk_home_path, 'platforms')\n  api_levels = sorted(os.listdir(platforms))\n  api_levels = [x.replace('android-', '') for x in api_levels]\n\n  def valid_api_level(api_level):\n    return os.path.exists(\n        os.path.join(android_sdk_home_path, 'platforms',\n                     'android-' + api_level))\n\n  android_api_level = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='ANDROID_API_LEVEL',\n      var_default=api_levels[-1],\n      ask_for_var=('Please specify the Android SDK API level to use. '\n                   '[Available levels: %s]') % api_levels,\n      check_success=valid_api_level,\n      error_msg='Android-%s is not present in the SDK path.')\n\n  build_tools = os.path.join(android_sdk_home_path, 'build-tools')\n  versions = sorted(os.listdir(build_tools))\n\n  def valid_build_tools(version):\n    return os.path.exists(\n        os.path.join(android_sdk_home_path, 'build-tools', version))\n\n  android_build_tools_version = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='ANDROID_BUILD_TOOLS_VERSION',\n      var_default=versions[-1],\n      ask_for_var=('Please specify an Android build tools version to use. '\n                   '[Available versions: %s]') % versions,\n      check_success=valid_build_tools,\n      error_msg=('The selected SDK does not have build-tools version %s '\n                 'available.'))\n\n  write_action_env_to_bazelrc('ANDROID_BUILD_TOOLS_VERSION',\n                              android_build_tools_version)\n  write_action_env_to_bazelrc('ANDROID_SDK_API_LEVEL', android_api_level)\n  write_action_env_to_bazelrc('ANDROID_SDK_HOME', android_sdk_home_path)\n\n\ndef get_ndk_api_level(environ_cp, android_ndk_home_path):\n  \"\"\"Gets the appropriate NDK API level to use for the provided Android NDK path.\n  \"\"\"\n\n  # First check to see if we're using a blessed version of the NDK.\n  properties_path = '%s/source.properties' % android_ndk_home_path\n  if is_windows() or is_cygwin():\n    properties_path = cygpath(properties_path)\n  with open(properties_path, 'r') as f:\n    filedata = f.read()\n\n  revision = re.search(r'Pkg.Revision = (\\d+)', filedata)\n  if revision:\n    ndk_version = revision.group(1)\n  else:\n    raise Exception('Unable to parse NDK revision.')\n  if int(ndk_version) not in _SUPPORTED_ANDROID_NDK_VERSIONS:\n    print('WARNING: The NDK version in %s is %s, which is not '\n          'supported by Bazel (officially supported versions: %s). Please use '\n          'another version. Compiling Android targets may result in confusing '\n          'errors.\\n' %\n          (android_ndk_home_path, ndk_version, _SUPPORTED_ANDROID_NDK_VERSIONS))\n  write_action_env_to_bazelrc('ANDROID_NDK_VERSION', ndk_version)\n\n  # Now grab the NDK API level to use. Note that this is different from the\n  # SDK API level, as the NDK API level is effectively the *min* target SDK\n  # version.\n  meta = open(os.path.join(android_ndk_home_path, 'meta/platforms.json'))\n  platforms = json.load(meta)\n  meta.close()\n  aliases = platforms['aliases']\n  api_levels = sorted(list(set([aliases[i] for i in aliases])))\n\n  android_ndk_api_level = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='ANDROID_NDK_API_LEVEL',\n      var_default='21',  # 21 is required for ARM64 support.\n      ask_for_var=(\n          'Please specify the (min) Android NDK API level to use. '\n          '[Available levels: %s]'\n      )\n      % api_levels,\n      check_success=(lambda *_: True),\n      error_msg='Android-%s is not present in the NDK path.',\n  )\n\n  return android_ndk_api_level\n\n\ndef set_gcc_host_compiler_path(environ_cp):\n  \"\"\"Set GCC_HOST_COMPILER_PATH.\"\"\"\n  default_gcc_host_compiler_path = shutil.which('gcc') or ''\n\n  gcc_host_compiler_path = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='GCC_HOST_COMPILER_PATH',\n      var_default=default_gcc_host_compiler_path,\n      ask_for_var='Please specify which gcc should be used by nvcc as the host '\n      'compiler.',\n      check_success=os.path.exists,\n      resolve_symlinks=True,\n      error_msg='Invalid gcc path. %s cannot be found.',\n  )\n\n  write_action_env_to_bazelrc('GCC_HOST_COMPILER_PATH', gcc_host_compiler_path)\n\n\ndef choose_compiler(environ_cp):\n  question = 'Do you want to use Clang to build TensorFlow?'\n  yes_reply = 'Clang will be used to compile TensorFlow.'\n  no_reply = 'GCC will be used to compile TensorFlow.'\n  var = int(\n      get_var(\n          environ_cp, 'TF_NEED_CLANG', None, True, question, yes_reply, no_reply\n      )\n  )\n  return var\n\n\ndef choose_compiler_Win(environ_cp):\n  question = 'Do you want to use Clang to build TensorFlow?'\n  yes_reply = 'Add \"--config=win_clang\" to compile TensorFlow with CLANG.'\n  no_reply = 'MSVC will be used to compile TensorFlow.'\n  var = int(\n      get_var(\n          environ_cp, 'TF_NEED_CLANG', None, True, question, yes_reply, no_reply\n      )\n  )\n  return var\n\n\ndef set_clang_compiler_path(environ_cp):\n  \"\"\"Set CLANG_COMPILER_PATH and environment variables.\n\n  Loop over user prompts for clang path until receiving a valid response.\n  Default is used if no input is given. Set CLANG_COMPILER_PATH and write\n  environment variables CC and BAZEL_COMPILER to .bazelrc.\n\n  Args:\n    environ_cp: (Dict) copy of the os.environ.\n\n  Returns:\n    string value for clang_compiler_path.\n  \"\"\"\n  # Default path if clang-18 is installed by using apt-get install\n  # remove 16 logic upon release of 19\n  default_clang_path = '/usr/lib/llvm-18/bin/clang'\n  if not os.path.exists(default_clang_path):\n    default_clang_path = '/usr/lib/llvm-17/bin/clang'\n    if not os.path.exists(default_clang_path):\n      default_clang_path = '/usr/lib/llvm-16/bin/clang'\n    if not os.path.exists(default_clang_path):\n      default_clang_path = shutil.which('clang') or ''\n\n  clang_compiler_path = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='CLANG_COMPILER_PATH',\n      var_default=default_clang_path,\n      ask_for_var='Please specify the path to clang executable.',\n      check_success=os.path.exists,\n      resolve_symlinks=True,\n      error_msg=(\n          'Invalid clang path. %s cannot be found. Note that TensorFlow now'\n          ' requires clang to compile. You may override this behavior by'\n          ' setting TF_NEED_CLANG=0'\n      ),\n  )\n\n  write_action_env_to_bazelrc('CLANG_COMPILER_PATH', clang_compiler_path)\n  write_to_bazelrc('build --repo_env=CC=%s' % clang_compiler_path)\n  write_to_bazelrc('build --repo_env=BAZEL_COMPILER=%s' % clang_compiler_path)\n\n  return clang_compiler_path\n\n\ndef set_clang_compiler_path_win(environ_cp):\n  \"\"\"Set CLANG_COMPILER_PATH and environment variables.\n\n  Loop over user prompts for clang path until receiving a valid response.\n  Default is used if no input is given. Set CLANG_COMPILER_PATH and write\n  environment variables CC and BAZEL_COMPILER to .bazelrc.\n\n  Args:\n    environ_cp: (Dict) copy of the os.environ.\n\n  Returns:\n    string value for clang_compiler_path.\n  \"\"\"\n  # Default path if clang-16 is installed by using apt-get install\n  default_clang_path = 'C:/Program Files/LLVM/bin/clang.exe'\n  if not os.path.exists(default_clang_path):\n    default_clang_path = shutil.which('clang') or ''\n\n  clang_compiler_path = prompt_loop_or_load_from_env(\n      environ_cp,\n      var_name='CLANG_COMPILER_PATH',\n      var_default=default_clang_path,\n      ask_for_var='Please specify the path to clang executable.',\n      check_success=os.path.exists,\n      resolve_symlinks=True,\n      error_msg=(\n          'Invalid clang path. %s cannot be found. Note that Clang is now'\n          'preferred compiler. You may use MSVC by removing --config=win_clang'\n      ),\n  )\n\n  write_action_env_to_bazelrc('CLANG_COMPILER_PATH', clang_compiler_path)\n  write_to_bazelrc(f'build --repo_env=CC=\"{clang_compiler_path}\"')\n  write_to_bazelrc(f'build --repo_env=BAZEL_COMPILER=\"{clang_compiler_path}\"')\n\n  return clang_compiler_path\n\n\ndef retrieve_clang_version(clang_executable):\n  \"\"\"Retrieve installed clang version.\n\n  Args:\n    clang_executable: (String) path to clang executable\n\n  Returns:\n    The clang version detected.\n  \"\"\"\n  stderr = open(os.devnull, 'wb')\n  curr_version = run_shell([clang_executable, '--version'],\n                           allow_non_zero=True,\n                           stderr=stderr)\n\n  curr_version_split = curr_version.lower().split('clang version ')\n  if len(curr_version_split) > 1:\n    curr_version = curr_version_split[1].split()[0].split('git')\n\n  if len(curr_version) > 1:\n    print('WARNING: current clang installation is not a release version.\\n')\n\n  curr_version = curr_version[0]\n  curr_version_int = convert_version_to_int(curr_version)\n  # Check if current clang version can be detected properly.\n  if not curr_version_int:\n    print('WARNING: current clang installation version unknown.\\n')\n    return None\n\n  print('You have Clang %s installed.\\n' % curr_version)\n  return curr_version\n\n\n# Disable clang extension that rejects type definitions within offsetof.\n# This was added in clang-16 by https://reviews.llvm.org/D133574.\n# Still required for clang-17.\n# Can be removed once upb is updated, since a type definition is used within\n# offset of in the current version of ubp. See\n# https://github.com/protocolbuffers/upb/blob/9effcbcb27f0a665f9f345030188c0b291e32482/upb/upb.c#L183.\ndef disable_clang_offsetof_extension(clang_version):\n  if int(clang_version.split('.')[0]) in (16, 17):\n    write_to_bazelrc('build --copt=-Wno-gnu-offsetof-extensions')\n\n\ndef set_hermetic_cuda_version(environ_cp):\n  \"\"\"Set HERMETIC_CUDA_VERSION.\"\"\"\n  ask_cuda_version = (\n      'Please specify the hermetic CUDA version you want to use '\n      'or leave empty to use the default version. '\n  )\n  hermetic_cuda_version = get_from_env_or_user_or_default(\n      environ_cp, 'HERMETIC_CUDA_VERSION', ask_cuda_version, None\n  )\n  if hermetic_cuda_version:\n    environ_cp['HERMETIC_CUDA_VERSION'] = hermetic_cuda_version\n    write_repo_env_to_bazelrc(\n        'cuda', 'HERMETIC_CUDA_VERSION', hermetic_cuda_version\n    )\n\n\ndef set_hermetic_cudnn_version(environ_cp):\n  \"\"\"Set HERMETIC_CUDNN_VERSION.\"\"\"\n  ask_cudnn_version = (\n      'Please specify the hermetic cuDNN version you want to use '\n      'or leave empty to use the default version. '\n  )\n  hermetic_cudnn_version = get_from_env_or_user_or_default(\n      environ_cp, 'HERMETIC_CUDNN_VERSION', ask_cudnn_version, None\n  )\n  if hermetic_cudnn_version:\n    environ_cp['HERMETIC_CUDNN_VERSION'] = hermetic_cudnn_version\n    write_repo_env_to_bazelrc(\n        'cuda', 'HERMETIC_CUDNN_VERSION', hermetic_cudnn_version\n    )\n\n\ndef set_hermetic_cuda_compute_capabilities(environ_cp):\n  \"\"\"Set HERMETIC_CUDA_COMPUTE_CAPABILITIES.\"\"\"\n  while True:\n    default_cuda_compute_capabilities = _DEFAULT_CUDA_COMPUTE_CAPABILITIES\n\n    ask_cuda_compute_capabilities = (\n        'Please specify a list of comma-separated CUDA compute capabilities '\n        'you want to build with.\\nYou can find the compute capability of your '\n        'device at: https://developer.nvidia.com/cuda-gpus. Each capability '\n        'can be specified as \"x.y\" or \"compute_xy\" to include both virtual and'\n        ' binary GPU code, or as \"sm_xy\" to only include the binary '\n        'code.\\nPlease note that each additional compute capability '\n        'significantly increases your build time and binary size, and that '\n        'TensorFlow only supports compute capabilities >= 3.5 [Default is: '\n        '%s]: ' % default_cuda_compute_capabilities)\n    hermetic_cuda_compute_capabilities = get_from_env_or_user_or_default(\n        environ_cp,\n        'HERMETIC_CUDA_COMPUTE_CAPABILITIES',\n        ask_cuda_compute_capabilities,\n        default_cuda_compute_capabilities,\n    )\n    # Check whether all capabilities from the input is valid\n    all_valid = True\n    # Remove all whitespace characters before splitting the string\n    # that users may insert by accident, as this will result in error\n    hermetic_cuda_compute_capabilities = ''.join(\n        hermetic_cuda_compute_capabilities.split()\n    )\n    for compute_capability in hermetic_cuda_compute_capabilities.split(','):\n      m = re.match('[0-9]+.[0-9]+', compute_capability)\n      if not m:\n        # We now support sm_35,sm_50,sm_60,compute_70.\n        sm_compute_match = re.match('(sm|compute)_?([0-9]+[0-9]+)',\n                                    compute_capability)\n        if not sm_compute_match:\n          print('Invalid compute capability: %s' % compute_capability)\n          all_valid = False\n        else:\n          ver = int(sm_compute_match.group(2))\n          if ver < 30:\n            print(\n                'ERROR: TensorFlow only supports small CUDA compute'\n                ' capabilities of sm_30 and higher. Please re-specify the list'\n                ' of compute capabilities excluding version %s.' % ver)\n            all_valid = False\n          if ver < 35:\n            print('WARNING: XLA does not support CUDA compute capabilities '\n                  'lower than sm_35. Disable XLA when running on older GPUs.')\n      else:\n        ver = float(m.group(0))\n        if ver < 3.0:\n          print('ERROR: TensorFlow only supports CUDA compute capabilities 3.0 '\n                'and higher. Please re-specify the list of compute '\n                'capabilities excluding version %s.' % ver)\n          all_valid = False\n        if ver < 3.5:\n          print('WARNING: XLA does not support CUDA compute capabilities '\n                'lower than 3.5. Disable XLA when running on older GPUs.')\n\n    if all_valid:\n      break\n\n    # Reset and Retry\n    environ_cp['HERMETIC_CUDA_COMPUTE_CAPABILITIES'] = ''\n\n  # Set HERMETIC_CUDA_COMPUTE_CAPABILITIES\n  environ_cp['HERMETIC_CUDA_COMPUTE_CAPABILITIES'] = (\n      hermetic_cuda_compute_capabilities\n  )\n  write_repo_env_to_bazelrc(\n      'cuda',\n      'HERMETIC_CUDA_COMPUTE_CAPABILITIES',\n      hermetic_cuda_compute_capabilities,\n  )\n\n\ndef set_cuda_local_path(environ_cp, dist_name, env_var):\n  ask_path = (\n      'Please specify the local {} path you want to use '\n      'or leave empty to use the default version. '\n  ).format(dist_name)\n  local_path = get_from_env_or_user_or_default(\n      environ_cp, env_var, ask_path, None\n  )\n  if local_path:\n    environ_cp[env_var] = local_path\n    write_repo_env_to_bazelrc('cuda', env_var, local_path)\n\n\ndef set_other_cuda_vars(environ_cp):\n  \"\"\"Set other CUDA related variables.\"\"\"\n  # If CUDA is enabled, always use GPU during build and test.\n  if environ_cp.get('TF_CUDA_CLANG') == '1':\n    write_to_bazelrc('build --config=cuda_clang')\n  else:\n    write_to_bazelrc('build --config=cuda')\n\n\ndef system_specific_test_config(environ_cp):\n  \"\"\"Add default build and test flags required for TF tests to bazelrc.\"\"\"\n  write_to_bazelrc('test --test_size_filters=small,medium')\n\n  # Each instance of --test_tag_filters or --build_tag_filters overrides all\n  # previous instances, so we need to build up a complete list and write a\n  # single list of filters for the .bazelrc file.\n\n  # Filters to use with both --test_tag_filters and --build_tag_filters\n  test_and_build_filters = ['-benchmark-test', '-no_oss', '-oss_excluded']\n  # Additional filters for --test_tag_filters beyond those in\n  # test_and_build_filters\n  test_only_filters = ['-oss_serial']\n  if is_windows():\n    test_and_build_filters += ['-no_windows', '-windows_excluded']\n    if ((environ_cp.get('TF_NEED_CUDA', None) == '1') or\n        (environ_cp.get('TF_NEED_ROCM', None) == '1')):\n      test_and_build_filters += ['-no_windows_gpu', '-no_gpu']\n    else:\n      test_and_build_filters.append('-gpu')\n  elif is_macos():\n    test_and_build_filters += ['-gpu', '-nomac', '-no_mac', '-mac_excluded']\n  elif is_linux():\n    if ((environ_cp.get('TF_NEED_CUDA', None) == '1') or\n        (environ_cp.get('TF_NEED_ROCM', None) == '1')):\n      test_and_build_filters.append('-no_gpu')\n      write_to_bazelrc('test --test_env=LD_LIBRARY_PATH')\n    else:\n      test_and_build_filters.append('-gpu')\n\n  # Disable tests with \"v1only\" tag in \"v2\" Bazel config, but not in \"v1\" config\n  write_to_bazelrc('test:v1 --test_tag_filters=%s' %\n                   ','.join(test_and_build_filters + test_only_filters))\n  write_to_bazelrc('test:v1 --build_tag_filters=%s' %\n                   ','.join(test_and_build_filters))\n  write_to_bazelrc(\n      'test:v2 --test_tag_filters=%s' %\n      ','.join(test_and_build_filters + test_only_filters + ['-v1only']))\n  write_to_bazelrc('test:v2 --build_tag_filters=%s' %\n                   ','.join(test_and_build_filters + ['-v1only']))\n\n\ndef set_system_libs_flag(environ_cp):\n  \"\"\"Set system libs flags.\"\"\"\n  syslibs = environ_cp.get('TF_SYSTEM_LIBS', '')\n\n  if is_s390x() and 'boringssl' not in syslibs:\n    syslibs = 'boringssl' + (', ' + syslibs if syslibs else '')\n\n  if syslibs:\n    if ',' in syslibs:\n      syslibs = ','.join(sorted(syslibs.split(',')))\n    else:\n      syslibs = ','.join(sorted(syslibs.split()))\n    write_action_env_to_bazelrc('TF_SYSTEM_LIBS', syslibs)\n\n  for varname in ('PREFIX', 'LIBDIR', 'INCLUDEDIR', 'PROTOBUF_INCLUDE_PATH'):\n    if varname in environ_cp:\n      write_to_bazelrc('build --define=%s=%s' % (varname, environ_cp[varname]))\n\n\ndef set_windows_build_flags(environ_cp):\n  \"\"\"Set Windows specific build options.\"\"\"\n\n  # First available in VS 16.4. Speeds up Windows compile times by a lot. See\n  # https://groups.google.com/a/tensorflow.org/d/topic/build/SsW98Eo7l3o/discussion\n  # pylint: disable=line-too-long\n  write_to_bazelrc(\n      'build --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions'\n  )\n\n  if get_var(\n      environ_cp, 'TF_OVERRIDE_EIGEN_STRONG_INLINE', 'Eigen strong inline',\n      True, ('Would you like to override eigen strong inline for some C++ '\n             'compilation to reduce the compilation time?'),\n      'Eigen strong inline overridden.', 'Not overriding eigen strong inline, '\n      'some compilations could take more than 20 mins.'):\n    # Due to a known MSVC compiler issue\n    # https://github.com/tensorflow/tensorflow/issues/10521\n    # Overriding eigen strong inline speeds up the compiling of\n    # conv_grad_ops_3d.cc and conv_ops_3d.cc by 20 minutes,\n    # but this also hurts the performance. Let users decide what they want.\n    write_to_bazelrc('build --define=override_eigen_strong_inline=true')\n\n\ndef config_info_line(name, help_text):\n  \"\"\"Helper function to print formatted help text for Bazel config options.\"\"\"\n  print('\\t--config=%-12s\\t# %s' % (name, help_text))\n\n\ndef configure_ios(environ_cp):\n  \"\"\"Configures TensorFlow for iOS builds.\"\"\"\n  if not is_macos():\n    return\n  if not get_var(environ_cp, 'TF_CONFIGURE_IOS', 'iOS', False):\n    return\n  for filepath in APPLE_BAZEL_FILES:\n    existing_filepath = os.path.join(_TF_WORKSPACE_ROOT, filepath + '.apple')\n    renamed_filepath = os.path.join(_TF_WORKSPACE_ROOT, filepath)\n    symlink_force(existing_filepath, renamed_filepath)\n  for filepath in IOS_FILES:\n    filename = os.path.basename(filepath)\n    new_filepath = os.path.join(_TF_WORKSPACE_ROOT, filename)\n    symlink_force(filepath, new_filepath)\n\n\ndef get_gcc_compiler(environ_cp):\n  gcc_env = environ_cp.get('CXX') or environ_cp.get('CC') or shutil.which('gcc')\n  if gcc_env is not None:\n    gcc_version = run_shell([gcc_env, '--version']).split()\n    if gcc_version[0] in ('gcc', 'g++'):\n      return gcc_env\n  return None\n\n\ndef main():\n  global _TF_WORKSPACE_ROOT\n  global _TF_BAZELRC\n  global _TF_CURRENT_BAZEL_VERSION\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '--workspace',\n      type=str,\n      default=os.path.abspath(os.path.dirname(__file__)),\n      help='The absolute path to your active Bazel workspace.')\n  args = parser.parse_args()\n\n  _TF_WORKSPACE_ROOT = args.workspace\n  _TF_BAZELRC = os.path.join(_TF_WORKSPACE_ROOT, _TF_BAZELRC_FILENAME)\n\n  # Make a copy of os.environ to be clear when functions and getting and setting\n  # environment variables.\n  environ_cp = dict(os.environ)\n\n  try:\n    current_bazel_version = retrieve_bazel_version()\n  except subprocess.CalledProcessError as e:\n    print('Error retrieving bazel version: ', e.output.decode('UTF-8').strip())\n    raise e\n\n  _TF_CURRENT_BAZEL_VERSION = convert_version_to_int(current_bazel_version)\n\n  reset_tf_configure_bazelrc()\n\n  cleanup_makefile()\n  setup_python(environ_cp)\n\n  if is_windows():\n    environ_cp['TF_NEED_OPENCL'] = '0'\n    environ_cp['TF_CUDA_CLANG'] = '0'\n    # TODO(ibiryukov): Investigate using clang as a cpu or cuda compiler on\n    # Windows.\n    environ_cp['TF_DOWNLOAD_CLANG'] = '0'\n    environ_cp['TF_NEED_MPI'] = '0'\n\n  if is_ppc64le():\n    # Enable MMA Dynamic Dispatch support if 'gcc' and if linker >= 2.35\n    gcc_env = get_gcc_compiler(environ_cp)\n    if gcc_env is not None:\n\n      # Use gold linker if 'gcc' and if 'ppc64le'\n      write_to_bazelrc('build --linkopt=\"-fuse-ld=gold\"')\n\n      # Get the linker version\n      ld_version = run_shell([gcc_env, '-Wl,-version']).split()\n\n      ld_version_int = 0\n      for i in range(len(ld_version)):\n        ld_version_int = convert_version_to_int(ld_version[i])\n        if ld_version_int is not None:\n          break\n\n      if ld_version_int is None:\n        ld_version_int = 0\n\n      # Enable if 'ld' version >= 2.35\n      if ld_version_int >= 2035000:\n        write_to_bazelrc(\n            'build --copt=\"-DEIGEN_ALTIVEC_ENABLE_MMA_DYNAMIC_DISPATCH=1\"')\n\n  with_xla_support = environ_cp.get('TF_ENABLE_XLA', None)\n  if with_xla_support is not None:\n    write_to_bazelrc('build --define=with_xla_support=%s' %\n                     ('true' if int(with_xla_support) else 'false'))\n\n  set_action_env_var(\n      environ_cp, 'TF_NEED_ROCM', 'ROCm', False, bazel_config_name='rocm')\n  if (environ_cp.get('TF_NEED_ROCM') == '1' and\n      'LD_LIBRARY_PATH' in environ_cp and\n      environ_cp.get('LD_LIBRARY_PATH') != '1'):\n    write_action_env_to_bazelrc('LD_LIBRARY_PATH',\n                                environ_cp.get('LD_LIBRARY_PATH'))\n\n  if (environ_cp.get('TF_NEED_ROCM') == '1' and environ_cp.get('ROCM_PATH')):\n    write_action_env_to_bazelrc('ROCM_PATH', environ_cp.get('ROCM_PATH'))\n\n  if (environ_cp.get('TF_NEED_ROCM') == '1' and environ_cp.get('HIP_PLATFORM')):\n    write_action_env_to_bazelrc('HIP_PLATFORM', environ_cp.get('HIP_PLATFORM'))\n\n  if is_windows():\n    print('\\nWARNING: Cannot build with CUDA support on Windows.\\n'\n          'Starting in TF 2.11, CUDA build is not supported for Windows. '\n          'For using TensorFlow GPU on Windows, you will need to build/install '\n          'TensorFlow in WSL2.\\n')\n    environ_cp['TF_NEED_CUDA'] = '0'\n  else:\n    environ_cp['TF_NEED_CUDA'] = str(\n        int(get_var(environ_cp, 'TF_NEED_CUDA', 'CUDA', False)))\n  if environ_cp.get('TF_NEED_CUDA') == '1':\n    set_hermetic_cuda_version(environ_cp)\n    set_hermetic_cudnn_version(environ_cp)\n    set_hermetic_cuda_compute_capabilities(environ_cp)\n    set_cuda_local_path(environ_cp, 'CUDA', 'LOCAL_CUDA_PATH')\n    set_cuda_local_path(environ_cp, 'CUDNN', 'LOCAL_CUDNN_PATH')\n    set_cuda_local_path(environ_cp, 'NCCL', 'LOCAL_NCCL_PATH')\n\n    if 'LD_LIBRARY_PATH' in environ_cp and environ_cp.get(\n        'LD_LIBRARY_PATH') != '1':\n      write_action_env_to_bazelrc('LD_LIBRARY_PATH',\n                                  environ_cp.get('LD_LIBRARY_PATH'))\n\n    set_tf_cuda_clang(environ_cp)\n    if environ_cp.get('TF_CUDA_CLANG') == '1':\n      # Set up which clang we should use as the cuda / host compiler.\n      clang_cuda_compiler_path = set_clang_cuda_compiler_path(environ_cp)\n      clang_version = retrieve_clang_version(clang_cuda_compiler_path)\n      disable_clang_offsetof_extension(clang_version)\n    else:\n      # Set up which gcc nvcc should use as the host compiler\n      # No need to set this on Windows\n      if not is_windows():\n        set_gcc_host_compiler_path(environ_cp)\n    set_other_cuda_vars(environ_cp)\n  else:\n    # CUDA not required. Ask whether we should use clang for the CPU build.\n    if is_linux():\n      environ_cp['TF_NEED_CLANG'] = str(choose_compiler(environ_cp))\n      if environ_cp.get('TF_NEED_CLANG') == '1':\n        clang_compiler_path = set_clang_compiler_path(environ_cp)\n        clang_version = retrieve_clang_version(clang_compiler_path)\n        disable_clang_offsetof_extension(clang_version)\n    if is_windows():\n      environ_cp['TF_NEED_CLANG'] = str(choose_compiler_Win(environ_cp))\n      if environ_cp.get('TF_NEED_CLANG') == '1':\n        clang_compiler_path = set_clang_compiler_path_win(environ_cp)\n        clang_version = retrieve_clang_version(clang_compiler_path)\n        disable_clang_offsetof_extension(clang_version)\n\n  # ROCm / CUDA are mutually exclusive.\n  # At most 1 GPU platform can be configured.\n  gpu_platform_count = 0\n  if environ_cp.get('TF_NEED_ROCM') == '1':\n    gpu_platform_count += 1\n  if environ_cp.get('TF_NEED_CUDA') == '1':\n    gpu_platform_count += 1\n  if gpu_platform_count >= 2:\n    raise UserInputError('CUDA / ROCm are mututally exclusive. '\n                         'At most 1 GPU platform can be configured.')\n\n  set_cc_opt_flags(environ_cp)\n  set_system_libs_flag(environ_cp)\n  if is_windows():\n    set_windows_build_flags(environ_cp)\n\n  if get_var(environ_cp, 'TF_SET_ANDROID_WORKSPACE', 'android workspace', False,\n             ('Would you like to interactively configure ./WORKSPACE for '\n              'Android builds?'), 'Searching for NDK and SDK installations.',\n             'Not configuring the WORKSPACE for Android builds.'):\n    create_android_ndk_rule(environ_cp)\n    create_android_sdk_rule(environ_cp)\n\n  system_specific_test_config(environ_cp)\n\n  configure_ios(environ_cp)\n\n  print('Preconfigured Bazel build configs. You can use any of the below by '\n        'adding \"--config=<>\" to your build command. See .bazelrc for more '\n        'details.')\n  config_info_line('mkl', 'Build with MKL support.')\n  config_info_line(\n      'mkl_aarch64',\n      'Build with oneDNN and Compute Library for the Arm Architecture (ACL).')\n  config_info_line('monolithic', 'Config for mostly static monolithic build.')\n  config_info_line('numa', 'Build with NUMA support.')\n  config_info_line(\n      'dynamic_kernels',\n      '(Experimental) Build kernels into separate shared objects.')\n  config_info_line('v1', 'Build with TensorFlow 1 API instead of TF 2 API.')\n\n  print('Preconfigured Bazel build configs to DISABLE default on features:')\n  config_info_line('nogcp', 'Disable GCP support.')\n  config_info_line('nonccl', 'Disable NVIDIA NCCL support.')\n\n\nif __name__ == '__main__':\n  main()\n",
        "imports": [
            "user.",
            "site",
            "platform",
            "posix",
            "env",
            "the",
            "sys",
            "json",
            "re",
            "a",
            "sysconfig",
            "os",
            "errno",
            "shutil",
            "argparse",
            "subprocess"
        ],
        "functions": [
            "set_windows_build_flags",
            "setup_python",
            "disable_clang_offsetof_extension",
            "set_other_cuda_vars",
            "is_s390x",
            "get_from_env_or_user_or_default",
            "create_android_sdk_rule",
            "main",
            "write_to_bazelrc",
            "get_python_path",
            "retrieve_clang_version",
            "get_ndk_api_level",
            "is_macos",
            "choose_compiler",
            "set_cuda_local_path",
            "get_gcc_compiler",
            "config_info_line",
            "set_tf_cuda_clang",
            "cleanup_makefile",
            "valid_api_level",
            "run_shell",
            "symlink_force",
            "is_linux",
            "is_cygwin",
            "cygpath",
            "choose_compiler_Win",
            "set_hermetic_cuda_version",
            "write_repo_env_to_bazelrc",
            "set_gcc_host_compiler_path",
            "valid_build_tools",
            "set_hermetic_cuda_compute_capabilities",
            "set_tf_download_clang",
            "set_clang_compiler_path_win",
            "create_android_ndk_rule",
            "set_system_libs_flag",
            "set_action_env_var",
            "set_clang_cuda_compiler_path",
            "get_input",
            "valid_ndk_path",
            "reset_tf_configure_bazelrc",
            "set_hermetic_cudnn_version",
            "configure_ios",
            "get_python_major_version",
            "get_var",
            "convert_version_to_int",
            "set_cc_opt_flags",
            "prompt_loop_or_load_from_env",
            "set_clang_compiler_path",
            "is_windows",
            "system_specific_test_config",
            "retrieve_bazel_version",
            "write_action_env_to_bazelrc",
            "valid_sdk_path",
            "is_ppc64le"
        ],
        "variables": [
            "help",
            "python_paths",
            "var",
            "val",
            "ver",
            "errno",
            "_TF_BAZELRC",
            "march",
            "NCCL_LIB_PATHS",
            "resolve_symlinks",
            "parser",
            "android_sdk_home_path",
            "local_path",
            "test_only_filters",
            "error_msg",
            "gcc_env",
            "default_python_bin_path",
            "allow_non_zero",
            "android_ndk_api_level",
            "user_input_origin",
            "meta",
            "clang_cuda_compiler_path",
            "all_valid",
            "type",
            "bazel_config_name",
            "library_paths",
            "clang_compiler_path",
            "test_tag_filters",
            "versions",
            "android_api_level",
            "default",
            "default_sdk_path",
            "default_clang_path",
            "curr_version_int",
            "_DEFAULT_PROMPT_ASK_ATTEMPTS",
            "hermetic_cudnn_version",
            "n_ask_attempts",
            "filename",
            "_TF_BAZELRC_FILENAME",
            "sm_compute_match",
            "IOS_FILES",
            "_SUPPORTED_ANDROID_NDK_VERSIONS",
            "version",
            "cc_opt_flags",
            "hermetic_cuda_version",
            "clang_version",
            "syslibs",
            "host_copt",
            "true_strings",
            "copt",
            "question",
            "args",
            "ask_cudnn_version",
            "m",
            "_DEFAULT_CUDA_COMPUTE_CAPABILITIES",
            "user_input",
            "var_default",
            "test_and_build_filters",
            "var_content",
            "android_build_tools_version",
            "repo_env",
            "full_query",
            "answer",
            "ndk_version",
            "ask_for_var",
            "android_ndk_home_path",
            "version_segments",
            "disable",
            "PYTHON_BIN_PATH",
            "ask_path",
            "var_name",
            "with_xla_support",
            "ask_python_bin_path",
            "gpu_platform_count",
            "suppress_default_error",
            "gcc_host_compiler_path",
            "revision",
            "current_bazel_version",
            "python_path",
            "version_str",
            "TF_NEED_CLANG",
            "makefile_download_dir",
            "_TF_CURRENT_BAZEL_VERSION",
            "host_force_python",
            "ask_cuda_version",
            "filedata",
            "properties_path",
            "python_major_version",
            "default_cuda_compute_capabilities",
            "ld_version_int",
            "build_tools",
            "linkopt",
            "APPLE_BAZEL_FILES",
            "stderr",
            "gcc_version",
            "paths",
            "environ_cp",
            "_TF_WORKSPACE_ROOT",
            "existing_filepath",
            "new_filepath",
            "aliases",
            "hermetic_cuda_compute_capabilities",
            "false_strings",
            "python_lib_paths",
            "curr_version_split",
            "define",
            "build_tag_filters",
            "platforms",
            "__name__",
            "api_levels",
            "curr_version",
            "renamed_filepath",
            "default_cc_opt_flags",
            "config",
            "no_reply",
            "python_bin_path",
            "all_paths",
            "ask_cuda_compute_capabilities",
            "default_ndk_path",
            "default_python_lib_path",
            "test_env",
            "yes_reply",
            "check_success",
            "default_gcc_host_compiler_path",
            "output",
            "test_size_filters",
            "bazel_executable",
            "python_lib_path",
            "ld_version"
        ]
    },
    {
        "file_name": "gles3_builders.py",
        "language": "python",
        "source_code": "\"\"\"Functions used to generate source files during build time\"\"\"\n\nimport os.path\nfrom typing import Optional\n\nfrom methods import print_error, to_raw_cstring\n\n\nclass GLES3HeaderStruct:\n    def __init__(self):\n        self.vertex_lines = []\n        self.fragment_lines = []\n        self.uniforms = []\n        self.fbos = []\n        self.texunits = []\n        self.texunit_names = []\n        self.ubos = []\n        self.ubo_names = []\n        self.feedbacks = []\n\n        self.vertex_included_files = []\n        self.fragment_included_files = []\n\n        self.reading = \"\"\n        self.line_offset = 0\n        self.vertex_offset = 0\n        self.fragment_offset = 0\n        self.variant_defines = []\n        self.variant_names = []\n        self.specialization_names = []\n        self.specialization_values = []\n\n\ndef include_file_in_gles3_header(filename: str, header_data: GLES3HeaderStruct, depth: int):\n    with open(filename, \"r\", encoding=\"utf-8\") as fs:\n        line = fs.readline()\n\n        while line:\n            if line.find(\"=\") != -1 and header_data.reading == \"\":\n                # Mode\n                eqpos = line.find(\"=\")\n                defname = line[:eqpos].strip().upper()\n                define = line[eqpos + 1 :].strip()\n                header_data.variant_names.append(defname)\n                header_data.variant_defines.append(define)\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.vertex_offset = header_data.line_offset\n                continue\n\n            if line.find(\"=\") != -1 and header_data.reading == \"specializations\":\n                # Specialization\n                eqpos = line.find(\"=\")\n                specname = line[:eqpos].strip()\n                specvalue = line[eqpos + 1 :]\n                header_data.specialization_names.append(specname)\n                header_data.specialization_values.append(specvalue)\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.vertex_offset = header_data.line_offset\n                continue\n\n            if line.find(\"#[modes]\") != -1:\n                # Nothing really, just skip\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.vertex_offset = header_data.line_offset\n                continue\n\n            if line.find(\"#[specializations]\") != -1:\n                header_data.reading = \"specializations\"\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.vertex_offset = header_data.line_offset\n                continue\n\n            if line.find(\"#[vertex]\") != -1:\n                header_data.reading = \"vertex\"\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.vertex_offset = header_data.line_offset\n                continue\n\n            if line.find(\"#[fragment]\") != -1:\n                header_data.reading = \"fragment\"\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.fragment_offset = header_data.line_offset\n                continue\n\n            while line.find(\"#include \") != -1:\n                includeline = line.replace(\"#include \", \"\").strip()[1:-1]\n\n                included_file = os.path.relpath(os.path.dirname(filename) + \"/\" + includeline)\n                if included_file not in header_data.vertex_included_files and header_data.reading == \"vertex\":\n                    header_data.vertex_included_files += [included_file]\n                    if include_file_in_gles3_header(included_file, header_data, depth + 1) is None:\n                        print_error(f'In file \"{filename}\": #include \"{includeline}\" could not be found!\"')\n                elif included_file not in header_data.fragment_included_files and header_data.reading == \"fragment\":\n                    header_data.fragment_included_files += [included_file]\n                    if include_file_in_gles3_header(included_file, header_data, depth + 1) is None:\n                        print_error(f'In file \"{filename}\": #include \"{includeline}\" could not be found!\"')\n\n                line = fs.readline()\n\n            if line.find(\"uniform\") != -1 and line.lower().find(\"texunit:\") != -1:\n                # texture unit\n                texunitstr = line[line.find(\":\") + 1 :].strip()\n                if texunitstr == \"auto\":\n                    texunit = \"-1\"\n                else:\n                    texunit = str(int(texunitstr))\n                uline = line[: line.lower().find(\"//\")]\n                uline = uline.replace(\"uniform\", \"\")\n                uline = uline.replace(\"highp\", \"\")\n                uline = uline.replace(\";\", \"\")\n                lines = uline.split(\",\")\n                for x in lines:\n                    x = x.strip()\n                    x = x[x.rfind(\" \") + 1 :]\n                    if x.find(\"[\") != -1:\n                        # unfiorm array\n                        x = x[: x.find(\"[\")]\n\n                    if x not in header_data.texunit_names:\n                        header_data.texunits += [(x, texunit)]\n                        header_data.texunit_names += [x]\n\n            elif line.find(\"uniform\") != -1 and line.lower().find(\"ubo:\") != -1:\n                # uniform buffer object\n                ubostr = line[line.find(\":\") + 1 :].strip()\n                ubo = str(int(ubostr))\n                uline = line[: line.lower().find(\"//\")]\n                uline = uline[uline.find(\"uniform\") + len(\"uniform\") :]\n                uline = uline.replace(\"highp\", \"\")\n                uline = uline.replace(\";\", \"\")\n                uline = uline.replace(\"{\", \"\").strip()\n                lines = uline.split(\",\")\n                for x in lines:\n                    x = x.strip()\n                    x = x[x.rfind(\" \") + 1 :]\n                    if x.find(\"[\") != -1:\n                        # unfiorm array\n                        x = x[: x.find(\"[\")]\n\n                    if x not in header_data.ubo_names:\n                        header_data.ubos += [(x, ubo)]\n                        header_data.ubo_names += [x]\n\n            elif line.find(\"uniform\") != -1 and line.find(\"{\") == -1 and line.find(\";\") != -1:\n                uline = line.replace(\"uniform\", \"\")\n                uline = uline.replace(\";\", \"\")\n                lines = uline.split(\",\")\n                for x in lines:\n                    x = x.strip()\n                    x = x[x.rfind(\" \") + 1 :]\n                    if x.find(\"[\") != -1:\n                        # unfiorm array\n                        x = x[: x.find(\"[\")]\n\n                    if x not in header_data.uniforms:\n                        header_data.uniforms += [x]\n\n            if (line.strip().find(\"out \") == 0 or line.strip().find(\"flat \") == 0) and line.find(\"tfb:\") != -1:\n                uline = line.replace(\"flat \", \"\")\n                uline = uline.replace(\"out \", \"\")\n                uline = uline.replace(\"highp \", \"\")\n                uline = uline.replace(\";\", \"\")\n                uline = uline[uline.find(\" \") :].strip()\n\n                if uline.find(\"//\") != -1:\n                    name, bind = uline.split(\"//\")\n                    if bind.find(\"tfb:\") != -1:\n                        name = name.strip()\n                        bind = bind.replace(\"tfb:\", \"\").strip()\n                        header_data.feedbacks += [(name, bind)]\n\n            line = line.replace(\"\\r\", \"\")\n            line = line.replace(\"\\n\", \"\")\n\n            if header_data.reading == \"vertex\":\n                header_data.vertex_lines += [line]\n            if header_data.reading == \"fragment\":\n                header_data.fragment_lines += [line]\n\n            line = fs.readline()\n            header_data.line_offset += 1\n\n    return header_data\n\n\ndef build_gles3_header(\n    filename: str,\n    include: str,\n    class_suffix: str,\n    optional_output_filename: Optional[str] = None,\n    header_data: Optional[GLES3HeaderStruct] = None,\n):\n    header_data = header_data or GLES3HeaderStruct()\n    include_file_in_gles3_header(filename, header_data, 0)\n\n    if optional_output_filename is None:\n        out_file = filename + \".gen.h\"\n    else:\n        out_file = optional_output_filename\n\n    with open(out_file, \"w\", encoding=\"utf-8\", newline=\"\\n\") as fd:\n        defspec = 0\n        defvariant = \"\"\n\n        fd.write(\"/* WARNING, THIS FILE WAS GENERATED, DO NOT EDIT */\\n\")\n\n        out_file_base = out_file\n        out_file_base = out_file_base[out_file_base.rfind(\"/\") + 1 :]\n        out_file_base = out_file_base[out_file_base.rfind(\"\\\\\") + 1 :]\n        out_file_ifdef = out_file_base.replace(\".\", \"_\").upper()\n        fd.write(\"#ifndef \" + out_file_ifdef + class_suffix + \"_GLES3\\n\")\n        fd.write(\"#define \" + out_file_ifdef + class_suffix + \"_GLES3\\n\")\n\n        out_file_class = (\n            out_file_base.replace(\".glsl.gen.h\", \"\").title().replace(\"_\", \"\").replace(\".\", \"\") + \"Shader\" + class_suffix\n        )\n        fd.write(\"\\n\\n\")\n        fd.write('#include \"' + include + '\"\\n\\n\\n')\n        fd.write(\"class \" + out_file_class + \" : public Shader\" + class_suffix + \" {\\n\\n\")\n\n        fd.write(\"public:\\n\\n\")\n\n        if header_data.uniforms:\n            fd.write(\"\\tenum Uniforms {\\n\")\n            for x in header_data.uniforms:\n                fd.write(\"\\t\\t\" + x.upper() + \",\\n\")\n            fd.write(\"\\t};\\n\\n\")\n\n        if header_data.variant_names:\n            fd.write(\"\\tenum ShaderVariant {\\n\")\n            for x in header_data.variant_names:\n                fd.write(\"\\t\\t\" + x + \",\\n\")\n            fd.write(\"\\t};\\n\\n\")\n        else:\n            fd.write(\"\\tenum ShaderVariant { DEFAULT };\\n\\n\")\n            defvariant = \"=DEFAULT\"\n\n        if header_data.specialization_names:\n            fd.write(\"\\tenum Specializations {\\n\")\n            counter = 0\n            for x in header_data.specialization_names:\n                fd.write(\"\\t\\t\" + x.upper() + \"=\" + str(1 << counter) + \",\\n\")\n                counter += 1\n            fd.write(\"\\t};\\n\\n\")\n\n        for i in range(len(header_data.specialization_names)):\n            defval = header_data.specialization_values[i].strip()\n            if defval.upper() == \"TRUE\" or defval == \"1\":\n                defspec |= 1 << i\n\n        fd.write(\n            \"\\t_FORCE_INLINE_ bool version_bind_shader(RID p_version,ShaderVariant p_variant\"\n            + defvariant\n            + \",uint64_t p_specialization=\"\n            + str(defspec)\n            + \") { return _version_bind_shader(p_version,p_variant,p_specialization); }\\n\\n\"\n        )\n\n        if header_data.uniforms:\n            fd.write(\n                \"\\t_FORCE_INLINE_ int version_get_uniform(Uniforms p_uniform,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { return _version_get_uniform(p_uniform,p_version,p_variant,p_specialization); }\\n\\n\"\n            )\n\n            fd.write(\n                \"\\t#define _FU if (version_get_uniform(p_uniform,p_version,p_variant,p_specialization)<0) return; \\n\\n \"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, float p_value,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform1f(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_value); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, double p_value,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform1f(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_value); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, uint8_t p_value,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform1ui(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_value); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, int8_t p_value,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform1i(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_value); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, uint16_t p_value,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform1ui(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_value); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, int16_t p_value,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform1i(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_value); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, uint32_t p_value,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform1ui(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_value); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, int32_t p_value,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform1i(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_value); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, const Color& p_color,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU GLfloat col[4]={p_color.r,p_color.g,p_color.b,p_color.a}; glUniform4fv(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),1,col); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, const Vector2& p_vec2,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU GLfloat vec2[2]={float(p_vec2.x),float(p_vec2.y)}; glUniform2fv(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),1,vec2); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, const Size2i& p_vec2,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU GLint vec2[2]={GLint(p_vec2.x),GLint(p_vec2.y)}; glUniform2iv(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),1,vec2); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, const Vector3& p_vec3,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU GLfloat vec3[3]={float(p_vec3.x),float(p_vec3.y),float(p_vec3.z)}; glUniform3fv(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),1,vec3); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, const Vector4& p_vec4,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU GLfloat vec4[4]={float(p_vec4.x),float(p_vec4.y),float(p_vec4.z),float(p_vec4.w)}; glUniform4fv(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),1,vec4); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, float p_a, float p_b,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform2f(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_a,p_b); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, float p_a, float p_b, float p_c,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform3f(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_a,p_b,p_c); }\\n\\n\"\n            )\n            fd.write(\n                \"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, float p_a, float p_b, float p_c, float p_d,RID p_version,ShaderVariant p_variant\"\n                + defvariant\n                + \",uint64_t p_specialization=\"\n                + str(defspec)\n                + \") { _FU glUniform4f(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),p_a,p_b,p_c,p_d); }\\n\\n\"\n            )\n\n            fd.write(\n                \"\"\"\\t_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, const Transform3D& p_transform,RID p_version,ShaderVariant p_variant\"\"\"\n                + defvariant\n                + \"\"\",uint64_t p_specialization=\"\"\"\n                + str(defspec)\n                + \"\"\") {  _FU\n\n                const Transform3D &tr = p_transform;\n\n                GLfloat matrix[16]={ /* build a 16x16 matrix */\n                    (GLfloat)tr.basis.rows[0][0],\n                    (GLfloat)tr.basis.rows[1][0],\n                    (GLfloat)tr.basis.rows[2][0],\n                    (GLfloat)0,\n                    (GLfloat)tr.basis.rows[0][1],\n                    (GLfloat)tr.basis.rows[1][1],\n                    (GLfloat)tr.basis.rows[2][1],\n                    (GLfloat)0,\n                    (GLfloat)tr.basis.rows[0][2],\n                    (GLfloat)tr.basis.rows[1][2],\n                    (GLfloat)tr.basis.rows[2][2],\n                    (GLfloat)0,\n                    (GLfloat)tr.origin.x,\n                    (GLfloat)tr.origin.y,\n                    (GLfloat)tr.origin.z,\n                    (GLfloat)1\n                };\n\n                        glUniformMatrix4fv(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),1,false,matrix);\n\n            }\n\n            \"\"\"\n            )\n\n            fd.write(\n                \"\"\"_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, const Transform2D& p_transform,RID p_version,ShaderVariant p_variant\"\"\"\n                + defvariant\n                + \"\"\",uint64_t p_specialization=\"\"\"\n                + str(defspec)\n                + \"\"\") {  _FU\n\n                const Transform2D &tr = p_transform;\n\n            GLfloat matrix[16]={ /* build a 16x16 matrix */\n                (GLfloat)tr.columns[0][0],\n                (GLfloat)tr.columns[0][1],\n                (GLfloat)0,\n                (GLfloat)0,\n                (GLfloat)tr.columns[1][0],\n                (GLfloat)tr.columns[1][1],\n                (GLfloat)0,\n                (GLfloat)0,\n                (GLfloat)0,\n                (GLfloat)0,\n                (GLfloat)1,\n                (GLfloat)0,\n                (GLfloat)tr.columns[2][0],\n                (GLfloat)tr.columns[2][1],\n                (GLfloat)0,\n                (GLfloat)1\n            };\n\n                glUniformMatrix4fv(version_get_uniform(p_uniform,p_version,p_variant,p_specialization),1,false,matrix);\n\n            }\n\n            \"\"\"\n            )\n\n            fd.write(\n                \"\"\"_FORCE_INLINE_ void version_set_uniform(Uniforms p_uniform, const Projection& p_matrix, RID p_version, ShaderVariant p_variant\"\"\"\n                + defvariant\n                + \"\"\",uint64_t p_specialization=\"\"\"\n                + str(defspec)\n                + \"\"\") {  _FU\n\n                GLfloat matrix[16];\n\n                for (int i = 0; i < 4; i++) {\n                    for (int j = 0; j < 4; j++) {\n                        matrix[i * 4 + j] = p_matrix.columns[i][j];\n                    }\n                }\n\n                glUniformMatrix4fv(version_get_uniform(p_uniform, p_version, p_variant, p_specialization), 1, false, matrix);\n        }\"\"\"\n            )\n\n            fd.write(\"\\n\\n#undef _FU\\n\\n\\n\")\n\n        fd.write(\"protected:\\n\\n\")\n\n        fd.write(\"\\tvirtual void _init() override {\\n\\n\")\n\n        if header_data.uniforms:\n            fd.write(\"\\t\\tstatic const char* _uniform_strings[]={\\n\")\n            if header_data.uniforms:\n                for x in header_data.uniforms:\n                    fd.write('\\t\\t\\t\"' + x + '\",\\n')\n            fd.write(\"\\t\\t};\\n\\n\")\n        else:\n            fd.write(\"\\t\\tstatic const char **_uniform_strings=nullptr;\\n\")\n\n        variant_count = 1\n        if len(header_data.variant_defines) > 0:\n            fd.write(\"\\t\\tstatic const char* _variant_defines[]={\\n\")\n            for x in header_data.variant_defines:\n                fd.write('\\t\\t\\t\"' + x + '\",\\n')\n            fd.write(\"\\t\\t};\\n\\n\")\n            variant_count = len(header_data.variant_defines)\n        else:\n            fd.write('\\t\\tstatic const char **_variant_defines[]={\" \"};\\n')\n\n        if header_data.texunits:\n            fd.write(\"\\t\\tstatic TexUnitPair _texunit_pairs[]={\\n\")\n            for x in header_data.texunits:\n                fd.write('\\t\\t\\t{\"' + x[0] + '\",' + x[1] + \"},\\n\")\n            fd.write(\"\\t\\t};\\n\\n\")\n        else:\n            fd.write(\"\\t\\tstatic TexUnitPair *_texunit_pairs=nullptr;\\n\")\n\n        if header_data.ubos:\n            fd.write(\"\\t\\tstatic UBOPair _ubo_pairs[]={\\n\")\n            for x in header_data.ubos:\n                fd.write('\\t\\t\\t{\"' + x[0] + '\",' + x[1] + \"},\\n\")\n            fd.write(\"\\t\\t};\\n\\n\")\n        else:\n            fd.write(\"\\t\\tstatic UBOPair *_ubo_pairs=nullptr;\\n\")\n\n        specializations_found = []\n\n        if header_data.specialization_names:\n            fd.write(\"\\t\\tstatic Specialization _spec_pairs[]={\\n\")\n            for i in range(len(header_data.specialization_names)):\n                defval = header_data.specialization_values[i].strip()\n                if defval.upper() == \"TRUE\" or defval == \"1\":\n                    defval = \"true\"\n                else:\n                    defval = \"false\"\n\n                fd.write('\\t\\t\\t{\"' + header_data.specialization_names[i] + '\",' + defval + \"},\\n\")\n                specializations_found.append(header_data.specialization_names[i])\n            fd.write(\"\\t\\t};\\n\\n\")\n        else:\n            fd.write(\"\\t\\tstatic Specialization *_spec_pairs=nullptr;\\n\")\n\n        feedback_count = 0\n\n        if header_data.feedbacks:\n            fd.write(\"\\t\\tstatic const Feedback _feedbacks[]={\\n\")\n            for x in header_data.feedbacks:\n                name = x[0]\n                spec = x[1]\n                if spec in specializations_found:\n                    fd.write('\\t\\t\\t{\"' + name + '\",' + str(1 << specializations_found.index(spec)) + \"},\\n\")\n                else:\n                    fd.write('\\t\\t\\t{\"' + name + '\",0},\\n')\n\n                feedback_count += 1\n\n            fd.write(\"\\t\\t};\\n\\n\")\n        else:\n            fd.write(\"\\t\\tstatic const Feedback* _feedbacks=nullptr;\\n\")\n\n        fd.write(\"\\t\\tstatic const char _vertex_code[]={\\n\")\n        fd.write(to_raw_cstring(header_data.vertex_lines))\n        fd.write(\"\\n\\t\\t};\\n\\n\")\n\n        fd.write(\"\\t\\tstatic const char _fragment_code[]={\\n\")\n        fd.write(to_raw_cstring(header_data.fragment_lines))\n        fd.write(\"\\n\\t\\t};\\n\\n\")\n\n        fd.write(\n            '\\t\\t_setup(_vertex_code,_fragment_code,\"'\n            + out_file_class\n            + '\",'\n            + str(len(header_data.uniforms))\n            + \",_uniform_strings,\"\n            + str(len(header_data.ubos))\n            + \",_ubo_pairs,\"\n            + str(feedback_count)\n            + \",_feedbacks,\"\n            + str(len(header_data.texunits))\n            + \",_texunit_pairs,\"\n            + str(len(header_data.specialization_names))\n            + \",_spec_pairs,\"\n            + str(variant_count)\n            + \",_variant_defines);\\n\"\n        )\n\n        fd.write(\"\\t}\\n\\n\")\n\n        fd.write(\"};\\n\\n\")\n        fd.write(\"#endif\\n\")\n\n\ndef build_gles3_headers(target, source, env):\n    env.NoCache(target)\n    for x in source:\n        build_gles3_header(str(x), include=\"drivers/gles3/shader_gles3.h\", class_suffix=\"GLES3\")\n",
        "imports": [
            "print_error",
            "methods",
            "typing",
            "os.path",
            "Optional"
        ],
        "functions": [
            "build_gles3_header",
            "__init__",
            "build_gles3_headers",
            "include_file_in_gles3_header"
        ],
        "variables": [
            "defval",
            "variant_names",
            "texunit_names",
            "fragment_lines",
            "feedbacks",
            "uniforms",
            "_ubo_pairs",
            "out_file_ifdef",
            "fbos",
            "ubos",
            "variant_count",
            "feedback_count",
            "i",
            "name",
            "specvalue",
            "tr",
            "specializations_found",
            "includeline",
            "out_file_base",
            "p_specialization",
            "define",
            "line_offset",
            "uline",
            "fragment_included_files",
            "defname",
            "_texunit_pairs",
            "_spec_pairs",
            "spec",
            "line",
            "_uniform_strings",
            "_feedbacks",
            "included_file",
            "header_data",
            "variant_defines",
            "lines",
            "j",
            "defspec",
            "specialization_names",
            "texunit",
            "ubostr",
            "eqpos",
            "out_file",
            "texunitstr",
            "specialization_values",
            "ubo_names",
            "x",
            "counter",
            "include",
            "fragment_offset",
            "ubo",
            "texunits",
            "vertex_included_files",
            "reading",
            "vertex_offset",
            "encoding",
            "bind",
            "out_file_class",
            "specname",
            "defvariant",
            "vertex_lines"
        ]
    },
    {
        "file_name": "glsl_builders.py",
        "language": "python",
        "source_code": "\"\"\"Functions used to generate source files during build time\"\"\"\n\nimport os.path\nfrom typing import Optional\n\nfrom methods import print_error, to_raw_cstring\n\n\nclass RDHeaderStruct:\n    def __init__(self):\n        self.vertex_lines = []\n        self.fragment_lines = []\n        self.compute_lines = []\n\n        self.vertex_included_files = []\n        self.fragment_included_files = []\n        self.compute_included_files = []\n\n        self.reading = \"\"\n        self.line_offset = 0\n        self.vertex_offset = 0\n        self.fragment_offset = 0\n        self.compute_offset = 0\n\n\ndef include_file_in_rd_header(filename: str, header_data: RDHeaderStruct, depth: int) -> RDHeaderStruct:\n    with open(filename, \"r\", encoding=\"utf-8\") as fs:\n        line = fs.readline()\n\n        while line:\n            index = line.find(\"//\")\n            if index != -1:\n                line = line[:index]\n\n            if line.find(\"#[vertex]\") != -1:\n                header_data.reading = \"vertex\"\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.vertex_offset = header_data.line_offset\n                continue\n\n            if line.find(\"#[fragment]\") != -1:\n                header_data.reading = \"fragment\"\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.fragment_offset = header_data.line_offset\n                continue\n\n            if line.find(\"#[compute]\") != -1:\n                header_data.reading = \"compute\"\n                line = fs.readline()\n                header_data.line_offset += 1\n                header_data.compute_offset = header_data.line_offset\n                continue\n\n            while line.find(\"#include \") != -1:\n                includeline = line.replace(\"#include \", \"\").strip()[1:-1]\n\n                if includeline.startswith(\"thirdparty/\"):\n                    included_file = os.path.relpath(includeline)\n\n                else:\n                    included_file = os.path.relpath(os.path.dirname(filename) + \"/\" + includeline)\n\n                if included_file not in header_data.vertex_included_files and header_data.reading == \"vertex\":\n                    header_data.vertex_included_files += [included_file]\n                    if include_file_in_rd_header(included_file, header_data, depth + 1) is None:\n                        print_error(f'In file \"{filename}\": #include \"{includeline}\" could not be found!\"')\n                elif included_file not in header_data.fragment_included_files and header_data.reading == \"fragment\":\n                    header_data.fragment_included_files += [included_file]\n                    if include_file_in_rd_header(included_file, header_data, depth + 1) is None:\n                        print_error(f'In file \"{filename}\": #include \"{includeline}\" could not be found!\"')\n                elif included_file not in header_data.compute_included_files and header_data.reading == \"compute\":\n                    header_data.compute_included_files += [included_file]\n                    if include_file_in_rd_header(included_file, header_data, depth + 1) is None:\n                        print_error(f'In file \"{filename}\": #include \"{includeline}\" could not be found!\"')\n\n                line = fs.readline()\n\n            line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n\n            if header_data.reading == \"vertex\":\n                header_data.vertex_lines += [line]\n            if header_data.reading == \"fragment\":\n                header_data.fragment_lines += [line]\n            if header_data.reading == \"compute\":\n                header_data.compute_lines += [line]\n\n            line = fs.readline()\n            header_data.line_offset += 1\n\n    return header_data\n\n\ndef build_rd_header(\n    filename: str, optional_output_filename: Optional[str] = None, header_data: Optional[RDHeaderStruct] = None\n) -> None:\n    header_data = header_data or RDHeaderStruct()\n    include_file_in_rd_header(filename, header_data, 0)\n\n    if optional_output_filename is None:\n        out_file = filename + \".gen.h\"\n    else:\n        out_file = optional_output_filename\n\n    out_file_base = out_file\n    out_file_base = out_file_base[out_file_base.rfind(\"/\") + 1 :]\n    out_file_base = out_file_base[out_file_base.rfind(\"\\\\\") + 1 :]\n    out_file_ifdef = out_file_base.replace(\".\", \"_\").upper()\n    out_file_class = out_file_base.replace(\".glsl.gen.h\", \"\").title().replace(\"_\", \"\").replace(\".\", \"\") + \"ShaderRD\"\n\n    if header_data.compute_lines:\n        body_parts = [\n            \"static const char _compute_code[] = {\\n%s\\n\\t\\t};\" % to_raw_cstring(header_data.compute_lines),\n            f'setup(nullptr, nullptr, _compute_code, \"{out_file_class}\");',\n        ]\n    else:\n        body_parts = [\n            \"static const char _vertex_code[] = {\\n%s\\n\\t\\t};\" % to_raw_cstring(header_data.vertex_lines),\n            \"static const char _fragment_code[] = {\\n%s\\n\\t\\t};\" % to_raw_cstring(header_data.fragment_lines),\n            f'setup(_vertex_code, _fragment_code, nullptr, \"{out_file_class}\");',\n        ]\n\n    body_content = \"\\n\\t\\t\".join(body_parts)\n\n    # Intended curly brackets are doubled so f-string doesn't eat them up.\n    shader_template = f\"\"\"/* WARNING, THIS FILE WAS GENERATED, DO NOT EDIT */\n#ifndef {out_file_ifdef}_RD\n#define {out_file_ifdef}_RD\n\n#include \"servers/rendering/renderer_rd/shader_rd.h\"\n\nclass {out_file_class} : public ShaderRD {{\n\npublic:\n\n\t{out_file_class}() {{\n\n\t\t{body_content}\n\t}}\n}};\n\n#endif\n\"\"\"\n\n    with open(out_file, \"w\", encoding=\"utf-8\", newline=\"\\n\") as fd:\n        fd.write(shader_template)\n\n\ndef build_rd_headers(target, source, env):\n    env.NoCache(target)\n    for x in source:\n        build_rd_header(filename=str(x))\n\n\nclass RAWHeaderStruct:\n    def __init__(self):\n        self.code = \"\"\n\n\ndef include_file_in_raw_header(filename: str, header_data: RAWHeaderStruct, depth: int) -> None:\n    with open(filename, \"r\", encoding=\"utf-8\") as fs:\n        line = fs.readline()\n\n        while line:\n            while line.find(\"#include \") != -1:\n                includeline = line.replace(\"#include \", \"\").strip()[1:-1]\n\n                included_file = os.path.relpath(os.path.dirname(filename) + \"/\" + includeline)\n                include_file_in_raw_header(included_file, header_data, depth + 1)\n\n                line = fs.readline()\n\n            header_data.code += line\n            line = fs.readline()\n\n\ndef build_raw_header(\n    filename: str, optional_output_filename: Optional[str] = None, header_data: Optional[RAWHeaderStruct] = None\n):\n    header_data = header_data or RAWHeaderStruct()\n    include_file_in_raw_header(filename, header_data, 0)\n\n    if optional_output_filename is None:\n        out_file = filename + \".gen.h\"\n    else:\n        out_file = optional_output_filename\n\n    out_file_base = out_file.replace(\".glsl.gen.h\", \"_shader_glsl\")\n    out_file_base = out_file_base[out_file_base.rfind(\"/\") + 1 :]\n    out_file_base = out_file_base[out_file_base.rfind(\"\\\\\") + 1 :]\n    out_file_ifdef = out_file_base.replace(\".\", \"_\").upper()\n\n    shader_template = f\"\"\"/* WARNING, THIS FILE WAS GENERATED, DO NOT EDIT */\n#ifndef {out_file_ifdef}_RAW_H\n#define {out_file_ifdef}_RAW_H\n\nstatic const char {out_file_base}[] = {{\n{to_raw_cstring(header_data.code)}\n}};\n#endif\n\"\"\"\n\n    with open(out_file, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n        f.write(shader_template)\n\n\ndef build_raw_headers(target, source, env):\n    env.NoCache(target)\n    for x in source:\n        build_raw_header(filename=str(x))\n",
        "imports": [
            "print_error",
            "methods",
            "typing",
            "os.path",
            "Optional"
        ],
        "functions": [
            "build_raw_header",
            "include_file_in_raw_header",
            "include_file_in_rd_header",
            "build_rd_headers",
            "build_rd_header",
            "build_raw_headers",
            "__init__"
        ],
        "variables": [
            "compute_lines",
            "fragment_lines",
            "out_file_ifdef",
            "filename",
            "includeline",
            "compute_included_files",
            "out_file_base",
            "line_offset",
            "fragment_included_files",
            "compute_offset",
            "code",
            "line",
            "included_file",
            "header_data",
            "out_file",
            "body_parts",
            "shader_template",
            "fragment_offset",
            "vertex_included_files",
            "reading",
            "vertex_offset",
            "encoding",
            "out_file_class",
            "body_content",
            "index",
            "vertex_lines"
        ]
    },
    {
        "file_name": "convert_hf_to_gguf.py",
        "language": "python",
        "source_code": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nfrom __future__ import annotations\n\nimport ast\nimport logging\nimport argparse\nimport contextlib\nimport json\nimport os\nimport re\nimport sys\nfrom enum import IntEnum\nfrom pathlib import Path\nfrom hashlib import sha256\nfrom typing import TYPE_CHECKING, Any, Callable, ContextManager, Iterable, Iterator, Literal, Sequence, TypeVar, cast\nfrom itertools import chain\n\nimport math\nimport numpy as np\nimport torch\n\nif TYPE_CHECKING:\n    from torch import Tensor\n\nif 'NO_LOCAL_GGUF' not in os.environ:\n    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\nimport gguf\n\nlogger = logging.getLogger(\"hf-to-gguf\")\n\n\n###### MODEL DEFINITIONS ######\n\nclass SentencePieceTokenTypes(IntEnum):\n    NORMAL = 1\n    UNKNOWN = 2\n    CONTROL = 3\n    USER_DEFINED = 4\n    UNUSED = 5\n    BYTE = 6\n\n\nAnyModel = TypeVar(\"AnyModel\", bound=\"type[Model]\")\n\n\nclass Model:\n    _model_classes: dict[str, type[Model]] = {}\n\n    dir_model: Path\n    ftype: gguf.LlamaFileType\n    fname_out: Path\n    is_big_endian: bool\n    endianess: gguf.GGUFEndian\n    use_temp_file: bool\n    lazy: bool\n    part_names: list[str]\n    is_safetensors: bool\n    hparams: dict[str, Any]\n    block_count: int\n    tensor_map: gguf.TensorNameMap\n    tensor_names: set[str] | None\n    gguf_writer: gguf.GGUFWriter\n    model_name: str | None\n    metadata_override: Path | None\n    dir_model_card: Path\n\n    # subclasses should define this!\n    model_arch: gguf.MODEL_ARCH\n\n    def __init__(self, dir_model: Path, ftype: gguf.LlamaFileType, fname_out: Path, is_big_endian: bool = False,\n                 use_temp_file: bool = False, eager: bool = False,\n                 metadata_override: Path | None = None, model_name: str | None = None,\n                 split_max_tensors: int = 0, split_max_size: int = 0, dry_run: bool = False,\n                 small_first_shard: bool = False, hparams: dict[str, Any] | None = None):\n        if type(self) is Model:\n            raise TypeError(f\"{type(self).__name__!r} should not be directly instantiated\")\n\n        self.dir_model = dir_model\n        self.ftype = ftype\n        self.fname_out = fname_out\n        self.is_big_endian = is_big_endian\n        self.endianess = gguf.GGUFEndian.BIG if is_big_endian else gguf.GGUFEndian.LITTLE\n        self.use_temp_file = use_temp_file\n        self.lazy = not eager\n        self.part_names = Model.get_model_part_names(self.dir_model, \"model\", \".safetensors\")\n        self.is_safetensors = len(self.part_names) > 0\n        if not self.is_safetensors:\n            self.part_names = Model.get_model_part_names(self.dir_model, \"pytorch_model\", \".bin\")\n        self.hparams = Model.load_hparams(self.dir_model) if hparams is None else hparams\n        self.block_count = self.find_hparam([\"n_layers\", \"num_hidden_layers\", \"n_layer\", \"num_layers\"])\n        self.tensor_map = gguf.get_tensor_name_map(self.model_arch, self.block_count)\n        self.tensor_names = None\n        self.metadata_override = metadata_override\n        self.model_name = model_name\n        self.dir_model_card = dir_model  # overridden in convert_lora_to_gguf.py\n\n        # Apply heuristics to figure out typical tensor encoding based on first layer tensor encoding type\n        if self.ftype == gguf.LlamaFileType.GUESSED:\n            # NOTE: can't use field \"torch_dtype\" in config.json, because some finetunes lie.\n            _, first_tensor = next(self.get_tensors())\n            if first_tensor.dtype == torch.float16:\n                logger.info(f\"choosing --outtype f16 from first tensor type ({first_tensor.dtype})\")\n                self.ftype = gguf.LlamaFileType.MOSTLY_F16\n            else:\n                logger.info(f\"choosing --outtype bf16 from first tensor type ({first_tensor.dtype})\")\n                self.ftype = gguf.LlamaFileType.MOSTLY_BF16\n\n        # Configure GGUF Writer\n        self.gguf_writer = gguf.GGUFWriter(path=None, arch=gguf.MODEL_ARCH_NAMES[self.model_arch], endianess=self.endianess, use_temp_file=self.use_temp_file,\n                                           split_max_tensors=split_max_tensors, split_max_size=split_max_size, dry_run=dry_run, small_first_shard=small_first_shard)\n\n    @classmethod\n    def __init_subclass__(cls):\n        # can't use an abstract property, because overriding it without type errors\n        # would require using decorated functions instead of simply defining the property\n        if \"model_arch\" not in cls.__dict__:\n            raise TypeError(f\"Missing property 'model_arch' for {cls.__name__!r}\")\n\n    def find_hparam(self, keys: Iterable[str], optional: bool = False) -> Any:\n        key = next((k for k in keys if k in self.hparams), None)\n        if key is not None:\n            return self.hparams[key]\n        if optional:\n            return None\n        raise KeyError(f\"could not find any of: {keys}\")\n\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n\n    def get_tensors(self) -> Iterator[tuple[str, Tensor]]:\n        tensor_names_from_parts: set[str] = set()\n\n        index_name = \"model.safetensors\" if self.is_safetensors else \"pytorch_model.bin\"\n        index_name += \".index.json\"\n        index_file = self.dir_model / index_name\n\n        if index_file.is_file():\n            self.tensor_names = set()\n            logger.info(f\"gguf: loading model weight map from '{index_name}'\")\n            with open(index_file, \"r\", encoding=\"utf-8\") as f:\n                index: dict[str, Any] = json.load(f)\n                weight_map = index.get(\"weight_map\")\n                if weight_map is None or not isinstance(weight_map, dict):\n                    raise ValueError(f\"Can't load 'weight_map' from {index_name!r}\")\n                self.tensor_names.update(weight_map.keys())\n        else:\n            self.tensor_names = tensor_names_from_parts\n            weight_map = {}\n\n        for part_name in self.part_names:\n            logger.info(f\"gguf: loading model part '{part_name}'\")\n            ctx: ContextManager[Any]\n            if self.is_safetensors:\n                from safetensors import safe_open\n                ctx = cast(ContextManager[Any], safe_open(self.dir_model / part_name, framework=\"pt\", device=\"cpu\"))\n            else:\n                ctx = contextlib.nullcontext(torch.load(str(self.dir_model / part_name), map_location=\"cpu\", mmap=True, weights_only=True))\n\n            with ctx as model_part:\n                tensor_names_from_parts.update(model_part.keys())\n\n                for name in model_part.keys():\n                    if self.is_safetensors:\n                        if self.lazy:\n                            data = model_part.get_slice(name)\n                            data = LazyTorchTensor.from_safetensors_slice(data)\n                        else:\n                            data = model_part.get_tensor(name)\n                    else:\n                        data = model_part[name]\n                        if self.lazy:\n                            data = LazyTorchTensor.from_eager(data)\n                    yield name, data\n\n        # verify tensor name presence and identify potentially missing files\n        if len(tensor_names_from_parts.symmetric_difference(self.tensor_names)) > 0:\n            missing = sorted(self.tensor_names.difference(tensor_names_from_parts))\n            extra = sorted(tensor_names_from_parts.difference(self.tensor_names))\n            missing_files = sorted(set(weight_map[n] for n in missing if n in weight_map))\n            if len(extra) == 0 and len(missing_files) > 0:\n                raise ValueError(f\"Missing or incomplete model files: {missing_files}\")\n            else:\n                raise ValueError(\"Mismatch between weight map and model parts for tensor names:\\n\"\n                                 f\"Missing tensors: {missing}\\n\"\n                                 f\"Extra tensors: {extra}\")\n\n    def format_tensor_name(self, key: gguf.MODEL_TENSOR, bid: int | None = None, suffix: str = \".weight\") -> str:\n        if key not in gguf.MODEL_TENSORS[self.model_arch]:\n            raise ValueError(f\"Missing {key!r} for MODEL_TENSORS of {self.model_arch!r}\")\n        name: str = gguf.TENSOR_NAMES[key]\n        if \"{bid}\" in name:\n            assert bid is not None\n            name = name.format(bid=bid)\n        return name + suffix\n\n    def match_model_tensor_name(self, name: str, key: gguf.MODEL_TENSOR, bid: int | None, suffix: str = \".weight\") -> bool:\n        if key not in gguf.MODEL_TENSORS[self.model_arch]:\n            return False\n        key_name: str = gguf.TENSOR_NAMES[key]\n        if \"{bid}\" in key_name:\n            if bid is None:\n                return False\n            key_name = key_name.format(bid=bid)\n        else:\n            if bid is not None:\n                return False\n        return name == (key_name + suffix)\n\n    def map_tensor_name(self, name: str, try_suffixes: Sequence[str] = (\".weight\", \".bias\")) -> str:\n        new_name = self.tensor_map.get_name(key=name, try_suffixes=try_suffixes)\n        if new_name is None:\n            raise ValueError(f\"Can not map tensor {name!r}\")\n        return new_name\n\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_block_count(self.block_count)\n\n        if (n_ctx := self.find_hparam([\"max_position_embeddings\", \"n_ctx\"], optional=True)) is not None:\n            self.gguf_writer.add_context_length(n_ctx)\n            logger.info(f\"gguf: context length = {n_ctx}\")\n\n        if (n_embd := self.find_hparam([\"hidden_size\", \"n_embd\"], optional=True)) is not None:\n            self.gguf_writer.add_embedding_length(n_embd)\n            logger.info(f\"gguf: embedding length = {n_embd}\")\n\n        if (n_ff := self.find_hparam([\"intermediate_size\", \"n_inner\"], optional=True)) is not None:\n            self.gguf_writer.add_feed_forward_length(n_ff)\n            logger.info(f\"gguf: feed forward length = {n_ff}\")\n\n        if (n_head := self.find_hparam([\"num_attention_heads\", \"n_head\"], optional=True)) is not None:\n            self.gguf_writer.add_head_count(n_head)\n            logger.info(f\"gguf: head count = {n_head}\")\n\n        if (n_head_kv := self.hparams.get(\"num_key_value_heads\")) is not None:\n            self.gguf_writer.add_head_count_kv(n_head_kv)\n            logger.info(f\"gguf: key-value head count = {n_head_kv}\")\n\n        if (rope_theta := self.hparams.get(\"rope_theta\")) is not None:\n            self.gguf_writer.add_rope_freq_base(rope_theta)\n            logger.info(f\"gguf: rope theta = {rope_theta}\")\n        if (f_rms_eps := self.hparams.get(\"rms_norm_eps\")) is not None:\n            self.gguf_writer.add_layer_norm_rms_eps(f_rms_eps)\n            logger.info(f\"gguf: rms norm epsilon = {f_rms_eps}\")\n        if (f_norm_eps := self.find_hparam([\"layer_norm_eps\", \"layer_norm_epsilon\", \"norm_epsilon\"], optional=True)) is not None:\n            self.gguf_writer.add_layer_norm_eps(f_norm_eps)\n            logger.info(f\"gguf: layer norm epsilon = {f_norm_eps}\")\n        if (n_experts := self.hparams.get(\"num_local_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n            logger.info(f\"gguf: expert count = {n_experts}\")\n        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\")) is not None:\n            self.gguf_writer.add_expert_used_count(n_experts_used)\n            logger.info(f\"gguf: experts used count = {n_experts_used}\")\n\n        if (head_dim := self.hparams.get(\"head_dim\")) is not None:\n            self.gguf_writer.add_key_length(head_dim)\n            self.gguf_writer.add_value_length(head_dim)\n\n        self.gguf_writer.add_file_type(self.ftype)\n        logger.info(f\"gguf: file type = {self.ftype}\")\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def tensor_force_quant(self, name: str, new_name: str, bid: int | None, n_dims: int) -> gguf.GGMLQuantizationType | bool:\n        del name, new_name, bid, n_dims  # unused\n\n        return False\n\n    # some models need extra generated tensors (like rope_freqs)\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        return ()\n\n    def prepare_tensors(self):\n        max_name_len = max(len(s) for _, s in self.tensor_map.mapping.values()) + len(\".weight,\")\n\n        for name, data_torch in chain(self.generate_extra_tensors(), self.get_tensors()):\n            # we don't need these\n            if name.endswith((\".attention.masked_bias\", \".attention.bias\", \".rotary_emb.inv_freq\")):\n                continue\n\n            old_dtype = data_torch.dtype\n\n            # convert any unsupported data types to float32\n            if data_torch.dtype not in (torch.float16, torch.float32):\n                data_torch = data_torch.to(torch.float32)\n\n            # use the first number-like part of the tensor name as the block id\n            bid = None\n            for part in name.split(\".\"):\n                if part.isdecimal():\n                    bid = int(part)\n                    break\n\n            for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n                # TODO: why do we squeeze here?\n                # data = data_torch.squeeze().numpy()\n                data = data_torch.numpy()\n\n                # if data ends up empty, it means data_torch was a scalar tensor -> restore\n                if len(data.shape) == 0:\n                    data = data_torch.numpy()\n\n                n_dims = len(data.shape)\n                data_qtype: gguf.GGMLQuantizationType | bool = self.tensor_force_quant(name, new_name, bid, n_dims)\n\n                # Most of the codebase that takes in 1D tensors or norms only handles F32 tensors\n                if n_dims <= 1 or new_name.endswith(\"_norm.weight\"):\n                    data_qtype = gguf.GGMLQuantizationType.F32\n\n                # Conditions should closely match those in llama_model_quantize_internal in llama.cpp\n                # Some tensor types are always in float32\n                if data_qtype is False and (\n                    any(\n                        self.match_model_tensor_name(new_name, key, bid)\n                        for key in (\n                            gguf.MODEL_TENSOR.FFN_GATE_INP,\n                            gguf.MODEL_TENSOR.POS_EMBD,\n                            gguf.MODEL_TENSOR.TOKEN_TYPES,\n                            gguf.MODEL_TENSOR.SSM_CONV1D,\n                            gguf.MODEL_TENSOR.TIME_MIX_FIRST,\n                            gguf.MODEL_TENSOR.TIME_MIX_W1,\n                            gguf.MODEL_TENSOR.TIME_MIX_W2,\n                            gguf.MODEL_TENSOR.TIME_MIX_DECAY_W1,\n                            gguf.MODEL_TENSOR.TIME_MIX_DECAY_W2,\n                            gguf.MODEL_TENSOR.TIME_MIX_LERP_FUSED,\n                            gguf.MODEL_TENSOR.POSNET_NORM1,\n                            gguf.MODEL_TENSOR.POSNET_NORM2,\n                        )\n                    )\n                    or not new_name.endswith(\".weight\")\n                ):\n                    data_qtype = gguf.GGMLQuantizationType.F32\n\n                if data_qtype is False and any(\n                    self.match_model_tensor_name(new_name, key, bid)\n                    for key in (\n                        gguf.MODEL_TENSOR.TOKEN_EMBD,\n                        gguf.MODEL_TENSOR.OUTPUT,\n                    )\n                ):\n                    if self.ftype in (\n                        gguf.LlamaFileType.MOSTLY_TQ1_0,\n                        gguf.LlamaFileType.MOSTLY_TQ2_0,\n                    ):\n                        # TODO: use Q4_K and Q6_K\n                        data_qtype = gguf.GGMLQuantizationType.F16\n\n                # No override (data_qtype is False), or wants to be quantized (data_qtype is True)\n                if isinstance(data_qtype, bool):\n                    if self.ftype == gguf.LlamaFileType.ALL_F32:\n                        data_qtype = gguf.GGMLQuantizationType.F32\n                    elif self.ftype == gguf.LlamaFileType.MOSTLY_F16:\n                        data_qtype = gguf.GGMLQuantizationType.F16\n                    elif self.ftype == gguf.LlamaFileType.MOSTLY_BF16:\n                        data_qtype = gguf.GGMLQuantizationType.BF16\n                    elif self.ftype == gguf.LlamaFileType.MOSTLY_Q8_0:\n                        data_qtype = gguf.GGMLQuantizationType.Q8_0\n                    elif self.ftype == gguf.LlamaFileType.MOSTLY_TQ1_0:\n                        data_qtype = gguf.GGMLQuantizationType.TQ1_0\n                    elif self.ftype == gguf.LlamaFileType.MOSTLY_TQ2_0:\n                        data_qtype = gguf.GGMLQuantizationType.TQ2_0\n                    else:\n                        raise ValueError(f\"Unknown file type: {self.ftype.name}\")\n\n                try:\n                    data = gguf.quants.quantize(data, data_qtype)\n                except gguf.QuantError as e:\n                    logger.warning(\"%s, %s\", e, \"falling back to F16\")\n                    data_qtype = gguf.GGMLQuantizationType.F16\n                    data = gguf.quants.quantize(data, data_qtype)\n\n                shape = gguf.quant_shape_from_byte_shape(data.shape, data_qtype) if data.dtype == np.uint8 else data.shape\n\n                # reverse shape to make it similar to the internal ggml dimension order\n                shape_str = f\"{{{', '.join(str(n) for n in reversed(shape))}}}\"\n\n                # n_dims is implicit in the shape\n                logger.info(f\"{f'%-{max_name_len}s' % f'{new_name},'} {old_dtype} --> {data_qtype.name}, shape = {shape_str}\")\n\n                self.gguf_writer.add_tensor(new_name, data, raw_dtype=data_qtype)\n\n    def set_type(self):\n        self.gguf_writer.add_type(gguf.GGUFType.MODEL)\n\n    def prepare_metadata(self, vocab_only: bool):\n\n        total_params, shared_params, expert_params, expert_count = self.gguf_writer.get_total_parameter_count()\n\n        self.metadata = gguf.Metadata.load(self.metadata_override, self.dir_model_card, self.model_name, total_params)\n\n        # Fallback to model directory name if metadata name is still missing\n        if self.metadata.name is None:\n            self.metadata.name = self.dir_model.name\n\n        # Generate parameter weight class (useful for leader boards) if not yet determined\n        if self.metadata.size_label is None and total_params > 0:\n            self.metadata.size_label = gguf.size_label(total_params, shared_params, expert_params, expert_count)\n\n        # Extract the encoding scheme from the file type name. e.g. 'gguf.LlamaFileType.MOSTLY_Q8_0' --> 'Q8_0'\n        output_type: str = self.ftype.name.partition(\"_\")[2]\n\n        # Filename Output\n        if self.fname_out.is_dir():\n            # Generate default filename based on model specification and available metadata\n            if not vocab_only:\n                fname_default: str = gguf.naming_convention(self.metadata.name, self.metadata.basename, self.metadata.finetune, self.metadata.version, self.metadata.size_label, output_type, model_type=\"LoRA\" if total_params < 0 else None)\n            else:\n                fname_default: str = gguf.naming_convention(self.metadata.name, self.metadata.basename, self.metadata.finetune, self.metadata.version, size_label=None, output_type=None, model_type=\"vocab\")\n\n            # Use the default filename\n            self.fname_out = self.fname_out / f\"{fname_default}.gguf\"\n        else:\n            # Output path is a custom defined templated filename\n            # Note: `not is_dir()` is used because `.is_file()` will not detect\n            #       file template strings as it doesn't actually exist as a file\n\n            # Process templated file name with the output ftype, useful with the \"auto\" ftype\n            self.fname_out = self.fname_out.parent / gguf.fill_templated_filename(self.fname_out.name, output_type)\n\n        self.set_type()\n\n        logger.info(\"Set meta model\")\n        self.metadata.set_gguf_meta_model(self.gguf_writer)\n\n        logger.info(\"Set model parameters\")\n        self.set_gguf_parameters()\n\n        logger.info(\"Set model tokenizer\")\n        self.set_vocab()\n\n        logger.info(\"Set model quantization version\")\n        self.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)\n\n    def write(self):\n        self.prepare_tensors()\n        self.prepare_metadata(vocab_only=False)\n        self.gguf_writer.write_header_to_file(path=self.fname_out)\n        self.gguf_writer.write_kv_data_to_file()\n        self.gguf_writer.write_tensors_to_file(progress=True)\n        self.gguf_writer.close()\n\n    def write_vocab(self):\n        if len(self.gguf_writer.tensors) != 1:\n            raise ValueError('Splitting the vocabulary is not supported')\n\n        self.prepare_metadata(vocab_only=True)\n        self.gguf_writer.write_header_to_file(path=self.fname_out)\n        self.gguf_writer.write_kv_data_to_file()\n        self.gguf_writer.close()\n\n    @staticmethod\n    def get_model_part_names(dir_model: Path, prefix: str, suffix: str) -> list[str]:\n        part_names: list[str] = []\n        for filename in os.listdir(dir_model):\n            if filename.startswith(prefix) and filename.endswith(suffix):\n                part_names.append(filename)\n\n        part_names.sort()\n\n        return part_names\n\n    @staticmethod\n    def load_hparams(dir_model: Path):\n        with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    @classmethod\n    def register(cls, *names: str) -> Callable[[AnyModel], AnyModel]:\n        assert names\n\n        def func(modelcls: AnyModel) -> AnyModel:\n            for name in names:\n                cls._model_classes[name] = modelcls\n            return modelcls\n        return func\n\n    @classmethod\n    def print_registered_models(cls):\n        for name in sorted(cls._model_classes.keys()):\n            logger.error(f\"- {name}\")\n\n    @classmethod\n    def from_model_architecture(cls, arch: str) -> type[Model]:\n        try:\n            return cls._model_classes[arch]\n        except KeyError:\n            raise NotImplementedError(f'Architecture {arch!r} not supported!') from None\n\n    def does_token_look_special(self, token: str | bytes) -> bool:\n        if isinstance(token, (bytes, bytearray)):\n            token_text = token.decode(encoding=\"utf-8\")\n        elif isinstance(token, memoryview):\n            token_text = token.tobytes().decode(encoding=\"utf-8\")\n        else:\n            token_text = token\n\n        # Some models mark some added tokens which ought to be control tokens as not special.\n        # (e.g. command-r, command-r-plus, deepseek-coder, gemma{,-2})\n        seems_special = token_text in (\n            \"<pad>\",  # deepseek-coder\n            \"<mask>\", \"<2mass>\", \"[@BOS@]\",  # gemma{,-2}\n        )\n\n        seems_special = seems_special or (token_text.startswith(\"<|\") and token_text.endswith(\"|>\"))\n        seems_special = seems_special or (token_text.startswith(\"<\uff5c\") and token_text.endswith(\"\uff5c>\"))  # deepseek-coder\n\n        # TODO: should these be marked as UNUSED instead? (maybe not)\n        seems_special = seems_special or (token_text.startswith(\"<unused\") and token_text.endswith(\">\"))  # gemma{,-2}\n\n        return seems_special\n\n    # used for GPT-2 BPE and WordPiece vocabs\n    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n        tokens: list[str] = []\n        toktypes: list[int] = []\n\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model)\n        vocab_size = self.hparams.get(\"vocab_size\", len(tokenizer.vocab))\n        assert max(tokenizer.vocab.values()) < vocab_size\n\n        tokpre = self.get_vocab_base_pre(tokenizer)\n\n        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in tokenizer.vocab.items()}\n        added_vocab = tokenizer.get_added_vocab()\n\n        for i in range(vocab_size):\n            if i not in reverse_vocab:\n                tokens.append(f\"[PAD{i}]\")\n                toktypes.append(gguf.TokenType.UNUSED)\n            else:\n                token: str = reverse_vocab[i]\n                if token in added_vocab:\n                    # The tokenizer in llama.cpp assumes the CONTROL and USER_DEFINED tokens are pre-normalized.\n                    # To avoid unexpected issues - we make sure to normalize non-normalized tokens\n                    if not tokenizer.added_tokens_decoder[i].normalized:\n                        previous_token = token\n                        token = tokenizer.decode(tokenizer.encode(token, add_special_tokens=False))\n                        if previous_token != token:\n                            logger.info(f\"{repr(previous_token)} is encoded and decoded back to {repr(token)} using AutoTokenizer\")\n\n                    if tokenizer.added_tokens_decoder[i].special or self.does_token_look_special(token):\n                        toktypes.append(gguf.TokenType.CONTROL)\n                    else:\n                        # NOTE: this was added for Gemma.\n                        # Encoding and decoding the tokens above isn't sufficient for this case.\n                        token = token.replace(b\"\\xe2\\x96\\x81\".decode(\"utf-8\"), \" \")  # pre-normalize user-defined spaces\n                        toktypes.append(gguf.TokenType.USER_DEFINED)\n                else:\n                    toktypes.append(gguf.TokenType.NORMAL)\n                tokens.append(token)\n\n        return tokens, toktypes, tokpre\n\n    # NOTE: this function is generated by convert_hf_to_gguf_update.py\n    #       do not modify it manually!\n    # ref:  https://github.com/ggerganov/llama.cpp/pull/6920\n    # Marker: Start get_vocab_base_pre\n    def get_vocab_base_pre(self, tokenizer) -> str:\n        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that\n        # is specific for the BPE pre-tokenizer used by the model\n        # we will use this unique identifier to write a \"tokenizer.ggml.pre\" entry in the GGUF file which we can\n        # use in llama.cpp to implement the same pre-tokenizer\n\n        chktxt = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n\ud83d\ude80 (normal) \ud83d\ude36\\u200d\ud83c\udf2b\ufe0f (multiple emojis concatenated) \u2705 \ud83e\udd99\ud83e\udd99 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 \u1780\u17b6\u1793\u17cb\u178f\u17c2\u1796\u17b7\u179f\u17c1\u179f\u17a2\u17b6\u1785\ud83d\ude01 ?\u6211\u60f3\u5728apple\u5de5\u4f5c1314151\u5929\uff5e ------======= \u043d\u0435\u0449\u043e \u043d\u0430 \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 \\'\\'\\'\\'\\'\\'```````\"\"\"\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n\n        chktok = tokenizer.encode(chktxt)\n        chkhsh = sha256(str(chktok).encode()).hexdigest()\n\n        logger.debug(f\"chktok: {chktok}\")\n        logger.debug(f\"chkhsh: {chkhsh}\")\n\n        res = None\n\n        # NOTE: if you get an error here, you need to update the convert_hf_to_gguf_update.py script\n        #       or pull the latest version of the model from Huggingface\n        #       don't edit the hashes manually!\n        if chkhsh == \"0ef9807a4087ebef797fc749390439009c3b9eda9ad1a097abbe738f486c01e5\":\n            # ref: https://huggingface.co/meta-llama/Meta-Llama-3-8B\n            res = \"llama-bpe\"\n        if chkhsh == \"049ecf7629871e3041641907f3de7c733e4dbfdc736f57d882ba0b0845599754\":\n            # ref: https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\n            res = \"deepseek-llm\"\n        if chkhsh == \"347715f544604f9118bb75ed199f68779f423cabb20db6de6f31b908d04d7821\":\n            # ref: https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\n            res = \"deepseek-coder\"\n        if chkhsh == \"8aeee3860c56296a157a1fe2fad249ec40aa59b1bb5709f4ade11c4e6fe652ed\":\n            # ref: https://huggingface.co/tiiuae/falcon-7b\n            res = \"falcon\"\n        if chkhsh == \"9d032fcbd5501f4a38150912590928bfb36091efb5df11b8e2124b0390e3fb1e\":\n            # ref: https://huggingface.co/tiiuae/Falcon3-7B-Base\n            res = \"falcon3\"\n        if chkhsh == \"0876d13b50744004aa9aeae05e7b0647eac9d801b5ba4668afc01e709c15e19f\":\n            # ref: https://huggingface.co/BAAI/bge-small-en-v1.5\n            res = \"bert-bge\"\n        if chkhsh == \"8e62295832751ca1e8f92f2226f403dea30dc5165e448b5bfa05af5340c64ec7\":\n            # ref: https://huggingface.co/BAAI/bge-large-zh-v1.5\n            res = \"bert-bge-large\"\n        if chkhsh == \"b6dc8df998e1cfbdc4eac8243701a65afe638679230920b50d6f17d81c098166\":\n            # ref: https://huggingface.co/mosaicml/mpt-7b\n            res = \"mpt\"\n        if chkhsh == \"35d91631860c815f952d711435f48d356ebac988362536bed955d43bfa436e34\":\n            # ref: https://huggingface.co/bigcode/starcoder2-3b\n            res = \"starcoder\"\n        if chkhsh == \"3ce83efda5659b07b1ad37ca97ca5797ea4285d9b9ab0dc679e4a720c9da7454\":\n            # ref: https://huggingface.co/openai-community/gpt2\n            res = \"gpt-2\"\n        if chkhsh == \"32d85c31273f8019248f2559fed492d929ea28b17e51d81d3bb36fff23ca72b3\":\n            # ref: https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\n            res = \"stablelm2\"\n        if chkhsh == \"6221ad2852e85ce96f791f476e0b390cf9b474c9e3d1362f53a24a06dc8220ff\":\n            # ref: https://huggingface.co/smallcloudai/Refact-1_6-base\n            res = \"refact\"\n        if chkhsh == \"9c2227e4dd922002fb81bde4fc02b0483ca4f12911410dee2255e4987644e3f8\":\n            # ref: https://huggingface.co/CohereForAI/c4ai-command-r-v01\n            res = \"command-r\"\n        if chkhsh == \"e636dc30a262dcc0d8c323492e32ae2b70728f4df7dfe9737d9f920a282b8aea\":\n            # ref: https://huggingface.co/Qwen/Qwen1.5-7B\n            res = \"qwen2\"\n        if chkhsh == \"b6dc8df998e1cfbdc4eac8243701a65afe638679230920b50d6f17d81c098166\":\n            # ref: https://huggingface.co/allenai/OLMo-1.7-7B-hf\n            res = \"olmo\"\n        if chkhsh == \"a8594e3edff7c29c003940395316294b2c623e09894deebbc65f33f1515df79e\":\n            # ref: https://huggingface.co/databricks/dbrx-base\n            res = \"dbrx\"\n        if chkhsh == \"c7699093ba4255a91e702aa38a596aa81669f3525dae06c2953267dde580f448\":\n            # ref: https://huggingface.co/jinaai/jina-reranker-v1-tiny-en\n            res = \"jina-v1-en\"\n        if chkhsh == \"0876d13b50744004aa9aeae05e7b0647eac9d801b5ba4668afc01e709c15e19f\":\n            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-en\n            res = \"jina-v2-en\"\n        if chkhsh == \"171aeeedd6fb548d418a7461d053f11b6f1f1fc9b387bd66640d28a4b9f5c643\":\n            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-es\n            res = \"jina-v2-es\"\n        if chkhsh == \"27949a2493fc4a9f53f5b9b029c82689cfbe5d3a1929bb25e043089e28466de6\":\n            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-de\n            res = \"jina-v2-de\"\n        if chkhsh == \"c136ed14d01c2745d4f60a9596ae66800e2b61fa45643e72436041855ad4089d\":\n            # ref: https://huggingface.co/abacusai/Smaug-Llama-3-70B-Instruct\n            res = \"smaug-bpe\"\n        if chkhsh == \"c7ea5862a53e4272c035c8238367063e2b270d51faa48c0f09e9d5b54746c360\":\n            # ref: https://huggingface.co/LumiOpen/Poro-34B-chat\n            res = \"poro-chat\"\n        if chkhsh == \"7967bfa498ade6b757b064f31e964dddbb80f8f9a4d68d4ba7998fcf281c531a\":\n            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-code\n            res = \"jina-v2-code\"\n        if chkhsh == \"b6e8e1518dc4305be2fe39c313ed643381c4da5db34a98f6a04c093f8afbe99b\" or chkhsh == \"81d72c7348a9f0ebe86f23298d37debe0a5e71149e29bd283904c02262b27516\":\n            # ref: https://huggingface.co/THUDM/glm-4-9b-chat\n            res = \"chatglm-bpe\"\n        if chkhsh == \"7fc505bd3104ca1083b150b17d088b59534ede9bde81f0dd2090967d7fe52cee\":\n            # ref: https://huggingface.co/LumiOpen/Viking-7B\n            res = \"viking\"\n        if chkhsh == \"b53802fb28e26d645c3a310b34bfe07da813026ec7c7716883404d5e0f8b1901\":\n            # ref: https://huggingface.co/core42/jais-13b\n            res = \"jais\"\n        if chkhsh == \"7b3e7548e4308f52a76e8229e4e6cc831195d0d1df43aed21ac6c93da05fec5f\":\n            # ref: https://huggingface.co/WisdomShell/CodeShell-7B\n            res = \"codeshell\"\n        if chkhsh == \"63b97e4253352e6f357cc59ea5b583e3a680eaeaf2632188c2b952de2588485e\":\n            # ref: https://huggingface.co/mistralai/Mistral-Nemo-Base-2407\n            res = \"tekken\"\n        if chkhsh == \"855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\":\n            # ref: https://huggingface.co/HuggingFaceTB/SmolLM-135M\n            res = \"smollm\"\n        if chkhsh == \"3c30d3ad1d6b64202cd222813e7736c2db6e1bd6d67197090fc1211fbc612ae7\":\n            # ref: https://huggingface.co/bigscience/bloom\n            res = \"bloom\"\n        if chkhsh == \"bc01ce58980e1db43859146dc51b1758b3b88729b217a74792e9f8d43e479d21\":\n            # ref: https://huggingface.co/TurkuNLP/gpt3-finnish-small\n            res = \"gpt3-finnish\"\n        if chkhsh == \"4e2b24cc4770243d65a2c9ec19770a72f08cffc161adbb73fcbb6b7dd45a0aae\":\n            # ref: https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\n            res = \"exaone\"\n        if chkhsh == \"fcace8b9cac38ce847670c970cd5892031a753a1ef381abd1d9af00f713da085\":\n            # ref: https://huggingface.co/microsoft/phi-2\n            res = \"phi-2\"\n        if chkhsh == \"60824e3c0d9401f89943cbb2fff727f0e2d4c545ba4df2d6e4f09a6db0f5b450\":\n            # ref: https://huggingface.co/facebook/chameleon-7b\n            res = \"chameleon\"\n        if chkhsh == \"1431a23e583c97432bc230bff598d103ddb5a1f89960c8f1d1051aaa944d0b35\":\n            # ref: https://huggingface.co/sapienzanlp/Minerva-7B-base-v1.0\n            res = \"minerva-7b\"\n        if chkhsh == \"8b5a93ed704057481f240da0be7e7dca721d7f8f4755263b6807227a2cbeae65\":\n            # ref: https://huggingface.co/sentence-transformers/stsb-roberta-base\n            res = \"roberta-bpe\"\n        if chkhsh == \"ad851be1dba641f2e3711822f816db2c265f788b37c63b4e1aeacb9ee92de8eb\":\n            # ref: https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct\n            res = \"gigachat\"\n        if chkhsh == \"d4c8f286ea6b520b3d495c4455483cfa2302c0cfcd4be05d781b6a8a0a7cdaf1\":\n            # ref: https://huggingface.co/Infinigence/Megrez-3B-Instruct\n            res = \"megrez\"\n        if chkhsh == \"877081d19cf6996e2c4ff0e1236341e9b7bde288f5311a56a937f0afbbb3aeb5\":\n            # ref: https://huggingface.co/deepseek-ai/DeepSeek-V3\n            res = \"deepseek-v3\"\n        if chkhsh == \"b3f499bb4255f8ca19fccd664443283318f2fd2414d5e0b040fbdd0cc195d6c5\":\n            # ref: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n            res = \"deepseek-r1-qwen\"\n\n        if res is None:\n            logger.warning(\"\\n\")\n            logger.warning(\"**************************************************************************************\")\n            logger.warning(\"** WARNING: The BPE pre-tokenizer was not recognized!\")\n            logger.warning(\"**          There are 2 possible reasons for this:\")\n            logger.warning(\"**          - the model has not been added to convert_hf_to_gguf_update.py yet\")\n            logger.warning(\"**          - the pre-tokenization config has changed upstream\")\n            logger.warning(\"**          Check your model files and convert_hf_to_gguf_update.py and update them accordingly.\")\n            logger.warning(\"** ref:     https://github.com/ggerganov/llama.cpp/pull/6920\")\n            logger.warning(\"**\")\n            logger.warning(f\"** chkhsh:  {chkhsh}\")\n            logger.warning(\"**************************************************************************************\")\n            logger.warning(\"\\n\")\n            raise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n\n        logger.debug(f\"tokenizer.ggml.pre: {repr(res)}\")\n        logger.debug(f\"chkhsh: {chkhsh}\")\n\n        return res\n        # Marker: End get_vocab_base_pre\n\n    def _set_vocab_none(self) -> None:\n        self.gguf_writer.add_tokenizer_model(\"none\")\n\n    def _set_vocab_gpt2(self) -> None:\n        tokens, toktypes, tokpre = self.get_vocab_base()\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=True)\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def _set_vocab_qwen(self):\n        dir_model = self.dir_model\n        hparams = self.hparams\n        tokens: list[str] = []\n        toktypes: list[int] = []\n\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(dir_model, trust_remote_code=True)\n        vocab_size = hparams[\"vocab_size\"]\n        assert max(tokenizer.get_vocab().values()) < vocab_size\n\n        tokpre = self.get_vocab_base_pre(tokenizer)\n\n        merges = []\n        vocab = {}\n        mergeable_ranks = tokenizer.mergeable_ranks\n        for token, rank in mergeable_ranks.items():\n            vocab[QwenModel.token_bytes_to_string(token)] = rank\n            if len(token) == 1:\n                continue\n            merged = QwenModel.bpe(mergeable_ranks, token, max_rank=rank)\n            assert len(merged) == 2\n            merges.append(' '.join(map(QwenModel.token_bytes_to_string, merged)))\n\n        # for this kind of tokenizer, added_vocab is not a subset of vocab, so they need to be combined\n        added_vocab = tokenizer.special_tokens\n        reverse_vocab = {id_ : encoded_tok for encoded_tok, id_ in {**vocab, **added_vocab}.items()}\n\n        for i in range(vocab_size):\n            if i not in reverse_vocab:\n                tokens.append(f\"[PAD{i}]\")\n                toktypes.append(gguf.TokenType.UNUSED)\n            elif reverse_vocab[i] in added_vocab:\n                tokens.append(reverse_vocab[i])\n                toktypes.append(gguf.TokenType.CONTROL)\n            else:\n                tokens.append(reverse_vocab[i])\n                toktypes.append(gguf.TokenType.NORMAL)\n\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(dir_model, load_merges=False)\n        special_vocab.merges = merges\n        # only add special tokens when they were not already loaded from config.json\n        if len(special_vocab.special_token_ids) == 0:\n            special_vocab._set_special_token(\"bos\", tokenizer.special_tokens[\"<|endoftext|>\"])\n            special_vocab._set_special_token(\"eos\", tokenizer.special_tokens[\"<|endoftext|>\"])\n        # this one is usually not in config.json anyway\n        special_vocab._set_special_token(\"unk\", tokenizer.special_tokens[\"<|endoftext|>\"])\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def _set_vocab_sentencepiece(self, add_to_gguf=True):\n        tokens, scores, toktypes = self._create_vocab_sentencepiece()\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def _create_vocab_sentencepiece(self):\n        from sentencepiece import SentencePieceProcessor\n\n        tokenizer_path = self.dir_model / 'tokenizer.model'\n\n        if not tokenizer_path.is_file():\n            raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\n\n        tokenizer = SentencePieceProcessor()\n        tokenizer.LoadFromFile(str(tokenizer_path))\n\n        vocab_size = self.hparams.get('vocab_size', tokenizer.vocab_size())\n\n        tokens: list[bytes] = [f\"[PAD{i}]\".encode(\"utf-8\") for i in range(vocab_size)]\n        scores: list[float] = [-10000.0] * vocab_size\n        toktypes: list[int] = [SentencePieceTokenTypes.UNUSED] * vocab_size\n\n        for token_id in range(tokenizer.vocab_size()):\n            piece = tokenizer.IdToPiece(token_id)\n            text = piece.encode(\"utf-8\")\n            score = tokenizer.GetScore(token_id)\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.IsUnknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.IsControl(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.IsUnused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.IsByte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n\n            tokens[token_id] = text\n            scores[token_id] = score\n            toktypes[token_id] = toktype\n\n        added_tokens_file = self.dir_model / 'added_tokens.json'\n        if added_tokens_file.is_file():\n            with open(added_tokens_file, \"r\", encoding=\"utf-8\") as f:\n                added_tokens_json = json.load(f)\n                for key in added_tokens_json:\n                    token_id = added_tokens_json[key]\n                    if token_id >= vocab_size:\n                        logger.warning(f'ignore token {token_id}: id is out of range, max={vocab_size - 1}')\n                        continue\n\n                    tokens[token_id] = key.encode(\"utf-8\")\n                    scores[token_id] = -1000.0\n                    toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                added_tokens_decoder = tokenizer_config_json.get(\"added_tokens_decoder\", {})\n                for token_id, token_data in added_tokens_decoder.items():\n                    token_id = int(token_id)\n                    token: str = token_data[\"content\"]\n                    if toktypes[token_id] != SentencePieceTokenTypes.UNUSED:\n                        if tokens[token_id] != token.encode(\"utf-8\"):\n                            logger.warning(f'replacing token {token_id}: {tokens[token_id].decode(\"utf-8\")!r} -> {token!r}')\n                    if token_data.get(\"special\") or self.does_token_look_special(token):\n                        toktypes[token_id] = SentencePieceTokenTypes.CONTROL\n                    else:\n                        token = token.replace(b\"\\xe2\\x96\\x81\".decode(\"utf-8\"), \" \")  # pre-normalize user-defined spaces\n                        toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n\n                    scores[token_id] = -1000.0\n                    tokens[token_id] = token.encode(\"utf-8\")\n\n        if vocab_size > len(tokens):\n            pad_count = vocab_size - len(tokens)\n            logger.debug(f\"Padding vocab with {pad_count} token(s) - [PAD1] through [PAD{pad_count}]\")\n            for i in range(1, pad_count + 1):\n                tokens.append(bytes(f\"[PAD{i}]\", encoding=\"utf-8\"))\n                scores.append(-1000.0)\n                toktypes.append(SentencePieceTokenTypes.UNUSED)\n\n        return tokens, scores, toktypes\n\n    def _set_vocab_llama_hf(self):\n        vocab = gguf.LlamaHfVocab(self.dir_model)\n        tokens = []\n        scores = []\n        toktypes = []\n\n        for text, score, toktype in vocab.all_tokens():\n            tokens.append(text)\n            scores.append(score)\n            toktypes.append(toktype)\n\n        assert len(tokens) == vocab.vocab_size\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def _set_vocab_builtin(self, model_name: Literal[\"gpt-neox\", \"llama-spm\"], vocab_size: int):\n        tokenizer_path = Path(sys.path[0]) / \"models\" / f\"ggml-vocab-{model_name}.gguf\"\n        logger.warning(f\"Using tokenizer from '{os.path.relpath(tokenizer_path, os.getcwd())}'\")\n        vocab_reader = gguf.GGUFReader(tokenizer_path, \"r\")\n\n        default_pre = \"mpt\" if model_name == \"gpt-neox\" else \"default\"\n\n        field = vocab_reader.get_field(gguf.Keys.Tokenizer.MODEL)\n        assert field  # tokenizer model\n        self.gguf_writer.add_tokenizer_model(bytes(field.parts[-1]).decode(\"utf-8\"))\n\n        field = vocab_reader.get_field(gguf.Keys.Tokenizer.PRE)\n        self.gguf_writer.add_tokenizer_pre(bytes(field.parts[-1]).decode(\"utf-8\") if field else default_pre)\n\n        field = vocab_reader.get_field(gguf.Keys.Tokenizer.LIST)\n        assert field  # token list\n        self.gguf_writer.add_token_list([bytes(field.parts[i]) for i in field.data][:vocab_size])\n\n        if model_name == \"llama-spm\":\n            field = vocab_reader.get_field(gguf.Keys.Tokenizer.SCORES)\n            assert field  # token scores\n            self.gguf_writer.add_token_scores([field.parts[i].tolist()[0] for i in field.data][:vocab_size])\n\n        field = vocab_reader.get_field(gguf.Keys.Tokenizer.TOKEN_TYPE)\n        assert field  # token types\n        self.gguf_writer.add_token_types([field.parts[i].tolist()[0] for i in field.data][:vocab_size])\n\n        if model_name != \"llama-spm\":\n            field = vocab_reader.get_field(gguf.Keys.Tokenizer.MERGES)\n            assert field  # token merges\n            self.gguf_writer.add_token_merges([bytes(field.parts[i]) for i in field.data])\n\n        if (field := vocab_reader.get_field(gguf.Keys.Tokenizer.BOS_ID)) is not None:\n            self.gguf_writer.add_bos_token_id(field.parts[-1].tolist()[0])\n        if (field := vocab_reader.get_field(gguf.Keys.Tokenizer.EOS_ID)) is not None:\n            self.gguf_writer.add_eos_token_id(field.parts[-1].tolist()[0])\n        if (field := vocab_reader.get_field(gguf.Keys.Tokenizer.UNK_ID)) is not None:\n            self.gguf_writer.add_unk_token_id(field.parts[-1].tolist()[0])\n        if (field := vocab_reader.get_field(gguf.Keys.Tokenizer.PAD_ID)) is not None:\n            self.gguf_writer.add_pad_token_id(field.parts[-1].tolist()[0])\n        if (field := vocab_reader.get_field(gguf.Keys.Tokenizer.ADD_BOS)) is not None:\n            self.gguf_writer.add_add_bos_token(field.parts[-1].tolist()[0])\n        if (field := vocab_reader.get_field(gguf.Keys.Tokenizer.ADD_EOS)) is not None:\n            self.gguf_writer.add_add_eos_token(field.parts[-1].tolist()[0])\n\n\n@Model.register(\"GPTNeoXForCausalLM\")\nclass GPTNeoXModel(Model):\n    model_arch = gguf.MODEL_ARCH.GPTNEOX\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"num_hidden_layers\"]\n\n        self.gguf_writer.add_context_length(self.hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_rope_dimension_count(\n            int(self.hparams[\"rotary_pct\"] * (self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])),\n        )\n        self.gguf_writer.add_head_count(self.hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_parallel_residual(self.hparams.get(\"use_parallel_residual\", True))\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_eps\"])\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        n_head = self.hparams.get(\"n_head\", self.hparams.get(\"num_attention_heads\"))\n        n_embed = self.hparams.get(\"hidden_size\", self.hparams.get(\"n_embed\"))\n\n        tensors: list[tuple[str, Tensor]] = []\n\n        if re.match(r\"gpt_neox\\.layers\\.\\d+\\.attention\\.query_key_value\\.weight\", name):\n            # Map bloom-style qkv_linear to gpt-style qkv_linear\n            # bloom: https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py#L238-L252  # noqa\n            # gpt-2: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L312  # noqa\n            qkv_weights = data_torch.reshape((n_head, 3, n_embed // n_head, n_embed))\n            data_torch = torch.cat(\n                (\n                    qkv_weights[:, 0, :, :].reshape((-1, n_embed)),\n                    qkv_weights[:, 1, :, :].reshape((-1, n_embed)),\n                    qkv_weights[:, 2, :, :].reshape((-1, n_embed)),\n                ),\n                dim=0,\n            )\n            logger.info(\"re-format attention.linear_qkv.weight\")\n        elif re.match(r\"gpt_neox\\.layers\\.\\d+\\.attention\\.query_key_value\\.bias\", name):\n            qkv_bias = data_torch.reshape((n_head, 3, n_embed // n_head))\n            data_torch = torch.cat(\n                (\n                    qkv_bias[:, 0, :].reshape((n_embed,)),\n                    qkv_bias[:, 1, :].reshape((n_embed,)),\n                    qkv_bias[:, 2, :].reshape((n_embed,)),\n                ),\n                dim=0,\n            )\n            logger.info(\"re-format attention.linear_qkv.bias\")\n\n        tensors.append((self.map_tensor_name(name), data_torch))\n\n        return tensors\n\n\n@Model.register(\"BloomForCausalLM\", \"BloomModel\")\nclass BloomModel(Model):\n    model_arch = gguf.MODEL_ARCH.BLOOM\n\n    def set_gguf_parameters(self):\n        n_embed = self.hparams.get(\"hidden_size\", self.hparams.get(\"n_embed\"))\n        n_head = self.hparams.get(\"n_head\", self.hparams.get(\"num_attention_heads\"))\n        self.gguf_writer.add_context_length(self.hparams.get(\"seq_length\", n_embed))\n        self.gguf_writer.add_embedding_length(n_embed)\n        self.gguf_writer.add_feed_forward_length(4 * n_embed)\n        self.gguf_writer.add_block_count(self.hparams[\"n_layer\"])\n        self.gguf_writer.add_head_count(n_head)\n        self.gguf_writer.add_head_count_kv(n_head)\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        n_head = self.hparams.get(\"n_head\", self.hparams.get(\"num_attention_heads\"))\n        n_embed = self.hparams.get(\"hidden_size\", self.hparams.get(\"n_embed\"))\n\n        name = re.sub(r'transformer\\.', '', name)\n\n        tensors: list[tuple[str, Tensor]] = []\n\n        if re.match(r\"h\\.\\d+\\.self_attention\\.query_key_value\\.weight\", name):\n            # Map bloom-style qkv_linear to gpt-style qkv_linear\n            # bloom: https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py#L238-L252  # noqa\n            # gpt-2: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L312  # noqa\n            qkv_weights = data_torch.reshape((n_head, 3, n_embed // n_head, n_embed))\n            data_torch = torch.cat(\n                (\n                    qkv_weights[:, 0, :, :].reshape((-1, n_embed)),\n                    qkv_weights[:, 1, :, :].reshape((-1, n_embed)),\n                    qkv_weights[:, 2, :, :].reshape((-1, n_embed)),\n                ),\n                dim=0,\n            )\n            logger.info(\"re-format attention.linear_qkv.weight\")\n        elif re.match(r\"h\\.\\d+\\.self_attention\\.query_key_value\\.bias\", name):\n            qkv_bias = data_torch.reshape((n_head, 3, n_embed // n_head))\n            data_torch = torch.cat(\n                (\n                    qkv_bias[:, 0, :].reshape((n_embed,)),\n                    qkv_bias[:, 1, :].reshape((n_embed,)),\n                    qkv_bias[:, 2, :].reshape((n_embed,)),\n                ),\n                dim=0,\n            )\n            logger.info(\"re-format attention.linear_qkv.bias\")\n\n        tensors.append((self.map_tensor_name(name), data_torch))\n\n        if name == \"word_embeddings.weight\":\n            assert self.tensor_names is not None\n\n            # TODO: tie them at runtime, don't duplicate in the model file\n            if all(s not in self.tensor_names for s in (\"lm_head.weight\", \"output.weight\")):\n                tensors.append((self.format_tensor_name(gguf.MODEL_TENSOR.OUTPUT), data_torch))\n\n        return tensors\n\n\n@Model.register(\"MPTForCausalLM\")\nclass MPTModel(Model):\n    model_arch = gguf.MODEL_ARCH.MPT\n\n    def set_vocab(self):\n        try:\n            self._set_vocab_gpt2()\n        except Exception:\n            # Fallback for SEA-LION model\n            self._set_vocab_sentencepiece()\n            self.gguf_writer.add_add_bos_token(False)\n            self.gguf_writer.add_pad_token_id(3)\n            self.gguf_writer.add_eos_token_id(1)\n            self.gguf_writer.add_unk_token_id(0)\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"n_layers\"]\n        self.gguf_writer.add_context_length(self.hparams[\"max_seq_len\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"d_model\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"d_model\"])\n        self.gguf_writer.add_head_count(self.hparams[\"n_heads\"])\n        if kv_n_heads := self.hparams[\"attn_config\"].get(\"kv_n_heads\"):\n            self.gguf_writer.add_head_count_kv(kv_n_heads)\n        self.gguf_writer.add_layer_norm_eps(1e-5)\n        if self.hparams[\"attn_config\"][\"clip_qkv\"] is not None:\n            self.gguf_writer.add_clamp_kqv(self.hparams[\"attn_config\"][\"clip_qkv\"])\n        if self.hparams[\"attn_config\"][\"alibi\"]:\n            self.gguf_writer.add_max_alibi_bias(self.hparams[\"attn_config\"][\"alibi_bias_max\"])\n        else:\n            self.gguf_writer.add_max_alibi_bias(0.0)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        if \"scales\" in name:\n            new_name = self.map_tensor_name(name, try_suffixes=(\".weight\", \".bias\", \".scales\"))\n            new_name = new_name.replace(\"scales\", \"act.scales\")\n        else:\n            new_name = self.map_tensor_name(name, try_suffixes=(\".weight\", \".bias\"))\n\n        return [(new_name, data_torch)]\n\n\n@Model.register(\"OrionForCausalLM\")\nclass OrionModel(Model):\n    model_arch = gguf.MODEL_ARCH.ORION\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"num_hidden_layers\"]\n        head_count = self.hparams[\"num_attention_heads\"]\n        head_count_kv = self.hparams.get(\"num_key_value_heads\", head_count)\n\n        ctx_length = 0\n        if \"max_sequence_length\" in self.hparams:\n            ctx_length = self.hparams[\"max_sequence_length\"]\n        elif \"max_position_embeddings\" in self.hparams:\n            ctx_length = self.hparams[\"max_position_embeddings\"]\n        elif \"model_max_length\" in self.hparams:\n            ctx_length = self.hparams[\"model_max_length\"]\n        else:\n            raise ValueError(\"gguf: can not find ctx length parameter.\")\n\n        self.gguf_writer.add_file_type(self.ftype)\n        self.gguf_writer.add_tensor_data_layout(\"Meta AI original pth\")\n        self.gguf_writer.add_context_length(ctx_length)\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_head_count(head_count)\n        self.gguf_writer.add_head_count_kv(head_count_kv)\n        # note: config provides rms norm but it is actually layer norm\n        # ref:  https://huggingface.co/OrionStarAI/Orion-14B-Chat/blob/276a17221ce42beb45f66fac657a41540e71f4f5/modeling_orion.py#L570-L571\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"rms_norm_eps\"])\n\n\n@Model.register(\"BaichuanForCausalLM\", \"BaiChuanForCausalLM\")\nclass BaichuanModel(Model):\n    model_arch = gguf.MODEL_ARCH.BAICHUAN\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"num_hidden_layers\"]\n        head_count = self.hparams[\"num_attention_heads\"]\n        head_count_kv = self.hparams.get(\"num_key_value_heads\", head_count)\n\n        ctx_length = 0\n        if \"max_sequence_length\" in self.hparams:\n            ctx_length = self.hparams[\"max_sequence_length\"]\n        elif \"max_position_embeddings\" in self.hparams:\n            ctx_length = self.hparams[\"max_position_embeddings\"]\n        elif \"model_max_length\" in self.hparams:\n            ctx_length = self.hparams[\"model_max_length\"]\n        else:\n            raise ValueError(\"gguf: can not find ctx length parameter.\")\n\n        self.gguf_writer.add_tensor_data_layout(\"Meta AI original pth\")\n        self.gguf_writer.add_context_length(ctx_length)\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_rope_dimension_count(self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count(head_count)\n        self.gguf_writer.add_head_count_kv(head_count_kv)\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"rms_norm_eps\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n        if self.hparams.get(\"rope_scaling\") is not None and \"factor\" in self.hparams[\"rope_scaling\"]:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"linear\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n                self.gguf_writer.add_rope_scaling_factor(self.hparams[\"rope_scaling\"][\"factor\"])\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        head_count = self.hparams[\"num_attention_heads\"]\n        head_count_kv = self.hparams.get(\"num_key_value_heads\", head_count)\n\n        tensors: list[tuple[str, Tensor]] = []\n\n        if bid is not None and name == f\"model.layers.{bid}.self_attn.W_pack.weight\":\n            logger.info(f\"Unpacking and permuting layer {bid}\")\n            tensors = [\n                (self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_Q, bid),\n                    self._reverse_hf_permute_part(data_torch, 0, head_count, head_count)),\n                (self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_K, bid),\n                    self._reverse_hf_permute_part(data_torch, 1, head_count, head_count_kv)),\n                (self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_V, bid),\n                    self._reverse_hf_part(data_torch, 2)),\n            ]\n        else:\n            tensors = [(self.map_tensor_name(name), data_torch)]\n\n        return tensors\n\n    def _reverse_hf_permute(self, weights: Tensor, n_head: int, n_kv_head: int | None = None) -> Tensor:\n        if n_kv_head is not None and n_head != n_kv_head:\n            n_head //= n_kv_head\n\n        return (\n            weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n            .swapaxes(1, 2)\n            .reshape(weights.shape)\n        )\n\n    def _reverse_hf_permute_part(\n        self, weights: Tensor, n_part: int, n_head: int, n_head_kv: int | None = None,\n    ) -> Tensor:\n        r = weights.shape[0] // 3\n        return self._reverse_hf_permute(weights[r * n_part:r * n_part + r, ...], n_head, n_head_kv)\n\n    def _reverse_hf_part(self, weights: Tensor, n_part: int) -> Tensor:\n        r = weights.shape[0] // 3\n        return weights[r * n_part:r * n_part + r, ...]\n\n\n@Model.register(\"XverseForCausalLM\")\nclass XverseModel(Model):\n    model_arch = gguf.MODEL_ARCH.XVERSE\n\n    def set_vocab(self):\n        assert (self.dir_model / \"tokenizer.json\").is_file()\n        dir_model = self.dir_model\n        hparams = self.hparams\n\n        tokens: list[bytes] = []\n        toktypes: list[int] = []\n\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(dir_model)\n        vocab_size = hparams.get(\"vocab_size\", len(tokenizer.vocab))\n        # Since we are checking the maximum index, we need to ensure it's strictly less than vocab_size,\n        # because vocab_size is the count of items, and indexes start at 0.\n        max_vocab_index = max(tokenizer.get_vocab().values())\n        if max_vocab_index >= vocab_size:\n            raise ValueError(\"Vocabulary size exceeds expected maximum size.\")\n\n        reverse_vocab: dict[int, str] = {id_: encoded_tok for encoded_tok, id_ in tokenizer.vocab.items()}\n        added_vocab = tokenizer.get_added_vocab()\n\n        for token_id in range(vocab_size):\n            token_text = reverse_vocab[token_id].encode('utf-8')\n            # replace \"\\x00\" to string with length > 0\n            if token_text == b\"\\x00\":\n                toktype = gguf.TokenType.BYTE  # special\n                token_text = f\"<{token_text}>\".encode('utf-8')\n            elif re.fullmatch(br\"<0x[0-9A-Fa-f]{2}>\", token_text):\n                toktype = gguf.TokenType.BYTE  # special\n            elif reverse_vocab[token_id] in added_vocab:\n                if tokenizer.added_tokens_decoder[token_id].special:\n                    toktype = gguf.TokenType.CONTROL\n                else:\n                    toktype = gguf.TokenType.USER_DEFINED\n            else:\n                toktype = gguf.TokenType.NORMAL\n\n            tokens.append(token_text)\n            toktypes.append(toktype)\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"num_hidden_layers\"]\n        head_count = self.hparams[\"num_attention_heads\"]\n        head_count_kv = self.hparams.get(\"num_key_value_heads\", head_count)\n\n        ctx_length = 0\n        if \"max_sequence_length\" in self.hparams:\n            ctx_length = self.hparams[\"max_sequence_length\"]\n        elif \"max_position_embeddings\" in self.hparams:\n            ctx_length = self.hparams[\"max_position_embeddings\"]\n        elif \"model_max_length\" in self.hparams:\n            ctx_length = self.hparams[\"model_max_length\"]\n        else:\n            raise ValueError(\"gguf: can not find ctx length parameter.\")\n\n        self.gguf_writer.add_tensor_data_layout(\"Meta AI original pth\")\n        self.gguf_writer.add_context_length(ctx_length)\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_rope_dimension_count(self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count(head_count)\n        self.gguf_writer.add_head_count_kv(head_count_kv)\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"rms_norm_eps\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n        if self.hparams.get(\"rope_scaling\") is not None and \"factor\" in self.hparams[\"rope_scaling\"]:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"linear\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n                self.gguf_writer.add_rope_scaling_factor(self.hparams[\"rope_scaling\"][\"factor\"])\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        head_count = self.hparams[\"num_attention_heads\"]\n        head_count_kv = self.hparams.get(\"num_key_value_heads\", head_count)\n\n        # HF models permute some of the tensors, so we need to undo that\n        if name.endswith(\"q_proj.weight\"):\n            data_torch = self._reverse_hf_permute(data_torch, head_count, head_count)\n        if name.endswith(\"k_proj.weight\"):\n            data_torch = self._reverse_hf_permute(data_torch, head_count, head_count_kv)\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def _reverse_hf_permute(self, weights: Tensor, n_head: int, n_kv_head: int | None = None) -> Tensor:\n        if n_kv_head is not None and n_head != n_kv_head:\n            n_head //= n_kv_head\n\n        return (\n            weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n            .swapaxes(1, 2)\n            .reshape(weights.shape)\n        )\n\n\n@Model.register(\"FalconForCausalLM\", \"RWForCausalLM\")\nclass FalconModel(Model):\n    model_arch = gguf.MODEL_ARCH.FALCON\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams.get(\"num_hidden_layers\")\n        if block_count is None:\n            block_count = self.hparams[\"n_layer\"]  # old name\n\n        n_head = self.hparams.get(\"num_attention_heads\")\n        if n_head is None:\n            n_head = self.hparams[\"n_head\"]  # old name\n\n        n_head_kv = self.hparams.get(\"num_kv_heads\")\n        if n_head_kv is None:\n            n_head_kv = self.hparams.get(\"n_head_kv\", 1)  # old name\n\n        self.gguf_writer.add_context_length(2048)  # not in config.json\n        self.gguf_writer.add_tensor_data_layout(\"jploski\")  # qkv tensor transform\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_head_count(n_head)\n        self.gguf_writer.add_head_count_kv(n_head_kv)\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        # QKV tensor transform\n        # The original query_key_value tensor contains n_head_kv \"kv groups\",\n        # each consisting of n_head/n_head_kv query weights followed by one key\n        # and one value weight (shared by all query heads in the kv group).\n        # This layout makes it a big pain to work with in GGML.\n        # So we rearrange them here,, so that we have n_head query weights\n        # followed by n_head_kv key weights followed by n_head_kv value weights,\n        # in contiguous fashion.\n        # ref: https://github.com/jploski/ggml/blob/falcon40b/examples/falcon/convert-hf-to-ggml.py\n\n        if \"query_key_value\" in name:\n            n_head = self.find_hparam([\"num_attention_heads\", \"n_head\"])\n            n_head_kv = self.find_hparam([\"num_kv_heads\", \"n_head_kv\"], optional=True) or 1\n            head_dim = self.hparams[\"hidden_size\"] // n_head\n\n            qkv = data_torch.view(n_head_kv, n_head // n_head_kv + 2, head_dim, head_dim * n_head)\n            q = qkv[:, :-2].reshape(n_head * head_dim, head_dim * n_head)\n            k = qkv[:, [-2]].reshape(n_head_kv * head_dim, head_dim * n_head)\n            v = qkv[:, [-1]].reshape(n_head_kv * head_dim, head_dim * n_head)\n            data_torch = torch.cat((q, k, v)).reshape_as(data_torch)\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"GPTBigCodeForCausalLM\")\nclass StarCoderModel(Model):\n    model_arch = gguf.MODEL_ARCH.STARCODER\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"n_layer\"]\n\n        self.gguf_writer.add_context_length(self.hparams[\"n_positions\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_head_count_kv(1)\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n\n@Model.register(\"GPTRefactForCausalLM\")\nclass RefactModel(Model):\n    model_arch = gguf.MODEL_ARCH.REFACT\n\n    def set_vocab(self):\n        super().set_vocab()\n\n        # TODO: how to determine special FIM tokens automatically?\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=False,\n                                          special_token_types = ['prefix', 'suffix', 'middle', 'eot'])\n        special_vocab._set_special_token(\"prefix\", 1)\n        special_vocab._set_special_token(\"suffix\", 3)\n        special_vocab._set_special_token(\"middle\", 2)\n        special_vocab.chat_template = None  # do not add it twice\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def set_gguf_parameters(self):\n        hidden_dim = self.hparams[\"n_embd\"]\n        inner_dim = 4 * hidden_dim\n        hidden_dim = int(2 * inner_dim / 3)\n        multiple_of = 256\n        ff_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        block_count = self.hparams[\"n_layer\"]\n\n        # refact uses Alibi. So this is from config.json which might be used by training.\n        self.gguf_writer.add_context_length(self.hparams[\"n_positions\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n\n        self.gguf_writer.add_feed_forward_length(ff_dim)\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_head_count_kv(1)\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        hidden_dim = self.hparams[\"n_embd\"]\n        inner_dim = 4 * hidden_dim\n        hidden_dim = int(2 * inner_dim / 3)\n        multiple_of = 256\n        ff_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n        n_head = self.hparams[\"n_head\"]\n        n_head_kv = 1\n        head_dim = self.hparams[\"n_embd\"] // n_head\n\n        tensors: list[tuple[str, Tensor]] = []\n\n        if bid is not None:\n            if name == f\"transformer.h.{bid}.attn.kv.weight\":\n                tensors.append((self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_K, bid), data_torch[:n_head_kv * head_dim]))\n                tensors.append((self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_V, bid), data_torch[n_head_kv * head_dim:]))\n            elif name == f\"transformer.h.{bid}.attn.q.weight\":\n                tensors.append((self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_Q, bid), data_torch))\n            elif name == f\"transformer.h.{bid}.mlp.gate_up_proj.weight\":\n                tensors.append((self.format_tensor_name(gguf.MODEL_TENSOR.FFN_GATE, bid), data_torch[:ff_dim]))\n                tensors.append((self.format_tensor_name(gguf.MODEL_TENSOR.FFN_UP, bid), data_torch[ff_dim:]))\n\n        if len(tensors) == 0:\n            tensors.append((self.map_tensor_name(name), data_torch))\n\n        return tensors\n\n\n@Model.register(\"StableLmForCausalLM\", \"StableLMEpochForCausalLM\", \"LlavaStableLMEpochForCausalLM\")\nclass StableLMModel(Model):\n    model_arch = gguf.MODEL_ARCH.STABLELM\n\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.json\").is_file():\n            self._set_vocab_gpt2()\n        else:\n            # StableLM 2 1.6B used to have a vocab in a similar format to Qwen's vocab\n            self._set_vocab_qwen()\n\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        block_count = hparams[\"num_hidden_layers\"]\n\n        self.gguf_writer.add_context_length(hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        rotary_factor = self.find_hparam([\"partial_rotary_factor\", \"rope_pct\"])\n        self.gguf_writer.add_rope_dimension_count(int(rotary_factor * (hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"])))\n        self.gguf_writer.add_head_count(hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count_kv(hparams[\"num_key_value_heads\"])\n        self.gguf_writer.add_parallel_residual(hparams[\"use_parallel_residual\"] if \"use_parallel_residual\" in hparams else True)\n        self.gguf_writer.add_layer_norm_eps(self.find_hparam([\"layer_norm_eps\", \"norm_eps\"]))\n        self.gguf_writer.add_file_type(self.ftype)\n\n    _q_norms: list[dict[str, Tensor]] | None = None\n    _k_norms: list[dict[str, Tensor]] | None = None\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        n_head = self.hparams[\"num_attention_heads\"]\n        n_kv_head = self.hparams[\"num_key_value_heads\"]\n\n        if name.find(\"q_layernorm.norms\") != -1:\n            assert bid is not None\n\n            if self._q_norms is None:\n                self._q_norms = [{} for _ in range(self.block_count)]\n\n            self._q_norms[bid][name] = data_torch\n\n            if len(self._q_norms[bid]) >= n_head:\n                return self._stack_qk_norm(bid, n_head, self._q_norms[bid], \"q_layernorm\")\n            else:\n                return []\n\n        if name.find(\"k_layernorm.norms\") != -1:\n            assert bid is not None\n\n            if self._k_norms is None:\n                self._k_norms = [{} for _ in range(self.block_count)]\n\n            self._k_norms[bid][name] = data_torch\n\n            if len(self._k_norms[bid]) >= n_kv_head:\n                return self._stack_qk_norm(bid, n_kv_head, self._k_norms[bid], \"k_layernorm\")\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def _stack_qk_norm(self, bid: int, n_head: int, norms: dict[str, Tensor], layer_name: str = \"q_layernorm\"):\n        datas: list[Tensor] = []\n        # extract the norms in order\n        for xid in range(n_head):\n            ename = f\"model.layers.{bid}.self_attn.{layer_name}.norms.{xid}.weight\"\n            datas.append(norms[ename])\n            del norms[ename]\n        data_torch = torch.stack(datas, dim=0)\n\n        merged_name = f\"model.layers.{bid}.self_attn.{layer_name}.weight\"\n        new_name = self.map_tensor_name(merged_name)\n\n        return [(new_name, data_torch)]\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n        if self._q_norms is not None or self._k_norms is not None:\n            # flatten two `list[dict[str, Tensor]]` into a single `list[str]`\n            norms = (\n                [k for d in self._q_norms for k in d.keys()] if self._q_norms is not None else []\n            ) + (\n                [k for d in self._k_norms for k in d.keys()] if self._k_norms is not None else []\n            )\n            if len(norms) > 0:\n                raise ValueError(f\"Unprocessed norms: {norms}\")\n\n\n@Model.register(\"LLaMAForCausalLM\", \"LlamaForCausalLM\", \"MistralForCausalLM\", \"MixtralForCausalLM\")\nclass LlamaModel(Model):\n    model_arch = gguf.MODEL_ARCH.LLAMA\n\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            try:\n                self._set_vocab_llama_hf()\n            except (FileNotFoundError, TypeError):\n                # Llama 3\n                self._set_vocab_gpt2()\n\n        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n            special_vocab = gguf.SpecialVocab(\n                self.dir_model, load_merges=False,\n                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n            )\n            special_vocab._set_special_token(\"prefix\", 32007)\n            special_vocab._set_special_token(\"suffix\", 32008)\n            special_vocab._set_special_token(\"middle\", 32009)\n            special_vocab._set_special_token(\"eot\",    32010)\n            special_vocab.add_to_gguf(self.gguf_writer)\n\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                if \"add_prefix_space\" in tokenizer_config_json:\n                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n\n        # Apply to granite small models only\n        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n            self.gguf_writer.add_add_bos_token(False)\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n\n        if \"head_dim\" in hparams:\n            rope_dim = hparams[\"head_dim\"]\n        else:\n            rope_dim = hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"]\n        self.gguf_writer.add_rope_dimension_count(rope_dim)\n\n        if self.hparams.get(\"rope_scaling\") is not None and \"factor\" in self.hparams[\"rope_scaling\"]:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"linear\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n                self.gguf_writer.add_rope_scaling_factor(self.hparams[\"rope_scaling\"][\"factor\"])\n\n    @staticmethod\n    def permute(weights: Tensor, n_head: int, n_head_kv: int | None):\n        if n_head_kv is not None and n_head != n_head_kv:\n            n_head = n_head_kv\n        return (weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n                .swapaxes(1, 2)\n                .reshape(weights.shape))\n\n    _experts: list[dict[str, Tensor]] | None = None\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        n_head = self.hparams[\"num_attention_heads\"]\n        n_kv_head = self.hparams.get(\"num_key_value_heads\")\n\n        if name.endswith((\"q_proj.weight\", \"q_proj.bias\")):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_head)\n        if name.endswith((\"k_proj.weight\", \"k_proj.bias\")):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_kv_head)\n\n        # process the experts separately\n        if name.find(\"block_sparse_moe.experts\") != -1:\n            n_experts = self.hparams[\"num_local_experts\"]\n\n            assert bid is not None\n\n            if self._experts is None:\n                self._experts = [{} for _ in range(self.block_count)]\n\n            self._experts[bid][name] = data_torch\n\n            if len(self._experts[bid]) >= n_experts * 3:\n                tensors: list[tuple[str, Tensor]] = []\n\n                # merge the experts into a single 3d tensor\n                for wid in [\"w1\", \"w2\", \"w3\"]:\n                    datas: list[Tensor] = []\n\n                    for xid in range(n_experts):\n                        ename = f\"model.layers.{bid}.block_sparse_moe.experts.{xid}.{wid}.weight\"\n                        datas.append(self._experts[bid][ename])\n                        del self._experts[bid][ename]\n\n                    data_torch = torch.stack(datas, dim=0)\n\n                    merged_name = f\"layers.{bid}.feed_forward.experts.{wid}.weight\"\n\n                    new_name = self.map_tensor_name(merged_name)\n\n                    tensors.append((new_name, data_torch))\n                return tensors\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        if rope_scaling := self.find_hparam([\"rope_scaling\"], optional=True):\n            if rope_scaling.get(\"rope_type\", '').lower() == \"llama3\":\n                base = self.hparams.get(\"rope_theta\", 10000.0)\n                dim = self.hparams.get(\"head_dim\", self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n                freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n\n                factor = rope_scaling.get(\"factor\", 8.0)\n                low_freq_factor = rope_scaling.get(\"low_freq_factor\", 1.0)\n                high_freq_factor = rope_scaling.get(\"high_freq_factor\", 4.0)\n                old_context_len = self.hparams.get(\"original_max_position_embeddings\", 8192)\n\n                low_freq_wavelen = old_context_len / low_freq_factor\n                high_freq_wavelen = old_context_len / high_freq_factor\n                assert low_freq_wavelen != high_freq_wavelen\n\n                rope_factors = []\n                for freq in freqs:\n                    wavelen = 2 * math.pi / freq\n                    if wavelen < high_freq_wavelen:\n                        rope_factors.append(1)\n                    elif wavelen > low_freq_wavelen:\n                        rope_factors.append(factor)\n                    else:\n                        smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n                        rope_factors.append(1 / ((1 - smooth) / factor + smooth))\n\n                yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FREQS), torch.tensor(rope_factors, dtype=torch.float32))\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n        if self._experts is not None:\n            # flatten `list[dict[str, Tensor]]` into `list[str]`\n            experts = [k for d in self._experts for k in d.keys()]\n            if len(experts) > 0:\n                raise ValueError(f\"Unprocessed experts: {experts}\")\n\n\n@Model.register(\"DeciLMForCausalLM\")\nclass DeciModel(Model):\n    model_arch = gguf.MODEL_ARCH.DECI\n\n    @staticmethod\n    def _ffn_mult_to_intermediate_size(ffn_mult: float, n_embd: int) -> int:\n        # DeciLM-specific code\n        intermediate_size = int(2 * ffn_mult * n_embd / 3)\n        return DeciModel._find_multiple(intermediate_size, 256)\n\n    @staticmethod\n    def _find_multiple(n: int, k: int) -> int:\n        # DeciLM-specific code\n        if n % k == 0:\n            return n\n        return n + k - (n % k)\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        if \"block_configs\" in self.hparams: # Llama-3_1-Nemotron-51B\n            _block_configs: list[dict[str,Any]] = self.hparams[\"block_configs\"]\n            assert self.block_count == len(_block_configs)\n            self._num_kv_heads = list()\n            self._num_heads = list()\n            _ffn_multipliers = list()\n            # ***linear attention layer***\n            # if n_heads_in_group is None and replace_with_linear is True\n            # then _num_kv_heads[il] is 0 and _num_heads[il] is num_attention_heads\n            # ***attention-free layer***\n            # if n_heads_in_group is None and replace_with_linear is False\n            # then _num_kv_heads[il] is 0 and _num_heads[il] is 0\n            # ***normal attention-layer***\n            # if n_heads_in_group is not None, then\n            # _num_kv_heads[il] is num_attention_head // n_heads_in_group and\n            # _num_heads[il] is num_attention_head\n            for il in range(len(_block_configs)):\n                if _block_configs[il][\"attention\"][\"n_heads_in_group\"] is None:\n                    if _block_configs[il][\"attention\"][\"replace_with_linear\"] is True:\n                        self._num_kv_heads.append(0)\n                        self._num_heads.append(self.hparams[\"num_attention_heads\"])\n                    else:\n                        self._num_kv_heads.append(0)\n                        self._num_heads.append(0)\n                else:\n                    self._num_kv_heads.append(self.hparams[\"num_attention_heads\"] // _block_configs[il][\"attention\"][\"n_heads_in_group\"])\n                    self._num_heads.append(self.hparams[\"num_attention_heads\"])\n                _ffn_multipliers.append(_block_configs[il][\"ffn\"][\"ffn_mult\"])\n            assert self.block_count == len(self._num_kv_heads)\n            assert self.block_count == len(self._num_heads)\n            assert self.block_count == len(_ffn_multipliers)\n            assert isinstance(self._num_kv_heads, list) and isinstance(self._num_kv_heads[0], int)\n            assert isinstance(self._num_heads, list) and isinstance(self._num_heads[0], int)\n            assert isinstance(_ffn_multipliers, list) and isinstance(_ffn_multipliers[0], float)\n            self._ffn_dims: list[int] = [\n                DeciModel._ffn_mult_to_intermediate_size(multiplier, self.hparams[\"hidden_size\"])\n                for multiplier in _ffn_multipliers\n            ]\n\n    def set_vocab(self):\n        # Please change tokenizer_config.json of Llama-3_1-Nemotron-51B's\n        # eos_token from '|eot_id|' to '|end_of_text|'\n        if self.hparams.get(\"vocab_size\", 128256) == 128256:\n            tokens, toktypes, tokpre = self.get_vocab_base()\n            self.gguf_writer.add_tokenizer_model(\"gpt2\")\n            self.gguf_writer.add_tokenizer_pre(tokpre)\n            self.gguf_writer.add_token_list(tokens)\n            self.gguf_writer.add_token_types(toktypes)\n\n            special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=True)\n            special_vocab.add_to_gguf(self.gguf_writer)\n        else:\n            # DeciLM-7B\n            self._set_vocab_llama_hf()\n\n    def set_gguf_parameters(self):\n        if \"block_configs\" in self.hparams: # Llama-3_1-Nemotron-51B\n            assert self.block_count == len(self._num_kv_heads)\n            assert self.block_count == len(self._num_heads)\n            assert self.block_count == len(self._ffn_dims)\n            if (rope_theta := self.hparams.get(\"rope_theta\")) is not None:\n                self.gguf_writer.add_rope_freq_base(rope_theta)\n            self.gguf_writer.add_head_count_kv(self._num_kv_heads)\n            self.gguf_writer.add_head_count(self._num_heads)\n            self.gguf_writer.add_feed_forward_length(self._ffn_dims)\n            self.gguf_writer.add_block_count(self.block_count)\n            self.gguf_writer.add_context_length(self.hparams[\"max_position_embeddings\"])\n            self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n            self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"rms_norm_eps\"])\n            self.gguf_writer.add_key_length(self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n            self.gguf_writer.add_value_length(self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n            self.gguf_writer.add_file_type(self.ftype)\n        else: # DeciLM-7B\n            super().set_gguf_parameters()\n            if \"num_key_value_heads_per_layer\" in self.hparams: # DeciLM-7B\n                self._num_kv_heads: list[int] = self.hparams[\"num_key_value_heads_per_layer\"]\n                assert self.block_count == len(self._num_kv_heads)\n                self.gguf_writer.add_head_count_kv(self._num_kv_heads)\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n\n        if \"head_dim\" in hparams:\n            rope_dim = hparams[\"head_dim\"]\n        else:\n            rope_dim = hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"]\n        self.gguf_writer.add_rope_dimension_count(rope_dim)\n\n        if self.hparams.get(\"rope_scaling\") is not None and \"factor\" in self.hparams[\"rope_scaling\"]:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"linear\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n                self.gguf_writer.add_rope_scaling_factor(self.hparams[\"rope_scaling\"][\"factor\"])\n\n    @staticmethod\n    def permute(weights: Tensor, n_head: int, n_head_kv: int | None):\n        if n_head_kv is not None and n_head != n_head_kv:\n            n_head = n_head_kv\n        return (weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n                .swapaxes(1, 2)\n                .reshape(weights.shape))\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        n_head = self.hparams[\"num_attention_heads\"]\n        if bid is not None:\n            if \"num_key_value_heads_per_layer\" in self.hparams:\n                n_kv_head = self.hparams[\"num_key_value_heads_per_layer\"][bid]\n            elif \"block_configs\" in self.hparams:\n                n_kv_head = self._num_kv_heads[bid]\n                n_head = self._num_heads[bid]\n            else:\n                n_kv_head = self.hparams.get(\"num_key_value_heads\")\n        else:\n            n_kv_head = self.hparams.get(\"num_key_value_heads\")\n\n        if name.endswith((\"q_proj.weight\", \"q_proj.bias\")):\n            data_torch = DeciModel.permute(data_torch, n_head, n_head)\n        if name.endswith((\"k_proj.weight\", \"k_proj.bias\")):\n            data_torch = DeciModel.permute(data_torch, n_head, n_kv_head)\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        if rope_scaling := self.find_hparam([\"rope_scaling\"], optional=True):\n            if rope_scaling.get(\"rope_type\", '').lower() == \"llama3\":\n                base = self.hparams.get(\"rope_theta\", 10000.0)\n                dim = self.hparams.get(\"head_dim\", self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n                freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n\n                factor = rope_scaling.get(\"factor\", 8.0)\n                low_freq_factor = rope_scaling.get(\"low_freq_factor\", 1.0)\n                high_freq_factor = rope_scaling.get(\"high_freq_factor\", 4.0)\n                old_context_len = self.hparams.get(\"original_max_position_embeddings\", 8192)\n\n                low_freq_wavelen = old_context_len / low_freq_factor\n                high_freq_wavelen = old_context_len / high_freq_factor\n                assert low_freq_wavelen != high_freq_wavelen\n\n                rope_factors = []\n                for freq in freqs:\n                    wavelen = 2 * math.pi / freq\n                    if wavelen < high_freq_wavelen:\n                        rope_factors.append(1)\n                    elif wavelen > low_freq_wavelen:\n                        rope_factors.append(factor)\n                    else:\n                        smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n                        rope_factors.append(1 / ((1 - smooth) / factor + smooth))\n\n                yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FREQS), torch.tensor(rope_factors, dtype=torch.float32))\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n\n@Model.register(\"BitnetForCausalLM\")\nclass BitnetModel(Model):\n    model_arch = gguf.MODEL_ARCH.BITNET\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n        self.gguf_writer.add_rope_scaling_factor(1.0)\n\n    def weight_quant(self, weight: Tensor) -> Tensor:\n        dtype = weight.dtype\n        weight = weight.float()\n        scale = weight.abs().mean().clamp(min=1e-5)\n        iscale = 1 / scale\n        # TODO: multiply by the scale directly instead of inverting it twice\n        # (this is also unnecessarily doubly inverted upstream)\n        # ref: https://huggingface.co/1bitLLM/bitnet_b1_58-3B/blob/af89e318d78a70802061246bf037199d2fb97020/utils_quant.py#L10\n        result = (weight * iscale).round().clamp(-1, 1) / iscale\n        return result.type(dtype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        new_name = self.map_tensor_name(name)\n\n        if any(self.match_model_tensor_name(new_name, key, bid) for key in [\n            gguf.MODEL_TENSOR.ATTN_Q,\n            gguf.MODEL_TENSOR.ATTN_K,\n            gguf.MODEL_TENSOR.ATTN_V,\n            gguf.MODEL_TENSOR.ATTN_OUT,\n            gguf.MODEL_TENSOR.FFN_UP,\n            gguf.MODEL_TENSOR.FFN_DOWN,\n            gguf.MODEL_TENSOR.FFN_GATE,\n        ]):\n            # transform weight into 1/0/-1 (in fp32)\n            data_torch = self.weight_quant(data_torch)\n\n        yield (new_name, data_torch)\n\n\n@Model.register(\"GrokForCausalLM\")\nclass GrokModel(Model):\n    model_arch = gguf.MODEL_ARCH.GROK\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n\n    _experts: list[dict[str, Tensor]] | None = None\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # process the experts separately\n        if name.find(\".moe.\") != -1:\n            n_experts = self.hparams[\"num_local_experts\"]\n\n            assert bid is not None\n\n            if self._experts is None:\n                self._experts = [{} for _ in range(self.block_count)]\n\n            self._experts[bid][name] = data_torch\n\n            if len(self._experts[bid]) >= n_experts * 3:\n                tensors: list[tuple[str, Tensor]] = []\n\n                # merge the experts into a single 3d tensor\n                for wid in [\"linear\", \"linear_1\", \"linear_v\"]:\n                    datas: list[Tensor] = []\n\n                    for xid in range(n_experts):\n                        ename = f\"transformer.decoder_layer.{bid}.moe.{xid}.{wid}.weight\"\n                        datas.append(self._experts[bid][ename])\n                        del self._experts[bid][ename]\n\n                    data_torch = torch.stack(datas, dim=0)\n\n                    merged_name = f\"transformer.decoder_layer.{bid}.moe.{wid}.weight\"\n\n                    new_name = self.map_tensor_name(merged_name)\n\n                    tensors.append((new_name, data_torch))\n                return tensors\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"DbrxForCausalLM\")\nclass DbrxModel(Model):\n    model_arch = gguf.MODEL_ARCH.DBRX\n\n    def set_gguf_parameters(self):\n        ffn_config = self.hparams[\"ffn_config\"]\n        attn_config = self.hparams[\"attn_config\"]\n        self.gguf_writer.add_block_count(self.hparams[\"n_layers\"])\n\n        self.gguf_writer.add_context_length(self.hparams[\"max_seq_len\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"d_model\"])\n        self.gguf_writer.add_feed_forward_length(ffn_config[\"ffn_hidden_size\"])\n\n        self.gguf_writer.add_head_count(self.hparams[\"n_heads\"])\n        self.gguf_writer.add_head_count_kv(attn_config[\"kv_n_heads\"])\n\n        self.gguf_writer.add_rope_freq_base(attn_config[\"rope_theta\"])\n\n        self.gguf_writer.add_clamp_kqv(attn_config[\"clip_qkv\"])\n\n        self.gguf_writer.add_expert_count(ffn_config[\"moe_num_experts\"])\n        self.gguf_writer.add_expert_used_count(ffn_config[\"moe_top_k\"])\n\n        self.gguf_writer.add_layer_norm_eps(1e-5)\n\n        self.gguf_writer.add_file_type(self.ftype)\n        logger.info(f\"gguf: file type = {self.ftype}\")\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        n_expert = self.hparams[\"ffn_config\"][\"moe_num_experts\"]\n        n_ff = self.hparams[\"ffn_config\"][\"ffn_hidden_size\"]\n        n_embd = self.hparams[\"d_model\"]\n\n        # Specific behavior for experts tensors: suffix .weight, view as 3D and transpose\n        # original implementation expects (n_expert, n_ff, n_embd) for all experts weights\n        # But llama.cpp moe graph works differently\n        # AND the dimensions in ggml are typically in the reverse order of the pytorch dimensions\n        # so (n_expert, n_ff, n_embd) in pytorch is {n_embd, n_ff, n_expert} in ggml_tensor\n        exp_tensor_names = {\"ffn.experts.mlp.w1\": None,       # LLM_TENSOR_FFN_GATE_EXPS ggml_tensor->ne{n_embd, n_ff,   n_expert}\n                            \"ffn.experts.mlp.w2\": (0, 2, 1),  # LLM_TENSOR_FFN_DOWN_EXPS ggml_tensor->ne{n_ff,   n_embd, n_expert}\n                            \"ffn.experts.mlp.v1\": None}       # LLM_TENSOR_FFN_UP_EXPS   ggml_tensor->ne{n_embd, n_ff,   n_expert}\n        experts = False\n\n        for exp_tensor_name in exp_tensor_names.keys():\n            if name.find(exp_tensor_name) != -1 and name.find(\".weight\") == -1:\n                experts = True\n                data_torch = data_torch.view(n_expert, n_ff, n_embd)\n                if (permute_tensor := exp_tensor_names[exp_tensor_name]) is not None:\n                    data_torch = data_torch.permute(*permute_tensor)\n                break\n\n        # map tensor names\n        # In MoE models the ffn tensors are typically most of the model weights,\n        # and need to be quantizable. Quantize expects tensor names to be suffixed by .weight.\n        # Every other model has the weight names ending in .weight,\n        # let's assume that is the convention which is not the case for dbrx:\n        # https://huggingface.co/databricks/dbrx-instruct/blob/main/model.safetensors.index.json#L15\n        new_name = self.map_tensor_name(name if not experts else name + \".weight\", try_suffixes=(\".weight\",))\n\n        return [(new_name, data_torch)]\n\n    def tensor_force_quant(self, name: str, new_name: str, bid: int | None, n_dims: int) -> gguf.GGMLQuantizationType | bool:\n        del name, new_name, bid  # unused\n\n        return n_dims > 1\n\n\n@Model.register(\"MiniCPMForCausalLM\")\nclass MiniCPMModel(Model):\n    model_arch = gguf.MODEL_ARCH.MINICPM\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        embedding_scale = float(self.hparams[\"scale_emb\"])\n        self.gguf_writer.add_embedding_scale(embedding_scale)\n        logger.info(f\"gguf: (minicpm) embedding_scale = {embedding_scale}\")\n        residual_scale = self.hparams[\"scale_depth\"] / self.hparams[\"num_hidden_layers\"] ** 0.5\n        self.gguf_writer.add_residual_scale(residual_scale)\n        logger.info(f\"gguf: (minicpm) residual_scale = {residual_scale}\")\n        logit_scale = self.hparams[\"hidden_size\"] / self.hparams[\"dim_model_base\"]\n        self.gguf_writer.add_logit_scale(logit_scale)\n        logger.info(f\"gguf: (minicpm) logit_scale = {logit_scale}\")\n        if self.hparams.get(\"rope_scaling\") is not None:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"longrope\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LONGROPE)\n                logger.info(f\"gguf: (minicpm) rope_scaling_type = {gguf.RopeScalingType.LONGROPE}\")\n\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        rope_dims = self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"]\n\n        rope_scaling = self.find_hparam(['rope_scaling'], True)\n        if rope_scaling is not None:\n            long_factors = rope_scaling.get('long_factor', None)\n            short_factors = rope_scaling.get('short_factor', None)\n\n            if long_factors is None or short_factors is None:\n                raise KeyError('Missing the required key rope_scaling.long_factor or rope_scaling_short_factor')\n\n            if len(long_factors) != len(short_factors) or len(long_factors) != rope_dims / 2:\n                raise ValueError(f'The length of rope long and short factors must be {rope_dims / 2}')\n\n            yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FACTORS_LONG), torch.tensor(long_factors, dtype=torch.float32))\n            yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FACTORS_SHORT), torch.tensor(short_factors, dtype=torch.float32))\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        n_head = self.hparams[\"num_attention_heads\"]\n        n_kv_head = self.hparams.get(\"num_key_value_heads\")\n\n        # HF models permute some of the tensors, so we need to undo that\n        if name.endswith((\"q_proj.weight\")):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_head)\n        if name.endswith((\"k_proj.weight\")):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_kv_head)\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"MiniCPM3ForCausalLM\")\nclass MiniCPM3Model(Model):\n    model_arch = gguf.MODEL_ARCH.MINICPM3\n\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n\n        self.gguf_writer.add_file_type(self.ftype)\n        self.gguf_writer.add_context_length(hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        self.gguf_writer.add_head_count(hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count_kv(hparams[\"num_key_value_heads\"])\n        self.gguf_writer.add_layer_norm_rms_eps(hparams[\"rms_norm_eps\"])\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n        if \"q_lora_rank\" in hparams and hparams[\"q_lora_rank\"] is not None:\n            self.gguf_writer.add_q_lora_rank(hparams[\"q_lora_rank\"])\n        self.gguf_writer.add_kv_lora_rank(hparams[\"kv_lora_rank\"])\n        self.gguf_writer.add_key_length(hparams[\"qk_nope_head_dim\"] + hparams[\"qk_rope_head_dim\"])\n        self.gguf_writer.add_rope_dimension_count(hparams[\"qk_rope_head_dim\"])\n\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        rope_scaling = self.find_hparam(['rope_scaling'], True)\n        if rope_scaling is not None:\n            rope_dims = self.hparams[\"qk_rope_head_dim\"]\n\n            long_factors = rope_scaling.get('long_factor', None)\n            short_factors = rope_scaling.get('short_factor', None)\n\n            if long_factors is None or short_factors is None:\n                raise KeyError('Missing the required key rope_scaling.long_factor or rope_scaling_short_factor')\n\n            if len(long_factors) != len(short_factors) or len(long_factors) != rope_dims / 2:\n                raise ValueError(f'The length of rope long and short factors must be {rope_dims / 2}')\n\n            yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FACTORS_LONG), torch.tensor(long_factors, dtype=torch.float32))\n            yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FACTORS_SHORT), torch.tensor(short_factors, dtype=torch.float32))\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n    def _reverse_hf_permute(self, weights: Tensor, n_head: int, n_kv_head: int | None = None) -> Tensor:\n        if n_kv_head is not None and n_head != n_kv_head:\n            n_head //= n_kv_head\n\n        return (\n            weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n            .swapaxes(1, 2)\n            .reshape(weights.shape)\n        )\n\n\n@Model.register(\"QWenLMHeadModel\")\nclass QwenModel(Model):\n    model_arch = gguf.MODEL_ARCH.QWEN\n\n    @staticmethod\n    def token_bytes_to_string(b):\n        from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n        byte_encoder = bytes_to_unicode()\n        return ''.join([byte_encoder[ord(char)] for char in b.decode('latin-1')])\n\n    @staticmethod\n    def bpe(mergeable_ranks: dict[bytes, int], token: bytes, max_rank: int | None = None) -> list[bytes]:\n        parts = [bytes([b]) for b in token]\n        while True:\n            min_idx = None\n            min_rank = None\n            for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n                rank = mergeable_ranks.get(pair[0] + pair[1])\n                if rank is not None and (min_rank is None or rank < min_rank):\n                    min_idx = i\n                    min_rank = rank\n            if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n                break\n            assert min_idx is not None\n            parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n        return parts\n\n    def set_vocab(self):\n        self._set_vocab_qwen()\n\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_context_length(self.hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_block_count(self.hparams[\"num_hidden_layers\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_rope_freq_base(self.hparams[\"rotary_emb_base\"])\n        self.gguf_writer.add_rope_dimension_count(self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count(self.hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n\n@Model.register(\"Qwen2ForCausalLM\")\nclass Qwen2Model(Model):\n    model_arch = gguf.MODEL_ARCH.QWEN2\n\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if self.hparams.get(\"rope_scaling\") is not None and \"factor\" in self.hparams[\"rope_scaling\"]:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"yarn\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.YARN)\n                self.gguf_writer.add_rope_scaling_factor(self.hparams[\"rope_scaling\"][\"factor\"])\n                self.gguf_writer.add_rope_scaling_orig_ctx_len(self.hparams[\"rope_scaling\"][\"original_max_position_embeddings\"])\n\n\n@Model.register(\"Qwen2VLForConditionalGeneration\")\nclass Qwen2VLModel(Model):\n    model_arch = gguf.MODEL_ARCH.QWEN2VL\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        mrope_section = self.hparams[\"rope_scaling\"][\"mrope_section\"]\n        mrope_section += [0] * max(0, 4 - len(mrope_section))\n        self.gguf_writer.add_rope_dimension_sections(mrope_section)\n\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n\n    def get_tensors(self) -> Iterator[tuple[str, Tensor]]:\n        for name, data in super().get_tensors():\n            if name.startswith(\"visual.\"):\n                continue\n            yield name, data\n\n\n@Model.register(\"WavTokenizerDec\")\nclass WavTokenizerDecModel(Model):\n    model_arch = gguf.MODEL_ARCH.WAVTOKENIZER_DEC\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        if \\\n                name.endswith(\"codebook.cluster_size\") or \\\n                name.endswith(\"codebook.embed_avg\") or \\\n                name.endswith(\"codebook.inited\"):\n            logger.debug(f\"Skipping {name!r}\")\n            return []\n\n        logger.info(f\"{self.map_tensor_name(name)} -> {data_torch.shape}\")\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def set_vocab(self):\n        self._set_vocab_none()\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_vocab_size         (self.hparams[\"vocab_size\"])\n        self.gguf_writer.add_features_length    (self.hparams[\"n_embd_features\"])\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"n_ff\"])\n        self.gguf_writer.add_group_norm_eps     (self.hparams[\"group_norm_epsilon\"])\n        self.gguf_writer.add_group_norm_groups  (self.hparams[\"group_norm_groups\"])\n\n        self.gguf_writer.add_posnet_embedding_length(self.hparams[\"posnet\"][\"n_embd\"])\n        self.gguf_writer.add_posnet_block_count     (self.hparams[\"posnet\"][\"n_layer\"])\n\n        self.gguf_writer.add_convnext_embedding_length(self.hparams[\"convnext\"][\"n_embd\"])\n        self.gguf_writer.add_convnext_block_count     (self.hparams[\"convnext\"][\"n_layer\"])\n\n        self.gguf_writer.add_causal_attention(False)\n\n\n@Model.register(\"Qwen2MoeForCausalLM\")\nclass Qwen2MoeModel(Model):\n    model_arch = gguf.MODEL_ARCH.QWEN2MOE\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (moe_intermediate_size := self.hparams.get(\"moe_intermediate_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)\n            logger.info(f\"gguf: expert feed forward length = {moe_intermediate_size}\")\n        if (shared_expert_intermediate_size := self.hparams.get('shared_expert_intermediate_size')) is not None:\n            self.gguf_writer.add_expert_shared_feed_forward_length(shared_expert_intermediate_size)\n            logger.info(f\"gguf: expert shared feed forward length = {shared_expert_intermediate_size}\")\n\n    _experts: list[dict[str, Tensor]] | None = None\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # process the experts separately\n        if name.find(\"experts\") != -1:\n            n_experts = self.hparams[\"num_experts\"]\n            assert bid is not None\n\n            if self._experts is None:\n                self._experts = [{} for _ in range(self.block_count)]\n\n            self._experts[bid][name] = data_torch\n\n            if len(self._experts[bid]) >= n_experts * 3:\n                tensors: list[tuple[str, Tensor]] = []\n\n                # merge the experts into a single 3d tensor\n                for w_name in [\"down_proj\", \"gate_proj\", \"up_proj\"]:\n                    datas: list[Tensor] = []\n\n                    for xid in range(n_experts):\n                        ename = f\"model.layers.{bid}.mlp.experts.{xid}.{w_name}.weight\"\n                        datas.append(self._experts[bid][ename])\n                        del self._experts[bid][ename]\n\n                    data_torch = torch.stack(datas, dim=0)\n\n                    merged_name = f\"model.layers.{bid}.mlp.experts.{w_name}.weight\"\n\n                    new_name = self.map_tensor_name(merged_name)\n\n                    tensors.append((new_name, data_torch))\n                return tensors\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n        if self._experts is not None:\n            # flatten `list[dict[str, Tensor]]` into `list[str]`\n            experts = [k for d in self._experts for k in d.keys()]\n            if len(experts) > 0:\n                raise ValueError(f\"Unprocessed experts: {experts}\")\n\n\n@Model.register(\"GPT2LMHeadModel\")\nclass GPT2Model(Model):\n    model_arch = gguf.MODEL_ARCH.GPT2\n\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_block_count(self.hparams[\"n_layer\"])\n        self.gguf_writer.add_context_length(self.hparams[\"n_ctx\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        tensors: list[tuple[str, Tensor]] = []\n\n        # we don't need these\n        if name.endswith((\".attn.bias\", \".attn.masked_bias\")):\n            return tensors\n\n        if name.endswith((\".c_attn.weight\", \".c_proj.weight\", \".c_fc.weight\", \".c_proj.weight\")):\n            data_torch = data_torch.transpose(1, 0)\n\n        new_name = self.map_tensor_name(name)\n\n        tensors.append((new_name, data_torch))\n\n        # note: GPT2 output is tied to (same as) wte in original model\n        if new_name == self.format_tensor_name(gguf.MODEL_TENSOR.TOKEN_EMBD):\n            tensors.append((self.format_tensor_name(gguf.MODEL_TENSOR.OUTPUT), data_torch))\n\n        return tensors\n\n\n@Model.register(\"PhiForCausalLM\")\nclass Phi2Model(Model):\n    model_arch = gguf.MODEL_ARCH.PHI2\n\n    def set_gguf_parameters(self):\n        block_count = self.find_hparam([\"num_hidden_layers\", \"n_layer\"])\n\n        rot_pct = self.find_hparam([\"partial_rotary_factor\"])\n        n_embd = self.find_hparam([\"hidden_size\", \"n_embd\"])\n        n_head = self.find_hparam([\"num_attention_heads\", \"n_head\"])\n\n        self.gguf_writer.add_context_length(self.find_hparam([\"n_positions\", \"max_position_embeddings\"]))\n\n        self.gguf_writer.add_embedding_length(n_embd)\n        self.gguf_writer.add_feed_forward_length(4 * n_embd)\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_head_count(n_head)\n        self.gguf_writer.add_head_count_kv(n_head)\n        self.gguf_writer.add_layer_norm_eps(self.find_hparam([\"layer_norm_epsilon\", \"layer_norm_eps\"]))\n        self.gguf_writer.add_rope_dimension_count(int(rot_pct * n_embd) // n_head)\n        self.gguf_writer.add_file_type(self.ftype)\n        self.gguf_writer.add_add_bos_token(False)\n\n\n@Model.register(\"Phi3ForCausalLM\")\nclass Phi3MiniModel(Model):\n    model_arch = gguf.MODEL_ARCH.PHI3\n\n    def set_vocab(self):\n        # Phi-4 model uses GPT2Tokenizer\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                tokenizer_class = tokenizer_config_json['tokenizer_class']\n                if tokenizer_class == 'GPT2Tokenizer':\n                    return self._set_vocab_gpt2()\n\n        from sentencepiece import SentencePieceProcessor\n\n        tokenizer_path = self.dir_model / 'tokenizer.model'\n\n        if not tokenizer_path.is_file():\n            raise ValueError(f'Error: Missing {tokenizer_path}')\n\n        tokenizer = SentencePieceProcessor()\n        tokenizer.LoadFromFile(str(tokenizer_path))\n\n        vocab_size = self.hparams.get('vocab_size', tokenizer.vocab_size())\n\n        tokens: list[bytes] = [f\"[PAD{i}]\".encode(\"utf-8\") for i in range(vocab_size)]\n        scores: list[float] = [-10000.0] * vocab_size\n        toktypes: list[int] = [SentencePieceTokenTypes.UNUSED] * vocab_size\n\n        for token_id in range(tokenizer.vocab_size()):\n\n            piece = tokenizer.IdToPiece(token_id)\n            text = piece.encode(\"utf-8\")\n            score = tokenizer.GetScore(token_id)\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.IsUnknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.IsControl(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.IsUnused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.IsByte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n\n            tokens[token_id] = text\n            scores[token_id] = score\n            toktypes[token_id] = toktype\n\n        added_tokens_file = self.dir_model / 'added_tokens.json'\n        if added_tokens_file.is_file():\n            with open(added_tokens_file, \"r\", encoding=\"utf-8\") as f:\n                added_tokens_json = json.load(f)\n\n                for key in added_tokens_json:\n                    token_id = added_tokens_json[key]\n                    if token_id >= vocab_size:\n                        logger.debug(f'ignore token {token_id}: id is out of range, max={vocab_size - 1}')\n                        continue\n\n                    tokens[token_id] = key.encode(\"utf-8\")\n                    scores[token_id] = -1000.0\n                    toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                added_tokens_decoder = tokenizer_config_json.get(\"added_tokens_decoder\", {})\n                for token_id, foken_data in added_tokens_decoder.items():\n                    token_id = int(token_id)\n                    token = foken_data[\"content\"].encode(\"utf-8\")\n                    if toktypes[token_id] != SentencePieceTokenTypes.UNUSED:\n                        if tokens[token_id] != token:\n                            logger.warning(f'replacing token {token_id}: {tokens[token_id].decode(\"utf-8\")!r} -> {token.decode(\"utf-8\")!r}')\n                    tokens[token_id] = token\n                    scores[token_id] = -1000.0\n                    toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n                    if foken_data.get(\"special\"):\n                        toktypes[token_id] = SentencePieceTokenTypes.CONTROL\n\n        tokenizer_file = self.dir_model / 'tokenizer.json'\n        if tokenizer_file.is_file():\n            with open(tokenizer_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_json = json.load(f)\n                added_tokens = tokenizer_json.get(\"added_tokens\", [])\n                for foken_data in added_tokens:\n                    token_id = int(foken_data[\"id\"])\n                    token = foken_data[\"content\"].encode(\"utf-8\")\n                    if toktypes[token_id] != SentencePieceTokenTypes.UNUSED:\n                        if tokens[token_id] != token:\n                            logger.warning(f'replacing token {token_id}: {tokens[token_id].decode(\"utf-8\")!r} -> {token.decode(\"utf-8\")!r}')\n                    tokens[token_id] = token\n                    scores[token_id] = -1000.0\n                    toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n                    if foken_data.get(\"special\"):\n                        toktypes[token_id] = SentencePieceTokenTypes.CONTROL\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def set_gguf_parameters(self):\n        block_count = self.find_hparam([\"num_hidden_layers\", \"n_layer\"])\n\n        n_embd = self.find_hparam([\"hidden_size\", \"n_embd\"])\n        n_head = self.find_hparam([\"num_attention_heads\", \"n_head\"])\n        n_head_kv = self.find_hparam([\"num_key_value_heads\", \"n_head_kv\"])\n        rms_eps = self.find_hparam([\"rms_norm_eps\"])\n        max_pos_embds = self.find_hparam([\"n_positions\", \"max_position_embeddings\"])\n        orig_max_pos_embds = self.find_hparam([\"original_max_position_embeddings\"])\n        rope_dims = n_embd // n_head\n\n        self.gguf_writer.add_context_length(max_pos_embds)\n        self.gguf_writer.add_rope_scaling_orig_ctx_len(orig_max_pos_embds)\n        self.gguf_writer.add_embedding_length(n_embd)\n        self.gguf_writer.add_feed_forward_length(self.find_hparam([\"intermediate_size\"]))\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_head_count(n_head)\n        self.gguf_writer.add_head_count_kv(n_head_kv)\n        self.gguf_writer.add_layer_norm_rms_eps(rms_eps)\n        self.gguf_writer.add_rope_dimension_count(rope_dims)\n        self.gguf_writer.add_rope_freq_base(self.find_hparam([\"rope_theta\"]))\n        self.gguf_writer.add_file_type(self.ftype)\n        sliding_window = self.hparams.get(\"sliding_window\")\n        # use zero value of sliding_window to distinguish Phi-4 from other PHI3 models\n        if sliding_window is None:\n            sliding_window = 0\n        self.gguf_writer.add_sliding_window(sliding_window)\n\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        n_embd = self.find_hparam([\"hidden_size\", \"n_embd\"])\n        n_head = self.find_hparam([\"num_attention_heads\", \"n_head\"])\n        max_pos_embds = self.find_hparam([\"n_positions\", \"max_position_embeddings\"])\n        orig_max_pos_embds = self.find_hparam([\"original_max_position_embeddings\"])\n        rope_dims = n_embd // n_head\n\n        # write rope scaling for long context (128k) model\n        rope_scaling = self.find_hparam(['rope_scaling'], True)\n        if rope_scaling is None:\n            return\n\n        scale = max_pos_embds / orig_max_pos_embds\n\n        rope_scaling_type = rope_scaling.get('type', '').lower()\n        if len(rope_scaling_type) == 0:\n            raise KeyError('Missing the required key rope_scaling.type')\n\n        if rope_scaling_type == 'su' or rope_scaling_type == 'longrope':\n            attn_factor = math.sqrt(1 + math.log(scale) / math.log(orig_max_pos_embds)) if scale > 1.0 else 1.0\n        elif rope_scaling_type == 'yarn':\n            attn_factor = 0.1 * math.log(scale) + 1.0 if scale > 1.0 else 1.0\n        else:\n            raise NotImplementedError(f'The rope scaling type {rope_scaling_type} is not supported yet')\n\n        self.gguf_writer.add_rope_scaling_attn_factors(attn_factor)\n\n        long_factors = rope_scaling.get('long_factor', None)\n        short_factors = rope_scaling.get('short_factor', None)\n\n        if long_factors is None or short_factors is None:\n            raise KeyError('Missing the required key rope_scaling.long_factor or rope_scaling_short_factor')\n\n        if len(long_factors) != len(short_factors) or len(long_factors) != rope_dims / 2:\n            raise ValueError(f'The length of rope long and short factors must be {rope_dims / 2}')\n\n        yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FACTORS_LONG), torch.tensor(long_factors, dtype=torch.float32))\n        yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FACTORS_SHORT), torch.tensor(short_factors, dtype=torch.float32))\n\n\n@Model.register(\"PhiMoEForCausalLM\")\nclass PhiMoeModel(Phi3MiniModel):\n    model_arch = gguf.MODEL_ARCH.PHIMOE\n\n    _experts: list[dict[str, Tensor]] | None = None\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_used_count(self.hparams[\"num_experts_per_tok\"])\n        self.gguf_writer.add_expert_count(self.hparams[\"num_local_experts\"])\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # process the experts separately\n        if name.find(\"block_sparse_moe.experts\") != -1:\n            n_experts = self.hparams[\"num_local_experts\"]\n            assert bid is not None\n\n            if self._experts is None:\n                self._experts = [{} for _ in range(self.block_count)]\n\n            self._experts[bid][name] = data_torch\n\n            if len(self._experts[bid]) >= n_experts * 3:\n                tensors: list[tuple[str, Tensor]] = []\n\n                # merge the experts into a single 3d tensor\n                for w_name in [\"w1\", \"w2\", \"w3\"]:\n                    datas: list[Tensor] = []\n\n                    for xid in range(n_experts):\n                        ename = f\"model.layers.{bid}.block_sparse_moe.experts.{xid}.{w_name}.weight\"\n                        datas.append(self._experts[bid][ename])\n                        del self._experts[bid][ename]\n\n                    data_torch = torch.stack(datas, dim=0)\n\n                    merged_name = f\"model.layers.{bid}.block_sparse_moe.experts.{w_name}.weight\"\n\n                    new_name = self.map_tensor_name(merged_name)\n\n                    tensors.append((new_name, data_torch))\n                return tensors\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n        if self._experts is not None:\n            # flatten `list[dict[str, Tensor]]` into `list[str]`\n            experts = [k for d in self._experts for k in d.keys()]\n            if len(experts) > 0:\n                raise ValueError(f\"Unprocessed experts: {experts}\")\n\n\n@Model.register(\"PlamoForCausalLM\")\nclass PlamoModel(Model):\n    model_arch = gguf.MODEL_ARCH.PLAMO\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        block_count = hparams[\"num_hidden_layers\"]\n\n        self.gguf_writer.add_context_length(4096)  # not in config.json\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_head_count(hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count_kv(5)  # hparams[\"num_key_value_heads\"]) is wrong\n        self.gguf_writer.add_layer_norm_rms_eps(hparams[\"rms_norm_eps\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def shuffle_attn_q_weight(self, data_torch):\n        assert data_torch.size() == (5120, 5120)\n        data_torch = data_torch.reshape(8, 5, 128, 5120)\n        data_torch = torch.permute(data_torch, (1, 0, 2, 3))\n        data_torch = torch.reshape(data_torch, (5120, 5120))\n        return data_torch\n\n    def shuffle_attn_output_weight(self, data_torch):\n        assert data_torch.size() == (5120, 5120)\n        data_torch = data_torch.reshape(5120, 8, 5, 128)\n        data_torch = torch.permute(data_torch, (0, 2, 1, 3))\n        data_torch = torch.reshape(data_torch, (5120, 5120))\n        return data_torch\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        new_name = self.map_tensor_name(name)\n\n        # shuffle for broadcasting of gqa in ggml_mul_mat\n        if new_name.endswith(\"attn_q.weight\"):\n            data_torch = self.shuffle_attn_q_weight(data_torch)\n        elif new_name.endswith(\"attn_output.weight\"):\n            data_torch = self.shuffle_attn_output_weight(data_torch)\n\n        return [(new_name, data_torch)]\n\n\n@Model.register(\"CodeShellForCausalLM\")\nclass CodeShellModel(Model):\n    model_arch = gguf.MODEL_ARCH.CODESHELL\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"n_layer\"]\n\n        self.gguf_writer.add_context_length(self.hparams[\"n_positions\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_head_count_kv(self.hparams[\"num_query_groups\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n        self.gguf_writer.add_rope_freq_base(10000.0)\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n        self.gguf_writer.add_rope_scaling_factor(1.0)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        new_name = self.map_tensor_name(name)\n\n        tensors: list[tuple[str, Tensor]] = [(new_name, data_torch)]\n\n        if new_name == self.format_tensor_name(gguf.MODEL_TENSOR.TOKEN_EMBD):\n            assert self.tensor_names is not None\n\n            if all(s not in self.tensor_names for s in (\"lm_head.weight\", \"output.weight\")):\n                # copy tok_embd.weight to output.weight\n                tensors.append((self.format_tensor_name(gguf.MODEL_TENSOR.OUTPUT), data_torch))\n\n        return tensors\n\n\n@Model.register(\"InternLM2ForCausalLM\")\nclass InternLM2Model(Model):\n    model_arch = gguf.MODEL_ARCH.INTERNLM2\n\n    def set_vocab(self):\n        # (TODO): Is there a better way?\n        # Copy from _set_vocab_sentencepiece, The only difference is that we will treat the character\n        # \\x00 specially and convert it into an emoji character to prevent it from being mistakenly\n        # recognized as an empty string in C++.\n        from sentencepiece import SentencePieceProcessor\n        from sentencepiece import sentencepiece_model_pb2 as model\n\n        tokenizer_path = self.dir_model / 'tokenizer.model'\n\n        tokens: list[bytes] = []\n        scores: list[float] = []\n        toktypes: list[int] = []\n\n        if not tokenizer_path.is_file():\n            logger.error(f'Error: Missing {tokenizer_path}')\n            sys.exit(1)\n\n        sentencepiece_model = model.ModelProto()  # pyright: ignore[reportAttributeAccessIssue]\n        sentencepiece_model.ParseFromString(open(tokenizer_path, \"rb\").read())\n        add_prefix = sentencepiece_model.normalizer_spec.add_dummy_prefix\n\n        tokenizer = SentencePieceProcessor()\n        tokenizer.LoadFromFile(str(tokenizer_path))\n\n        vocab_size = self.hparams.get('vocab_size', tokenizer.vocab_size())\n\n        for token_id in range(vocab_size):\n            piece = tokenizer.IdToPiece(token_id)\n            text = piece.encode(\"utf-8\")\n            score = tokenizer.GetScore(token_id)\n            if text == b\"\\x00\":\n                # (TODO): fixme\n                # Hack here and replace the \\x00 characters.\n                logger.warning(f\"InternLM2 convert token '{text}' to '\ud83d\udc09'!\")\n                text = \"\ud83d\udc09\".encode(\"utf-8\")\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.IsUnknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.IsControl(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.IsUnused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.IsByte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n            # take care of ununsed raw token\n            if piece.startswith('[UNUSED'):\n                toktype = SentencePieceTokenTypes.UNUSED\n\n            tokens.append(text)\n            scores.append(score)\n            toktypes.append(toktype)\n\n        added_tokens_file = self.dir_model / 'added_tokens.json'\n        if added_tokens_file.is_file():\n            with open(added_tokens_file, \"r\", encoding=\"utf-8\") as f:\n                added_tokens_json = json.load(f)\n\n                for key in added_tokens_json:\n                    tokens.append(key.encode(\"utf-8\"))\n                    scores.append(-1000.0)\n                    toktypes.append(SentencePieceTokenTypes.USER_DEFINED)\n\n        chat_eos_token = '<|im_end|>'\n        chat_eos_token_id = None\n\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                added_tokens_decoder = tokenizer_config_json.get(\"added_tokens_decoder\", {})\n                for token_id, foken_data in added_tokens_decoder.items():\n                    token_id = int(token_id)\n                    token = foken_data[\"content\"]\n                    if token == chat_eos_token:\n                        chat_eos_token_id = token_id\n                    token = token.encode(\"utf-8\")\n                    if toktypes[token_id] != SentencePieceTokenTypes.UNUSED:\n                        if tokens[token_id] != token:\n                            logger.warning(f'replacing token {token_id}: {tokens[token_id].decode(\"utf-8\")!r} -> {token.decode(\"utf-8\")!r}')\n                    tokens[token_id] = token\n                    scores[token_id] = -1000.0\n                    toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n                    if foken_data.get(\"special\"):\n                        toktypes[token_id] = SentencePieceTokenTypes.CONTROL\n\n        tokenizer_file = self.dir_model / 'tokenizer.json'\n        if tokenizer_file.is_file():\n            with open(tokenizer_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_json = json.load(f)\n                added_tokens = tokenizer_json.get(\"added_tokens\", [])\n                for foken_data in added_tokens:\n                    token_id = int(foken_data[\"id\"])\n                    token = foken_data[\"content\"]\n                    if token == chat_eos_token:\n                        chat_eos_token_id = token_id\n                    token = token.encode(\"utf-8\")\n                    if toktypes[token_id] != SentencePieceTokenTypes.UNUSED:\n                        if tokens[token_id] != token:\n                            logger.warning(f'replacing token {token_id}: {tokens[token_id].decode(\"utf-8\")!r} -> {token.decode(\"utf-8\")!r}')\n                    tokens[token_id] = token\n                    scores[token_id] = -1000.0\n                    toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n                    if foken_data.get(\"special\"):\n                        toktypes[token_id] = SentencePieceTokenTypes.CONTROL\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n        self.gguf_writer.add_add_space_prefix(add_prefix)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        old_eos = special_vocab.special_token_ids[\"eos\"]\n        if chat_eos_token_id is not None:\n            # For the chat model, we replace the eos with '<|im_end|>'.\n            # TODO: this is a hack, should be fixed\n            #       https://github.com/ggerganov/llama.cpp/pull/6745#issuecomment-2067687048\n            special_vocab.special_token_ids[\"eos\"] = chat_eos_token_id\n            logger.warning(f\"Replace eos:{old_eos} with a special token:{chat_eos_token_id}\"\n                           \" in chat mode so that the conversation can end normally.\")\n\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_context_length(self.hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_block_count(self.hparams[\"num_hidden_layers\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_rope_freq_base(self.hparams[\"rope_theta\"])\n        self.gguf_writer.add_head_count(self.hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"rms_norm_eps\"])\n        self.gguf_writer.add_head_count_kv(self.hparams[\"num_key_value_heads\"])\n        self.gguf_writer.add_file_type(self.ftype)\n        if self.hparams.get(\"rope_scaling\") is not None and \"factor\" in self.hparams[\"rope_scaling\"]:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"linear\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n                self.gguf_writer.add_rope_scaling_factor(self.hparams[\"rope_scaling\"][\"factor\"])\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        num_heads = self.hparams[\"num_attention_heads\"]\n        num_kv_heads = self.hparams[\"num_key_value_heads\"]\n        n_embd = self.hparams[\"hidden_size\"]\n        q_per_kv = num_heads // num_kv_heads\n        head_dim = n_embd // num_heads\n        num_groups = num_heads // q_per_kv\n\n        if bid is not None and f\"model.layers.{bid}.attention.wqkv\" in name:\n            qkv = data_torch\n\n            qkv = qkv.reshape((num_groups, q_per_kv + 2, head_dim, n_embd))\n            q, k, v = qkv[:, : q_per_kv], qkv[:, -2], qkv[:, -1]\n\n            # The model weights of q and k equire additional reshape.\n            q = LlamaModel.permute(q.reshape((-1, q.shape[-1])), num_heads, num_heads)\n            k = LlamaModel.permute(k.reshape((-1, k.shape[-1])), num_heads, num_kv_heads)\n            v = v.reshape((-1, v.shape[-1]))\n\n            return [\n                (self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_Q, bid), q),\n                (self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_K, bid), k),\n                (self.format_tensor_name(gguf.MODEL_TENSOR.ATTN_V, bid), v),\n            ]\n        else:\n            return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"InternLM3ForCausalLM\")\nclass InternLM3Model(Model):\n    model_arch = gguf.MODEL_ARCH.LLAMA\n\n    def set_vocab(self):\n        tokens, scores, toktypes = self._create_vocab_sentencepiece()\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                if \"add_prefix_space\" in tokenizer_config_json:\n                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n\n                if \"added_tokens_decoder\" in tokenizer_config_json:\n                    for token_id, token_data in tokenizer_config_json[\"added_tokens_decoder\"].items():\n                        if token_data.get(\"special\"):\n                            token_id = int(token_id)\n                            token = token_data[\"content\"]\n                            special_vocab._set_special_token(token, token_id)\n                            # update eos token\n                            if token == '<|im_end|>' and \"eos\" in special_vocab.special_token_ids:\n                                special_vocab.special_token_ids[\"eos\"] = token_id\n\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n\n        if \"head_dim\" in hparams:\n            rope_dim = hparams[\"head_dim\"]\n        else:\n            rope_dim = hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"]\n        self.gguf_writer.add_rope_dimension_count(rope_dim)\n\n        if self.hparams.get(\"rope_scaling\") is not None and \"factor\" in self.hparams[\"rope_scaling\"]:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"linear\" or self.hparams[\"rope_scaling\"].get(\"rope_type\") == \"linear\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n                self.gguf_writer.add_rope_scaling_factor(self.hparams[\"rope_scaling\"][\"factor\"])\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        n_head = self.hparams[\"num_attention_heads\"]\n        n_kv_head = self.hparams.get(\"num_key_value_heads\")\n        if name.endswith((\"q_proj.weight\", \"q_proj.bias\")):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_head)\n        if name.endswith((\"k_proj.weight\", \"k_proj.bias\")):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_kv_head)\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"BertModel\", \"BertForMaskedLM\", \"CamembertModel\")\nclass BertModel(Model):\n    model_arch = gguf.MODEL_ARCH.BERT\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.vocab_size = None\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_causal_attention(False)\n\n        # get pooling path\n        pooling_path = None\n        module_path = self.dir_model / \"modules.json\"\n        if module_path.is_file():\n            with open(module_path, encoding=\"utf-8\") as f:\n                modules = json.load(f)\n            for mod in modules:\n                if mod[\"type\"] == \"sentence_transformers.models.Pooling\":\n                    pooling_path = mod[\"path\"]\n                    break\n\n        # get pooling type\n        if pooling_path is not None:\n            with open(self.dir_model / pooling_path / \"config.json\", encoding=\"utf-8\") as f:\n                pooling = json.load(f)\n            if pooling[\"pooling_mode_mean_tokens\"]:\n                pooling_type = gguf.PoolingType.MEAN\n            elif pooling[\"pooling_mode_cls_token\"]:\n                pooling_type = gguf.PoolingType.CLS\n            else:\n                raise NotImplementedError(\"Only MEAN and CLS pooling types supported\")\n            self.gguf_writer.add_pooling_type(pooling_type)\n\n    def set_vocab(self):\n        tokens, toktypes, tokpre = self.get_vocab_base()\n        self.vocab_size = len(tokens)\n\n        # we need this to validate the size of the token_type embeddings\n        # though currently we are passing all zeros to the token_type embeddings\n        # \"Sequence A\" or \"Sequence B\"\n        self.gguf_writer.add_token_type_count(self.hparams.get(\"type_vocab_size\", 1))\n\n        # convert to phantom space vocab\n        def phantom(tok):\n            if tok.startswith(\"[\") and tok.endswith(\"]\"):\n                return tok\n            if tok.startswith(\"##\"):\n                return tok[2:]\n            return \"\\u2581\" + tok\n        tokens = list(map(phantom, tokens))\n\n        # add vocab to gguf\n        self.gguf_writer.add_tokenizer_model(\"bert\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n\n        # handle special tokens\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        if name.startswith(\"bert.\"):\n            name = name[5:]\n\n        if name.endswith(\".gamma\"):\n            name = name[:-6] + \".weight\"\n\n        if name.endswith(\".beta\"):\n            name = name[:-5] + \".bias\"\n\n        # we are only using BERT for embeddings so we don't need the pooling layer\n        if name in (\"embeddings.position_ids\", \"pooler.dense.weight\", \"pooler.dense.bias\"):\n            return [] # we don't need these\n\n        if name.startswith(\"cls.predictions\"):\n            return []\n\n        if name.startswith(\"cls.seq_relationship\"):\n            return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"RobertaModel\")\nclass RobertaModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # we need the pad_token_id to know how to chop down position_embd matrix\n        if (pad_token_id := self.hparams.get(\"pad_token_id\")) is not None:\n            self._position_offset = 1 + pad_token_id\n            if \"max_position_embeddings\" in self.hparams:\n                self.hparams[\"max_position_embeddings\"] -= self._position_offset\n        else:\n            self._position_offset = None\n\n    def set_vocab(self):\n        \"\"\"Support BPE tokenizers for roberta models\"\"\"\n        bpe_tok_path = self.dir_model / \"tokenizer.json\"\n        if bpe_tok_path.exists():\n            self._set_vocab_gpt2()\n            self.gguf_writer.add_add_bos_token(True)\n            self.gguf_writer.add_add_eos_token(True)\n\n            # we need this to validate the size of the token_type embeddings\n            # though currently we are passing all zeros to the token_type embeddings\n            # \"Sequence A\" or \"Sequence B\"\n            self.gguf_writer.add_token_type_count(self.hparams.get(\"type_vocab_size\", 1))\n\n        else:\n            return super().set_vocab()\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # if name starts with \"roberta.\", remove the prefix\n        # e.g. https://huggingface.co/BAAI/bge-reranker-v2-m3/tree/main\n        if name.startswith(\"roberta.\"):\n            name = name[8:]\n\n        # position embeddings start at pad_token_id + 1, so just chop down the weight tensor\n        if name == \"embeddings.position_embeddings.weight\":\n            if self._position_offset is not None:\n                data_torch = data_torch[self._position_offset:,:]\n\n        return super().modify_tensors(data_torch, name, bid)\n\n\n@Model.register(\"NomicBertModel\")\nclass NomicBertModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.NOMIC_BERT\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # the HF config claims n_ctx=8192, but it uses RoPE scaling\n        self.hparams[\"n_ctx\"] = 2048\n\n        # SwigLU activation\n        assert self.hparams[\"activation_function\"] == \"swiglu\"\n        # this doesn't do anything in the HF version\n        assert self.hparams[\"causal\"] is False\n        # no bias tensors\n        assert self.hparams[\"qkv_proj_bias\"] is False\n        assert self.hparams[\"mlp_fc1_bias\"] is False\n        assert self.hparams[\"mlp_fc2_bias\"] is False\n        # norm at end of layer\n        assert self.hparams[\"prenorm\"] is False\n        # standard RoPE\n        assert self.hparams[\"rotary_emb_fraction\"] == 1.0\n        assert self.hparams[\"rotary_emb_interleaved\"] is False\n        assert self.hparams[\"rotary_emb_scale_base\"] is None\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_rope_freq_base(self.hparams[\"rotary_emb_base\"])\n\n\n@Model.register(\"XLMRobertaModel\", \"XLMRobertaForSequenceClassification\")\nclass XLMRobertaModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # we need the pad_token_id to know how to chop down position_embd matrix\n        if (pad_token_id := self.hparams.get(\"pad_token_id\")) is not None:\n            self._position_offset = 1 + pad_token_id\n            if \"max_position_embeddings\" in self.hparams:\n                self.hparams[\"max_position_embeddings\"] -= self._position_offset\n        else:\n            self._position_offset = None\n\n    def set_vocab(self):\n        # to avoid TypeError: Descriptors cannot be created directly\n        # exception when importing sentencepiece_model_pb2\n        os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n        from sentencepiece import SentencePieceProcessor\n        from sentencepiece import sentencepiece_model_pb2 as model\n\n        tokenizer_path = self.dir_model / 'sentencepiece.bpe.model'\n        if not tokenizer_path.is_file():\n            raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\n\n        sentencepiece_model = model.ModelProto()  # pyright: ignore[reportAttributeAccessIssue]\n        sentencepiece_model.ParseFromString(open(tokenizer_path, \"rb\").read())\n        assert sentencepiece_model.trainer_spec.model_type == 1  # UNIGRAM\n\n        add_prefix = sentencepiece_model.normalizer_spec.add_dummy_prefix\n        remove_whitespaces = sentencepiece_model.normalizer_spec.remove_extra_whitespaces\n        precompiled_charsmap = sentencepiece_model.normalizer_spec.precompiled_charsmap\n\n        tokenizer = SentencePieceProcessor()\n        tokenizer.LoadFromFile(str(tokenizer_path))\n\n        vocab_size = self.hparams.get('vocab_size', tokenizer.vocab_size())\n\n        tokens: list[bytes] = [f\"[PAD{i}]\".encode(\"utf-8\") for i in range(vocab_size)]\n        scores: list[float] = [-10000.0] * vocab_size\n        toktypes: list[int] = [SentencePieceTokenTypes.UNUSED] * vocab_size\n\n        for token_id in range(tokenizer.vocab_size()):\n            piece = tokenizer.IdToPiece(token_id)\n            text = piece.encode(\"utf-8\")\n            score = tokenizer.GetScore(token_id)\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.IsUnknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.IsControl(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.IsUnused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.IsByte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n\n            tokens[token_id] = text\n            scores[token_id] = score\n            toktypes[token_id] = toktype\n\n        if vocab_size > len(tokens):\n            pad_count = vocab_size - len(tokens)\n            logger.debug(f\"Padding vocab with {pad_count} token(s) - [PAD1] through [PAD{pad_count}]\")\n            for i in range(1, pad_count + 1):\n                tokens.append(bytes(f\"[PAD{i}]\", encoding=\"utf-8\"))\n                scores.append(-1000.0)\n                toktypes.append(SentencePieceTokenTypes.UNUSED)\n\n        # realign tokens (see HF tokenizer code)\n        tokens = [b'<s>', b'<pad>', b'</s>', b'<unk>'] + tokens[3:-1]\n        scores = [0.0, 0.0, 0.0, 0.0] + scores[3:-1]\n        toktypes = [\n            SentencePieceTokenTypes.CONTROL,\n            SentencePieceTokenTypes.CONTROL,\n            SentencePieceTokenTypes.CONTROL,\n            SentencePieceTokenTypes.UNKNOWN,\n        ] + toktypes[3:-1]\n\n        self.gguf_writer.add_tokenizer_model(\"t5\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n        self.gguf_writer.add_add_space_prefix(add_prefix)\n        self.gguf_writer.add_token_type_count(self.hparams.get(\"type_vocab_size\", 1))\n        self.gguf_writer.add_remove_extra_whitespaces(remove_whitespaces)\n        if precompiled_charsmap:\n            self.gguf_writer.add_precompiled_charsmap(precompiled_charsmap)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n        self.gguf_writer.add_add_bos_token(True)\n        self.gguf_writer.add_add_eos_token(True)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # if name starts with \"roberta.\", remove the prefix\n        # e.g. https://huggingface.co/BAAI/bge-reranker-v2-m3/tree/main\n        if name.startswith(\"roberta.\"):\n            name = name[8:]\n\n        # position embeddings start at pad_token_id + 1, so just chop down the weight tensor\n        if name == \"embeddings.position_embeddings.weight\":\n            if self._position_offset is not None:\n                data_torch = data_torch[self._position_offset:,:]\n\n        return super().modify_tensors(data_torch, name, bid)\n\n\n@Model.register(\"GemmaForCausalLM\")\nclass GemmaModel(Model):\n    model_arch = gguf.MODEL_ARCH.GEMMA\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n        # TODO: these special tokens should be exported only for the CodeGemma family\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=False,\n                                          special_token_types = ['prefix', 'suffix', 'middle', 'fsep', 'eot'])\n        special_vocab._set_special_token(\"prefix\", 67)\n        special_vocab._set_special_token(\"suffix\", 69)\n        special_vocab._set_special_token(\"middle\", 68)\n        special_vocab._set_special_token(\"fsep\",   70)\n        special_vocab._set_special_token(\"eot\",    107)\n        special_vocab.chat_template = None  # do not add it twice\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n        self.gguf_writer.add_add_space_prefix(False)\n\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        block_count = hparams[\"num_hidden_layers\"]\n\n        self.gguf_writer.add_context_length(hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        self.gguf_writer.add_head_count(hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count_kv(self.hparams[\"num_key_value_heads\"] if \"num_key_value_heads\" in hparams else hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"rms_norm_eps\"])\n        self.gguf_writer.add_key_length(hparams[\"head_dim\"])\n        self.gguf_writer.add_value_length(hparams[\"head_dim\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        # lm_head is not used in llama.cpp, while autoawq will include this tensor in model\n        # To prevent errors, skip loading lm_head.weight.\n        if name == \"lm_head.weight\":\n            logger.debug(f\"Skipping get tensor {name!r} in safetensors so that convert can end normally.\")\n            return []\n\n        # ref: https://github.com/huggingface/transformers/blob/fc37f38915372c15992b540dfcbbe00a916d4fc6/src/transformers/models/gemma/modeling_gemma.py#L89\n        if name.endswith(\"norm.weight\"):\n            data_torch = data_torch + 1\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"Gemma2ForCausalLM\")\nclass Gemma2Model(Model):\n    model_arch = gguf.MODEL_ARCH.GEMMA2\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n        self.gguf_writer.add_add_space_prefix(False)\n\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        block_count = hparams[\"num_hidden_layers\"]\n\n        self.gguf_writer.add_context_length(hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        self.gguf_writer.add_head_count(hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count_kv(self.hparams[\"num_key_value_heads\"] if \"num_key_value_heads\" in hparams else hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"rms_norm_eps\"])\n        self.gguf_writer.add_key_length(hparams[\"head_dim\"])\n        self.gguf_writer.add_value_length(hparams[\"head_dim\"])\n        self.gguf_writer.add_file_type(self.ftype)\n        self.gguf_writer.add_attn_logit_softcapping(\n            self.hparams[\"attn_logit_softcapping\"]\n        )\n        self.gguf_writer.add_final_logit_softcapping(\n            self.hparams[\"final_logit_softcapping\"]\n        )\n        self.gguf_writer.add_sliding_window(self.hparams[\"sliding_window\"])\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        # lm_head is not used in llama.cpp, while autoawq will include this tensor in model\n        # To prevent errors, skip loading lm_head.weight.\n        if name == \"lm_head.weight\":\n            logger.debug(f\"Skipping get tensor {name!r} in safetensors so that convert can end normally.\")\n            return []\n\n        # ref: https://github.com/huggingface/transformers/blob/fc37f38915372c15992b540dfcbbe00a916d4fc6/src/transformers/models/gemma/modeling_gemma.py#L89\n        if name.endswith(\"norm.weight\"):\n            data_torch = data_torch + 1\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"Starcoder2ForCausalLM\")\nclass StarCoder2Model(Model):\n    model_arch = gguf.MODEL_ARCH.STARCODER2\n\n\n@Model.register(\"Rwkv6ForCausalLM\")\nclass Rwkv6Model(Model):\n    model_arch = gguf.MODEL_ARCH.RWKV6\n\n    def set_vocab(self):\n        assert (self.dir_model / \"rwkv_vocab_v20230424.txt\").is_file()\n        vocab_size = self.hparams.get(\"vocab_size\", 65536)\n\n        tokens: list[bytes] = ['<s>'.encode(\"utf-8\")]\n        toktypes: list[int] = [gguf.TokenType.CONTROL]\n\n        with open(self.dir_model / \"rwkv_vocab_v20230424.txt\", \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n            for line in lines:\n                parts = line.split(' ')\n                assert len(parts) >= 3\n                token, token_len = ast.literal_eval(' '.join(parts[1:-1])), int(parts[-1])\n                token = token.encode(\"utf-8\") if isinstance(token, str) else token\n                assert isinstance(token, bytes)\n                assert len(token) == token_len\n                token_text: str = repr(token)[2:-1]  # \"b'\\xff'\" -> \"\\xff\"\n                tokens.append(token_text.encode(\"utf-8\"))\n                toktypes.append(gguf.TokenType.NORMAL)\n        remainder = vocab_size - len(tokens)\n        assert remainder >= 0\n        for i in range(len(tokens), vocab_size):\n            tokens.append(f\"[PAD{i}]\".encode(\"utf-8\"))\n            toktypes.append(gguf.TokenType.UNUSED)\n\n        self.gguf_writer.add_tokenizer_model(\"rwkv\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=False)\n        special_vocab.chat_template = \"rwkv-world\"\n        # hack: Add '\\n\\n' as the EOT token to make it chat normally\n        special_vocab._set_special_token(\"eot\", 261)\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"num_hidden_layers\"]\n        head_size = self.hparams[\"head_size\"]\n        hidden_size = self.hparams[\"hidden_size\"]\n        layer_norm_eps = self.hparams[\"layer_norm_epsilon\"]\n        rescale_every_n_layers = self.hparams[\"rescale_every\"]\n        intermediate_size = self.hparams[\"intermediate_size\"] if self.hparams[\"intermediate_size\"] is not None else int((hidden_size * 3.5) // 32 * 32)\n        time_mix_extra_dim = 64 if hidden_size == 4096 else 32\n        time_decay_extra_dim = 128 if hidden_size == 4096 else 64\n\n        # RWKV isn't context limited\n        self.gguf_writer.add_context_length(1048576)\n        self.gguf_writer.add_embedding_length(hidden_size)\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_layer_norm_eps(layer_norm_eps)\n        self.gguf_writer.add_rescale_every_n_layers(rescale_every_n_layers)\n        self.gguf_writer.add_wkv_head_size(head_size)\n        self.gguf_writer.add_time_mix_extra_dim(time_mix_extra_dim)\n        self.gguf_writer.add_time_decay_extra_dim(time_decay_extra_dim)\n        self.gguf_writer.add_feed_forward_length(intermediate_size)\n        self.gguf_writer.add_file_type(self.ftype)\n\n        # required by llama.cpp, unused\n        self.gguf_writer.add_head_count(0)\n\n    lerp_weights: dict[int, dict[str, Tensor]] = {}\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        new_name = self.map_tensor_name(name)\n\n        if not (new_name.endswith(\".weight\") or new_name.endswith(\".bias\")):\n            new_name += \".weight\"\n\n        if new_name.endswith(\"time_mix_w1.weight\") or new_name.endswith(\"time_mix_decay_w1.weight\") or new_name.endswith(\"time_mix_decay_w2.weight\"):\n            data_torch = data_torch.transpose(0, 1)\n\n        if new_name.endswith(\"time_mix_w2.weight\"):\n            data_torch = data_torch.permute(0, 2, 1)\n\n        if new_name.endswith(\"time_mix_decay.weight\") or \"lerp\" in new_name:\n            data_torch = data_torch.squeeze()\n\n        try:\n            rescale_every_n_layers = self.hparams[\"rescale_every\"]\n            if rescale_every_n_layers > 0:\n                if new_name.endswith(\"time_mix_output.weight\") or new_name.endswith(\"channel_mix_value.weight\"):\n                    data_torch = data_torch.div_(2 ** int(bid // rescale_every_n_layers))\n        except KeyError:\n            pass\n\n        # concat time_mix_lerp weights to reduce some cpu overhead\n        # also reduces the number of tensors in the model\n        if bid is not None and \"time_mix_lerp\" in new_name and \"time_mix_lerp_x\" not in new_name:\n            try:\n                self.lerp_weights[bid][new_name] = data_torch\n            except KeyError:\n                self.lerp_weights[bid] = {new_name: data_torch}\n            if all(f\"blk.{bid}.time_mix_lerp_{i}.weight\" in self.lerp_weights[bid].keys() for i in [\"w\", \"k\", \"v\", \"r\", \"g\"]):\n                new_name = f\"blk.{bid}.time_mix_lerp_fused.weight\"\n                data = torch.stack([self.lerp_weights[bid][f\"blk.{bid}.time_mix_lerp_{i}.weight\"].unsqueeze(0) for i in [\"w\", \"k\", \"v\", \"r\", \"g\"]], dim=0).unsqueeze(1)\n                yield (new_name, data)\n            return\n\n        yield (new_name, data_torch)\n\n\n@Model.register(\"RWKV6Qwen2ForCausalLM\")\nclass RWKV6Qwen2Model(Rwkv6Model):\n    model_arch = gguf.MODEL_ARCH.RWKV6QWEN2\n\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n\n    def set_gguf_parameters(self):\n        block_count = self.hparams[\"num_hidden_layers\"]\n        num_attention_heads = self.hparams[\"num_attention_heads\"]\n        num_key_value_heads = self.hparams[\"num_key_value_heads\"]\n        hidden_size = self.hparams[\"hidden_size\"]\n        head_size = hidden_size // num_attention_heads\n        rms_norm_eps = self.hparams[\"rms_norm_eps\"]\n        intermediate_size = self.hparams[\"intermediate_size\"]\n        time_mix_extra_dim = 64 if hidden_size >= 4096 else 32\n        time_decay_extra_dim = 128 if hidden_size >= 4096 else 64\n\n        # RWKV isn't context limited\n        self.gguf_writer.add_context_length(1048576)\n        self.gguf_writer.add_embedding_length(hidden_size)\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_wkv_head_size(head_size)\n        self.gguf_writer.add_time_mix_extra_dim(time_mix_extra_dim)\n        self.gguf_writer.add_time_decay_extra_dim(time_decay_extra_dim)\n        self.gguf_writer.add_feed_forward_length(intermediate_size)\n        self.gguf_writer.add_file_type(self.ftype)\n\n        # special parameters for time_mixing in RWKV6QWEN2\n        self.gguf_writer.add_layer_norm_rms_eps(rms_norm_eps)\n        self.gguf_writer.add_token_shift_count(1)\n        # RWKV6QWEN2 use grouped key/value like GQA\n        self.gguf_writer.add_head_count_kv(num_key_value_heads)\n\n        # required by llama.cpp, unused\n        self.gguf_writer.add_head_count(0)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        for new_name, data in super().modify_tensors(data_torch, name, bid):\n            if \"time_mix_w1\" in new_name or \"time_mix_w2\" in new_name:\n                data = data.view(5, -1, data.shape[-1])\n                # rwkv6qwen2 has a different order of rkvwg instead of the original wkvrg\n                # permute them here to avoid code changes\n                data = torch.stack([data[3], data[1], data[2], data[0], data[4]], dim=0).view(-1, data.shape[-1])\n                if \"w2\" in new_name:\n                    data = data.view(5, -1, data.shape[-1])\n                yield (new_name, data)\n                continue\n            yield (new_name, data)\n\n\n@Model.register(\"MambaForCausalLM\", \"MambaLMHeadModel\", \"FalconMambaForCausalLM\")\nclass MambaModel(Model):\n    model_arch = gguf.MODEL_ARCH.MAMBA\n\n    def set_vocab(self):\n        vocab_size = self.hparams[\"vocab_size\"]\n        # Round vocab size to next multiple of 8\n        pad_vocab = self.hparams.get(\"pad_vocab_size_multiple\", 8)\n        # pad using ceiling division\n        # ref: https://stackoverflow.com/a/17511341/22827863\n        vocab_size = -(vocab_size // -pad_vocab) * pad_vocab\n        self.hparams[\"vocab_size\"] = vocab_size\n\n        if (self.dir_model / \"tokenizer.json\").is_file():\n            self._set_vocab_gpt2()\n        elif (self.dir_model / \"tokenizer.model\").is_file():\n            self._set_vocab_sentencepiece()\n        else:\n            # Use the GPT-NeoX tokenizer when no tokenizer files are present\n            self._set_vocab_builtin(\"gpt-neox\", vocab_size)\n\n    def set_gguf_parameters(self):\n        d_model = self.find_hparam([\"hidden_size\",       \"d_model\"])\n        d_conv  = self.find_hparam([\"conv_kernel\",       \"d_conv\"],  optional=True) or 4\n        d_inner = self.find_hparam([\"intermediate_size\", \"d_inner\"], optional=True) or 2 * d_model\n        d_state = self.find_hparam([\"state_size\",        \"d_state\"], optional=True) or 16\n        # ceiling division\n        # ref: https://stackoverflow.com/a/17511341/22827863\n        # ref: https://github.com/state-spaces/mamba/blob/ce59daea3a090d011d6476c6e5b97f6d58ddad8b/mamba_ssm/modules/mamba_simple.py#L58\n        dt_rank      = self.find_hparam([\"time_step_rank\",     \"dt_rank\"],      optional=True) or -(d_model // -16)\n        rms_norm_eps = self.find_hparam([\"layer_norm_epsilon\", \"rms_norm_eps\"], optional=True) or 1e-5\n        use_dt_b_c_norm = False\n        # For falconmamba we do apply RMS norm on B / DT and C layers\n        if self.find_hparam([\"model_type\"], optional=True) in (\"falcon_mamba\",):\n            use_dt_b_c_norm = True\n        # Fail early for models which don't have a block expansion factor of 2\n        assert d_inner == 2 * d_model\n\n        self.gguf_writer.add_context_length(2**20) # arbitrary value; for those who use the default\n        self.gguf_writer.add_embedding_length(d_model)\n        self.gguf_writer.add_feed_forward_length(0) # unused, but seemingly required when loading\n        self.gguf_writer.add_head_count(0) # unused, but seemingly required when loading\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_ssm_conv_kernel(d_conv)\n        self.gguf_writer.add_ssm_inner_size(d_inner)\n        self.gguf_writer.add_ssm_state_size(d_state)\n        self.gguf_writer.add_ssm_time_step_rank(dt_rank)\n        self.gguf_writer.add_layer_norm_rms_eps(rms_norm_eps)\n        self.gguf_writer.add_ssm_dt_b_c_rms(use_dt_b_c_norm) # For classic Mamba we don't apply rms norm on B / DT layers\n        self.gguf_writer.add_file_type(self.ftype)\n\n    _tok_embd = None\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        output_name = self.format_tensor_name(gguf.MODEL_TENSOR.OUTPUT)\n        tok_embd_name = self.format_tensor_name(gguf.MODEL_TENSOR.TOKEN_EMBD)\n\n        new_name = self.map_tensor_name(name)\n\n        if name.endswith(\".A_log\"):\n            logger.debug(\"A_log --> A ==> \" + new_name)\n            data_torch = -torch.exp(data_torch)\n\n        # assuming token_embd.weight is seen before output.weight\n        if self._tok_embd is not None and new_name == output_name:\n            if torch.equal(self._tok_embd, data_torch):\n                logger.debug(f\"{output_name} is equivalent to {tok_embd_name}, omitting\")\n                return []\n        elif new_name == tok_embd_name:\n            self._tok_embd = data_torch\n\n        return [(new_name, data_torch)]\n\n\n@Model.register(\"CohereForCausalLM\")\nclass CommandR2Model(Model):\n    model_arch = gguf.MODEL_ARCH.COMMAND_R\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # max_position_embeddings = 8192 in config.json but model was actually\n        # trained on 128k context length\n        # aya-23 models don't have model_max_length specified\n        self.hparams[\"max_position_embeddings\"] = self.find_hparam([\"model_max_length\", \"max_position_embeddings\"])\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_logit_scale(self.hparams[\"logit_scale\"])\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.NONE)\n\n\n@Model.register(\"Cohere2ForCausalLM\")\nclass Cohere2Model(Model):\n    model_arch = gguf.MODEL_ARCH.COHERE2\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n\n        self.gguf_writer.add_logit_scale(self.hparams[\"logit_scale\"])\n        self.gguf_writer.add_sliding_window(self.hparams[\"sliding_window\"])\n        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n\n        rotary_pct = self.hparams[\"rotary_pct\"]\n        hidden_size = self.hparams[\"hidden_size\"]\n        num_attention_heads = self.hparams[\"num_attention_heads\"]\n        self.gguf_writer.add_rope_dimension_count(int(rotary_pct * (hidden_size // num_attention_heads)))\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.NONE)\n\n\n@Model.register(\"OlmoForCausalLM\")\n@Model.register(\"OLMoForCausalLM\")\nclass OlmoModel(Model):\n    model_arch = gguf.MODEL_ARCH.OLMO\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_layer_norm_eps(1e-5)\n        clip_qkv = self.hparams.get(\"clip_qkv\")\n        if clip_qkv is not None:\n            self.gguf_writer.add_clamp_kqv(clip_qkv)\n\n    # Same as super class, but permuting q_proj, k_proj\n    # Copied from: LlamaModel\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        n_head = self.hparams[\"num_attention_heads\"]\n        n_kv_head = self.hparams.get(\"num_key_value_heads\")\n\n        if name.endswith(\"q_proj.weight\"):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_head)\n        if name.endswith(\"k_proj.weight\"):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_kv_head)\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"Olmo2ForCausalLM\")\nclass Olmo2Model(Model):\n    model_arch = gguf.MODEL_ARCH.OLMO2\n\n\n@Model.register(\"OlmoeForCausalLM\")\nclass OlmoeModel(Model):\n    model_arch = gguf.MODEL_ARCH.OLMOE\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_layer_norm_rms_eps(1e-5)\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n\n    _experts: list[dict[str, Tensor]] | None = None\n\n    # Copied from: Qwen2MoeModel\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # process the experts separately\n        if name.find(\"experts\") != -1:\n            n_experts = self.hparams[\"num_experts\"]\n            assert bid is not None\n\n            if self._experts is None:\n                self._experts = [{} for _ in range(self.block_count)]\n\n            self._experts[bid][name] = data_torch\n\n            if len(self._experts[bid]) >= n_experts * 3:\n                tensors: list[tuple[str, Tensor]] = []\n\n                # merge the experts into a single 3d tensor\n                for w_name in [\"down_proj\", \"gate_proj\", \"up_proj\"]:\n                    datas: list[Tensor] = []\n\n                    for xid in range(n_experts):\n                        ename = f\"model.layers.{bid}.mlp.experts.{xid}.{w_name}.weight\"\n                        datas.append(self._experts[bid][ename])\n                        del self._experts[bid][ename]\n\n                    data_torch = torch.stack(datas, dim=0)\n\n                    merged_name = f\"model.layers.{bid}.mlp.experts.{w_name}.weight\"\n\n                    new_name = self.map_tensor_name(merged_name)\n\n                    tensors.append((new_name, data_torch))\n                return tensors\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    # Copied from: Qwen2MoeModel\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n        if self._experts is not None:\n            # flatten `list[dict[str, Tensor]]` into `list[str]`\n            experts = [k for d in self._experts for k in d.keys()]\n            if len(experts) > 0:\n                raise ValueError(f\"Unprocessed experts: {experts}\")\n\n\n@Model.register(\"JinaBertModel\", \"JinaBertForMaskedLM\")\nclass JinaBertV2Model(BertModel):\n    model_arch = gguf.MODEL_ARCH.JINA_BERT_V2\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.intermediate_size = self.hparams[\"intermediate_size\"]\n\n    def get_tensors(self):\n        for name, data in super().get_tensors():\n            if 'gated_layer' in name:\n                d1 = data[:self.intermediate_size, :]\n                name1 = name.replace('gated_layers', 'gated_layers_w')\n                name1 = name1.replace('up_gated_layer', 'gated_layers_v')\n                d2 = data[self.intermediate_size:, :]\n                name2 = name.replace('gated_layers', 'gated_layers_v')\n                name2 = name2.replace('up_gated_layer', 'gated_layers_w')\n                yield name1, d1\n                yield name2, d2\n                continue\n\n            yield name, data\n\n    def set_vocab(self):\n        tokenizer_class = 'BertTokenizer'\n        with open(self.dir_model / \"tokenizer_config.json\", \"r\", encoding=\"utf-8\") as f:\n            tokenizer_class = json.load(f)['tokenizer_class']\n\n        if tokenizer_class == 'BertTokenizer':\n            super().set_vocab()\n        elif tokenizer_class == 'RobertaTokenizer':\n            self._set_vocab_gpt2()\n            self.gguf_writer.add_token_type_count(2)\n        else:\n            raise NotImplementedError(f'Tokenizer {tokenizer_class} is not supported for JinaBertModel')\n        self.gguf_writer.add_add_bos_token(True)\n        self.gguf_writer.add_add_eos_token(True)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # if name starts with \"bert.\", remove the prefix\n        # e.g. https://huggingface.co/jinaai/jina-reranker-v1-tiny-en\n        if name.startswith(\"bert.\"):\n            name = name[5:]\n\n        return super().modify_tensors(data_torch, name, bid)\n\n\n@Model.register(\"OpenELMForCausalLM\")\nclass OpenELMModel(Model):\n    model_arch = gguf.MODEL_ARCH.OPENELM\n\n    @staticmethod\n    def _make_divisible(v: float | int, divisor: int) -> int:\n        # ref: https://huggingface.co/apple/OpenELM-270M-Instruct/blob/eb111ff2e6724348e5b905984063d4064d4bc579/configuration_openelm.py#L34-L38\n        new_v = max(divisor, int(v + divisor / 2) // divisor * divisor)\n        # Make sure that round down does not go down by more than 10%.\n        if new_v < 0.9 * v:\n            new_v += divisor\n        return new_v\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        ffn_multipliers: list[float] = self.hparams[\"ffn_multipliers\"]\n        ffn_dim_divisor: int = self.hparams[\"ffn_dim_divisor\"]\n        self._n_embd: int = self.hparams[\"model_dim\"]\n        self._num_kv_heads: list[int] = self.hparams[\"num_kv_heads\"]\n        self._num_query_heads: list[int] = self.hparams[\"num_query_heads\"]\n        self._ffn_dims: list[int] = [\n            OpenELMModel._make_divisible(multiplier * self._n_embd, ffn_dim_divisor)\n            for multiplier in ffn_multipliers\n        ]\n        assert isinstance(self._num_kv_heads, list) and isinstance(self._num_kv_heads[0], int)\n        assert isinstance(self._num_query_heads, list) and isinstance(self._num_query_heads[0], int)\n\n    # Uses the tokenizer from meta-llama/Llama-2-7b-hf\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_builtin(\"llama-spm\", self.hparams[\"vocab_size\"])\n\n    def set_gguf_parameters(self):\n        n_embd = self._n_embd\n        head_dim = self.hparams[\"head_dim\"]\n        rot_pct = 1.0\n        assert self.block_count == len(self._num_kv_heads)\n        assert self.block_count == len(self._num_query_heads)\n        assert self.block_count == len(self._ffn_dims)\n\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_context_length(self.hparams[\"max_context_length\"])\n        self.gguf_writer.add_embedding_length(n_embd)\n        self.gguf_writer.add_feed_forward_length(self._ffn_dims)\n        self.gguf_writer.add_head_count(self._num_query_heads)\n        self.gguf_writer.add_head_count_kv(self._num_kv_heads)\n        self.gguf_writer.add_rope_freq_base(self.hparams[\"rope_freq_constant\"])\n        # https://huggingface.co/apple/OpenELM-270M-Instruct/blob/c401df2/modeling_openelm.py#L30\n        self.gguf_writer.add_layer_norm_rms_eps(1e-6)\n        self.gguf_writer.add_rope_dimension_count(int(rot_pct * head_dim))\n        self.gguf_writer.add_key_length(head_dim)\n        self.gguf_writer.add_value_length(head_dim)\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def find_hparam(self, keys: Iterable[str], optional: bool = False) -> Any:\n        if \"n_layers\" in keys:\n            return self.hparams[\"num_transformer_layers\"]\n\n        return super().find_hparam(keys, optional)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n\n        # split ff\n        if bid is not None and name == f\"transformer.layers.{bid}.ffn.proj_1.weight\":\n            ff_dim = self._ffn_dims[bid]\n            yield (self.format_tensor_name(gguf.MODEL_TENSOR.FFN_GATE, bid), data_torch[:ff_dim])\n            yield (self.format_tensor_name(gguf.MODEL_TENSOR.FFN_UP, bid), data_torch[ff_dim:])\n            return\n\n        yield (self.map_tensor_name(name), data_torch)\n\n\n@Model.register(\"ArcticForCausalLM\")\nclass ArcticModel(Model):\n    model_arch = gguf.MODEL_ARCH.ARCTIC\n\n    def set_vocab(self):\n        # The reason for using a custom implementation here is that the\n        # snowflake-arctic-instruct model redefined tokens 31998 and 31999 from\n        # tokenizer.model and used them as BOS and EOS instead of adding new tokens.\n        from sentencepiece import SentencePieceProcessor\n\n        tokenizer_path = self.dir_model / 'tokenizer.model'\n\n        if not tokenizer_path.is_file():\n            logger.error(f'Error: Missing {tokenizer_path}')\n            sys.exit(1)\n\n        # Read the whole vocabulary from the tokenizer.model file\n        tokenizer = SentencePieceProcessor()\n        tokenizer.LoadFromFile(str(tokenizer_path))\n\n        vocab_size = self.hparams.get('vocab_size', tokenizer.vocab_size())\n\n        tokens: list[bytes] = [f\"[PAD{i}]\".encode(\"utf-8\") for i in range(vocab_size)]\n        scores: list[float] = [-10000.0] * vocab_size\n        toktypes: list[int] = [SentencePieceTokenTypes.UNUSED] * vocab_size\n\n        for token_id in range(tokenizer.vocab_size()):\n\n            piece = tokenizer.IdToPiece(token_id)\n            text = piece.encode(\"utf-8\")\n            score = tokenizer.GetScore(token_id)\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.IsUnknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.IsControl(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.IsUnused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.IsByte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n\n            tokens[token_id] = text\n            scores[token_id] = score\n            toktypes[token_id] = toktype\n\n        # Use the added_tokens_decoder field from tokeniser_config.json as the source\n        # of information about added/redefined tokens and modify them accordingly.\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n\n                if \"added_tokens_decoder\" in tokenizer_config_json:\n                    added_tokens_decoder = tokenizer_config_json[\"added_tokens_decoder\"]\n                    for token_id, token_json in added_tokens_decoder.items():\n                        token_id = int(token_id)\n                        if token_id >= vocab_size:\n                            logger.debug(f'ignore token {token_id}: id is out of range, max={vocab_size - 1}')\n                            continue\n\n                        token_content = token_json[\"content\"]\n                        token_type = SentencePieceTokenTypes.USER_DEFINED\n                        token_score = -10000.0\n\n                        # Map unk_token to UNKNOWN, other special tokens to CONTROL\n                        # Set the score to 0.0 as in the original tokenizer.model\n                        if (\"special\" in token_json) and token_json[\"special\"]:\n                            if token_content == tokenizer_config_json[\"unk_token\"]:\n                                token_type = SentencePieceTokenTypes.UNKNOWN\n                            else:\n                                token_type = SentencePieceTokenTypes.CONTROL\n                            token_score = 0.0\n\n                        logger.info(f\"Setting added token {token_id} to '{token_content}' (type: {token_type}, score: {token_score:.2f})\")\n                        tokens[token_id] = token_content.encode(\"utf-8\")\n                        toktypes[token_id] = token_type\n                        scores[token_id] = token_score\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n        self.gguf_writer.add_rope_dimension_count(hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"])\n\n    _experts: list[dict[str, Tensor]] | None = None\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        n_head = self.hparams[\"num_attention_heads\"]\n        n_kv_head = self.hparams.get(\"num_key_value_heads\")\n\n        if name.endswith(\"q_proj.weight\"):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_head)\n        if name.endswith(\"k_proj.weight\"):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_kv_head)\n\n        # process the experts separately\n        if name.find(\"block_sparse_moe.experts\") != -1:\n            n_experts = self.hparams[\"num_local_experts\"]\n\n            assert bid is not None\n\n            if self._experts is None:\n                self._experts = [{} for _ in range(self.block_count)]\n\n            self._experts[bid][name] = data_torch\n\n            if len(self._experts[bid]) >= n_experts * 3:\n                tensors: list[tuple[str, Tensor]] = []\n\n                # merge the experts into a single 3d tensor\n                for wid in [\"w1\", \"w2\", \"w3\"]:\n                    datas: list[Tensor] = []\n\n                    for xid in range(n_experts):\n                        ename = f\"model.layers.{bid}.block_sparse_moe.experts.{xid}.{wid}.weight\"\n                        datas.append(self._experts[bid][ename])\n                        del self._experts[bid][ename]\n\n                    data_torch = torch.stack(datas, dim=0)\n\n                    merged_name = f\"layers.{bid}.feed_forward.experts.{wid}.weight\"\n\n                    new_name = self.map_tensor_name(merged_name)\n\n                    tensors.append((new_name, data_torch))\n                return tensors\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n        if self._experts is not None:\n            # flatten `list[dict[str, Tensor]]` into `list[str]`\n            experts = [k for d in self._experts for k in d.keys()]\n            if len(experts) > 0:\n                raise ValueError(f\"Unprocessed experts: {experts}\")\n\n\n@Model.register(\"DeepseekForCausalLM\")\nclass DeepseekModel(Model):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK\n\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        if \"head_dim\" in hparams:\n            rope_dim = hparams[\"head_dim\"]\n        else:\n            rope_dim = hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"]\n\n        self.gguf_writer.add_rope_dimension_count(rope_dim)\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.NONE)\n        self.gguf_writer.add_leading_dense_block_count(hparams[\"first_k_dense_replace\"])\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n        self.gguf_writer.add_expert_feed_forward_length(hparams[\"moe_intermediate_size\"])\n        self.gguf_writer.add_expert_weights_scale(1.0)\n        self.gguf_writer.add_expert_count(hparams[\"n_routed_experts\"])\n        self.gguf_writer.add_expert_shared_count(hparams[\"n_shared_experts\"])\n\n    _experts: list[dict[str, Tensor]] | None = None\n\n    @staticmethod\n    def permute(weights: Tensor, n_head: int, n_head_kv: int | None):\n        if n_head_kv is not None and n_head != n_head_kv:\n            n_head = n_head_kv\n        return (weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n                .swapaxes(1, 2)\n                .reshape(weights.shape))\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        n_head = self.hparams[\"num_attention_heads\"]\n        n_kv_head = self.hparams.get(\"num_key_value_heads\")\n\n        if name.endswith((\"q_proj.weight\", \"q_proj.bias\")):\n            data_torch = DeepseekModel.permute(data_torch, n_head, n_head)\n        if name.endswith((\"k_proj.weight\", \"k_proj.bias\")):\n            data_torch = DeepseekModel.permute(data_torch, n_head, n_kv_head)\n\n        # process the experts separately\n        if name.find(\"mlp.experts\") != -1:\n            n_experts = self.hparams[\"n_routed_experts\"]\n            assert bid is not None\n\n            if self._experts is None:\n                self._experts = [{} for _ in range(self.block_count)]\n\n            self._experts[bid][name] = data_torch\n\n            if len(self._experts[bid]) >= n_experts * 3:\n                tensors: list[tuple[str, Tensor]] = []\n\n                # merge the experts into a single 3d tensor\n                for w_name in [\"down_proj\", \"gate_proj\", \"up_proj\"]:\n                    datas: list[Tensor] = []\n\n                    for xid in range(n_experts):\n                        ename = f\"model.layers.{bid}.mlp.experts.{xid}.{w_name}.weight\"\n                        datas.append(self._experts[bid][ename])\n                        del self._experts[bid][ename]\n\n                    data_torch = torch.stack(datas, dim=0)\n\n                    merged_name = f\"model.layers.{bid}.mlp.experts.{w_name}.weight\"\n\n                    new_name = self.map_tensor_name(merged_name)\n\n                    tensors.append((new_name, data_torch))\n                return tensors\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n        if self._experts is not None:\n            # flatten `list[dict[str, Tensor]]` into `list[str]`\n            experts = [k for d in self._experts for k in d.keys()]\n            if len(experts) > 0:\n                raise ValueError(f\"Unprocessed experts: {experts}\")\n\n\n@Model.register(\"DeepseekV2ForCausalLM\")\n@Model.register(\"DeepseekV3ForCausalLM\")\nclass DeepseekV2Model(Model):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK2\n\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n\n        self.gguf_writer.add_leading_dense_block_count(hparams[\"first_k_dense_replace\"])\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n        if \"q_lora_rank\" in hparams and hparams[\"q_lora_rank\"] is not None:\n            self.gguf_writer.add_q_lora_rank(hparams[\"q_lora_rank\"])\n        self.gguf_writer.add_kv_lora_rank(hparams[\"kv_lora_rank\"])\n        self.gguf_writer.add_key_length(hparams[\"qk_nope_head_dim\"] + hparams[\"qk_rope_head_dim\"])\n        self.gguf_writer.add_value_length(hparams[\"v_head_dim\"])\n        self.gguf_writer.add_expert_feed_forward_length(hparams[\"moe_intermediate_size\"])\n        self.gguf_writer.add_expert_count(hparams[\"n_routed_experts\"])\n        self.gguf_writer.add_expert_shared_count(hparams[\"n_shared_experts\"])\n        self.gguf_writer.add_expert_weights_scale(hparams[\"routed_scaling_factor\"])\n        self.gguf_writer.add_expert_weights_norm(hparams[\"norm_topk_prob\"])\n\n        if hparams[\"scoring_func\"] == \"sigmoid\":\n            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SIGMOID)\n        elif hparams[\"scoring_func\"] == \"softmax\":\n            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SOFTMAX)\n        else:\n            raise ValueError(f\"Unsupported scoring_func value: {hparams['scoring_func']}\")\n\n        self.gguf_writer.add_rope_dimension_count(hparams[\"qk_rope_head_dim\"])\n\n        if self.hparams.get(\"rope_scaling\") is not None and \"factor\" in self.hparams[\"rope_scaling\"]:\n            if self.hparams[\"rope_scaling\"].get(\"type\") == \"yarn\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.YARN)\n                self.gguf_writer.add_rope_scaling_factor(self.hparams[\"rope_scaling\"][\"factor\"])\n                self.gguf_writer.add_rope_scaling_orig_ctx_len(self.hparams[\"rope_scaling\"][\"original_max_position_embeddings\"])\n                self.gguf_writer.add_rope_scaling_yarn_log_mul(0.1 * hparams[\"rope_scaling\"][\"mscale_all_dim\"])\n\n    _experts: list[dict[str, Tensor]] | None = None\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # rename e_score_correction_bias tensors\n        if name.endswith(\"e_score_correction_bias\"):\n            name = name.replace(\"e_score_correction_bias\", \"e_score_correction.bias\")\n\n        # skip Multi-Token Prediction (MTP) layers\n        block_count = self.hparams[\"num_hidden_layers\"]\n        match = re.match(r\"model.layers.(\\d+)\", name)\n        if match and int(match.group(1)) >= block_count:\n            return []\n\n        # process the experts separately\n        if name.find(\"mlp.experts\") != -1:\n            n_experts = self.hparams[\"n_routed_experts\"]\n            assert bid is not None\n\n            if self._experts is None:\n                self._experts = [{} for _ in range(self.block_count)]\n\n            self._experts[bid][name] = data_torch\n\n            if len(self._experts[bid]) >= n_experts * 3:\n                tensors: list[tuple[str, Tensor]] = []\n\n                # merge the experts into a single 3d tensor\n                for w_name in [\"down_proj\", \"gate_proj\", \"up_proj\"]:\n                    datas: list[Tensor] = []\n\n                    for xid in range(n_experts):\n                        ename = f\"model.layers.{bid}.mlp.experts.{xid}.{w_name}.weight\"\n                        datas.append(self._experts[bid][ename])\n                        del self._experts[bid][ename]\n\n                    data_torch = torch.stack(datas, dim=0)\n\n                    merged_name = f\"model.layers.{bid}.mlp.experts.{w_name}.weight\"\n\n                    new_name = self.map_tensor_name(merged_name)\n\n                    tensors.append((new_name, data_torch))\n                return tensors\n            else:\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n\n        if self._experts is not None:\n            # flatten `list[dict[str, Tensor]]` into `list[str]`\n            experts = [k for d in self._experts for k in d.keys()]\n            if len(experts) > 0:\n                raise ValueError(f\"Unprocessed experts: {experts}\")\n\n\n@Model.register(\"T5WithLMHeadModel\")\n@Model.register(\"T5ForConditionalGeneration\")\n@Model.register(\"MT5ForConditionalGeneration\")\n@Model.register(\"UMT5ForConditionalGeneration\")\nclass T5Model(Model):\n    model_arch = gguf.MODEL_ARCH.T5\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shared_token_embeddings_found = False\n\n    def set_vocab(self):\n        # to avoid TypeError: Descriptors cannot be created directly\n        # exception when importing sentencepiece_model_pb2\n        os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n        from sentencepiece import SentencePieceProcessor\n        from sentencepiece import sentencepiece_model_pb2 as model\n\n        tokenizer_path = self.dir_model / 'tokenizer.model'\n\n        # many older models use spiece.model tokenizer model filename\n        if not tokenizer_path.is_file():\n            tokenizer_path = self.dir_model / 'spiece.model'\n\n        if not tokenizer_path.is_file():\n            raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\n\n        sentencepiece_model = model.ModelProto()  # pyright: ignore[reportAttributeAccessIssue]\n        sentencepiece_model.ParseFromString(open(tokenizer_path, \"rb\").read())\n\n        # some models like Pile-T5 family use BPE tokenizer instead of Unigram\n        if sentencepiece_model.trainer_spec.model_type == 2:  # BPE\n            # assure the tokenizer model file name is correct\n            assert tokenizer_path.name == 'tokenizer.model'\n            return self._set_vocab_sentencepiece()\n        else:\n            assert sentencepiece_model.trainer_spec.model_type == 1  # UNIGRAM\n\n        add_prefix = sentencepiece_model.normalizer_spec.add_dummy_prefix\n        remove_whitespaces = sentencepiece_model.normalizer_spec.remove_extra_whitespaces\n        precompiled_charsmap = sentencepiece_model.normalizer_spec.precompiled_charsmap\n\n        tokenizer = SentencePieceProcessor()\n        tokenizer.LoadFromFile(str(tokenizer_path))\n\n        vocab_size = self.hparams.get('vocab_size', tokenizer.vocab_size())\n\n        tokens: list[bytes] = [f\"[PAD{i}]\".encode(\"utf-8\") for i in range(vocab_size)]\n        scores: list[float] = [-10000.0] * vocab_size\n        toktypes: list[int] = [SentencePieceTokenTypes.UNUSED] * vocab_size\n\n        for token_id in range(tokenizer.vocab_size()):\n            piece = tokenizer.IdToPiece(token_id)\n            text = piece.encode(\"utf-8\")\n            score = tokenizer.GetScore(token_id)\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.IsUnknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.IsControl(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.IsUnused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.IsByte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n\n            tokens[token_id] = text\n            scores[token_id] = score\n            toktypes[token_id] = toktype\n\n        added_tokens_file = self.dir_model / 'added_tokens.json'\n        if added_tokens_file.is_file():\n            with open(added_tokens_file, \"r\", encoding=\"utf-8\") as f:\n                added_tokens_json = json.load(f)\n                for key in added_tokens_json:\n                    token_id = added_tokens_json[key]\n                    if token_id >= vocab_size:\n                        logger.warning(f'ignore token {token_id}: id is out of range, max={vocab_size - 1}')\n                        continue\n\n                    tokens[token_id] = key.encode(\"utf-8\")\n                    scores[token_id] = -1000.0\n                    toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n\n        if vocab_size > len(tokens):\n            pad_count = vocab_size - len(tokens)\n            logger.debug(f\"Padding vocab with {pad_count} token(s) - [PAD1] through [PAD{pad_count}]\")\n            for i in range(1, pad_count + 1):\n                tokens.append(bytes(f\"[PAD{i}]\", encoding=\"utf-8\"))\n                scores.append(-1000.0)\n                toktypes.append(SentencePieceTokenTypes.UNUSED)\n\n        self.gguf_writer.add_tokenizer_model(\"t5\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n        self.gguf_writer.add_add_space_prefix(add_prefix)\n        self.gguf_writer.add_remove_extra_whitespaces(remove_whitespaces)\n        if precompiled_charsmap:\n            self.gguf_writer.add_precompiled_charsmap(precompiled_charsmap)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n        self.gguf_writer.add_add_bos_token(False)\n        self.gguf_writer.add_add_eos_token(True)\n\n    def set_gguf_parameters(self):\n        if (n_ctx := self.find_hparam([\"n_positions\"], optional=True)) is None:\n            logger.warning(\"Couldn't find context length in config.json, assuming default value of 512\")\n            n_ctx = 512\n        self.gguf_writer.add_context_length(n_ctx)\n        self.gguf_writer.add_embedding_length(self.hparams[\"d_model\"])\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"d_ff\"])\n        self.gguf_writer.add_block_count(self.hparams[\"num_layers\"])\n        self.gguf_writer.add_head_count(self.hparams[\"num_heads\"])\n        self.gguf_writer.add_key_length(self.hparams[\"d_kv\"])\n        self.gguf_writer.add_value_length(self.hparams[\"d_kv\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_relative_attn_buckets_count(self.hparams[\"relative_attention_num_buckets\"])\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_decoder_start_token_id(self.hparams[\"decoder_start_token_id\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        # T5 based models contain shared token embeddings tensors saved randomly as either \"encoder.embed_tokens.weight\",\n        # \"decoder.embed_tokens.weight\" or \"shared.weight\" tensor. In some models there are even multiple of them stored\n        # in the safetensors files. We use the first tensor from these three as the token embeddings for both encoder\n        # and decoder and ignore the remaining ones.\n        if name in [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\", \"shared.weight\"]:\n            if not self.shared_token_embeddings_found:\n                name = \"shared.weight\"\n                self.shared_token_embeddings_found = True\n            else:\n                logger.debug(f\"Skipping shared tensor {name!r} in safetensors so that convert can end normally.\")\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"T5EncoderModel\")\nclass T5EncoderModel(Model):\n    model_arch = gguf.MODEL_ARCH.T5ENCODER\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shared_token_embeddings_found = False\n\n    def set_vocab(self):\n        # to avoid TypeError: Descriptors cannot be created directly\n        # exception when importing sentencepiece_model_pb2\n        os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n        from sentencepiece import SentencePieceProcessor\n        from sentencepiece import sentencepiece_model_pb2 as model\n\n        tokenizer_path = self.dir_model / 'tokenizer.model'\n\n        # many older models use spiece.model tokenizer model filename\n        if not tokenizer_path.is_file():\n            tokenizer_path = self.dir_model / 'spiece.model'\n\n        if not tokenizer_path.is_file():\n            raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\n\n        sentencepiece_model = model.ModelProto()  # pyright: ignore[reportAttributeAccessIssue]\n        sentencepiece_model.ParseFromString(open(tokenizer_path, \"rb\").read())\n\n        # some models like Pile-T5 family use BPE tokenizer instead of Unigram\n        if sentencepiece_model.trainer_spec.model_type == 2:  # BPE\n            # assure the tokenizer model file name is correct\n            assert tokenizer_path.name == 'tokenizer.model'\n            return self._set_vocab_sentencepiece()\n        else:\n            assert sentencepiece_model.trainer_spec.model_type == 1  # UNIGRAM\n\n        add_prefix = sentencepiece_model.normalizer_spec.add_dummy_prefix\n        remove_whitespaces = sentencepiece_model.normalizer_spec.remove_extra_whitespaces\n        precompiled_charsmap = sentencepiece_model.normalizer_spec.precompiled_charsmap\n\n        tokenizer = SentencePieceProcessor()\n        tokenizer.LoadFromFile(str(tokenizer_path))\n\n        vocab_size = self.hparams.get('vocab_size', tokenizer.vocab_size())\n\n        tokens: list[bytes] = [f\"[PAD{i}]\".encode(\"utf-8\") for i in range(vocab_size)]\n        scores: list[float] = [-10000.0] * vocab_size\n        toktypes: list[int] = [SentencePieceTokenTypes.UNUSED] * vocab_size\n\n        for token_id in range(tokenizer.vocab_size()):\n            piece = tokenizer.IdToPiece(token_id)\n            text = piece.encode(\"utf-8\")\n            score = tokenizer.GetScore(token_id)\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.IsUnknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.IsControl(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.IsUnused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.IsByte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n\n            tokens[token_id] = text\n            scores[token_id] = score\n            toktypes[token_id] = toktype\n\n        added_tokens_file = self.dir_model / 'added_tokens.json'\n        if added_tokens_file.is_file():\n            with open(added_tokens_file, \"r\", encoding=\"utf-8\") as f:\n                added_tokens_json = json.load(f)\n                for key in added_tokens_json:\n                    token_id = added_tokens_json[key]\n                    if token_id >= vocab_size:\n                        logger.warning(f'ignore token {token_id}: id is out of range, max={vocab_size - 1}')\n                        continue\n\n                    tokens[token_id] = key.encode(\"utf-8\")\n                    scores[token_id] = -1000.0\n                    toktypes[token_id] = SentencePieceTokenTypes.USER_DEFINED\n\n        if vocab_size > len(tokens):\n            pad_count = vocab_size - len(tokens)\n            logger.debug(f\"Padding vocab with {pad_count} token(s) - [PAD1] through [PAD{pad_count}]\")\n            for i in range(1, pad_count + 1):\n                tokens.append(bytes(f\"[PAD{i}]\", encoding=\"utf-8\"))\n                scores.append(-1000.0)\n                toktypes.append(SentencePieceTokenTypes.UNUSED)\n\n        self.gguf_writer.add_tokenizer_model(\"t5\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n        self.gguf_writer.add_add_space_prefix(add_prefix)\n        self.gguf_writer.add_remove_extra_whitespaces(remove_whitespaces)\n        if precompiled_charsmap:\n            self.gguf_writer.add_precompiled_charsmap(precompiled_charsmap)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n        self.gguf_writer.add_add_bos_token(False)\n        self.gguf_writer.add_add_eos_token(True)\n\n    def set_gguf_parameters(self):\n        if (n_ctx := self.find_hparam([\"n_positions\"], optional=True)) is None:\n            logger.warning(\"Couldn't find context length in config.json, assuming default value of 512\")\n            n_ctx = 512\n        self.gguf_writer.add_context_length(n_ctx)\n        self.gguf_writer.add_embedding_length(self.hparams[\"d_model\"])\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"d_ff\"])\n        self.gguf_writer.add_block_count(self.hparams[\"num_layers\"])\n        self.gguf_writer.add_head_count(self.hparams[\"num_heads\"])\n        self.gguf_writer.add_key_length(self.hparams[\"d_kv\"])\n        self.gguf_writer.add_value_length(self.hparams[\"d_kv\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_relative_attn_buckets_count(self.hparams[\"relative_attention_num_buckets\"])\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        # T5 based models contain shared token embeddings tensors saved randomly as either \"encoder.embed_tokens.weight\",\n        # \"decoder.embed_tokens.weight\" or \"shared.weight\" tensor. In some models there are even multiple of them stored\n        # in the safetensors files. We use the first tensor from these three as the token embeddings for both encoder\n        # and decoder and ignore the remaining ones.\n        if name in [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\", \"shared.weight\"]:\n            if not self.shared_token_embeddings_found:\n                name = \"shared.weight\"\n                self.shared_token_embeddings_found = True\n            else:\n                logger.debug(f\"Skipping shared tensor {name!r} in safetensors so that convert can end normally.\")\n                return []\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"JAISLMHeadModel\")\nclass JaisModel(Model):\n    model_arch = gguf.MODEL_ARCH.JAIS\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # SwigLU activation\n        assert self.hparams[\"activation_function\"] == \"swiglu\"\n        # ALiBi position embedding\n        assert self.hparams[\"position_embedding_type\"] == \"alibi\"\n\n        # Embeddings scale\n        self.embeddings_scale = 1.0\n        if 'mup_embeddings_scale' in self.hparams:\n            self.embeddings_scale = self.hparams['mup_embeddings_scale']\n        elif 'embeddings_scale' in self.hparams:\n            self.embeddings_scale = self.hparams['embeddings_scale']\n        else:\n            assert False\n\n        self.width_scale = 1.0\n        if 'mup_output_alpha' in self.hparams:\n            assert 'mup_width_scale' in self.hparams\n            self.width_scale = self.hparams['mup_output_alpha'] * self.hparams['mup_width_scale']\n        elif 'width_scale' in self.hparams:\n            self.width_scale = self.hparams['width_scale']\n        else:\n            assert False\n\n        self.max_alibi_bias = 8.0\n\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_block_count(self.hparams[\"n_layer\"])\n        self.gguf_writer.add_context_length(self.hparams[\"n_positions\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"n_inner\"])\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        tensors: list[tuple[str, Tensor]] = []\n\n        # we don't need these\n        if name.endswith((\".attn.bias\")):\n            return tensors\n\n        if name.endswith((\"relative_pe.slopes\")):\n            # Calculate max ALiBi bias (this is the inverse of the ALiBi calculation)\n            # Some other models has max_alibi_bias spelled out explicitly in the hyperparams,\n            # but Jais's PyTorch model simply precalculates the slope values and places them\n            # in relative_pes.slopes\n            n_head_closest_log2 = 2 ** math.floor(math.log2(self.hparams[\"n_head\"]))\n            first_val = float(data_torch[0].item())\n            self.max_alibi_bias = -round(math.log2(first_val) * n_head_closest_log2)\n\n            return tensors\n\n        if name.endswith((\".c_attn.weight\", \".c_proj.weight\", \".c_fc.weight\", \".c_fc2.weight\")):\n            data_torch = data_torch.transpose(1, 0)\n\n        new_name = self.map_tensor_name(name)\n\n        if new_name == self.format_tensor_name(gguf.MODEL_TENSOR.TOKEN_EMBD):\n            tensors.append((new_name, data_torch * self.embeddings_scale))\n        elif new_name == self.format_tensor_name(gguf.MODEL_TENSOR.OUTPUT):\n            tensors.append((new_name, data_torch * self.width_scale))\n        else:\n            tensors.append((new_name, data_torch))\n\n        return tensors\n\n    def prepare_tensors(self):\n        super().prepare_tensors()\n        self.gguf_writer.add_max_alibi_bias(self.max_alibi_bias)\n\n\n@Model.register(\"GlmForCausalLM\", \"ChatGLMModel\", \"ChatGLMForConditionalGeneration\")\nclass ChatGLMModel(Model):\n    model_arch = gguf.MODEL_ARCH.CHATGLM\n\n    def set_vocab_chatglm3(self):\n        dir_model = self.dir_model\n        hparams = self.hparams\n        tokens: list[bytes] = []\n        toktypes: list[int] = []\n        scores: list[float] = []\n\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(dir_model, trust_remote_code=True)\n        vocab_size = hparams.get(\"padded_vocab_size\", len(tokenizer.get_vocab()))\n        assert max(tokenizer.get_vocab().values()) < vocab_size\n        role_special_tokens = [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|observation|>\"]\n        special_tokens = [\"[MASK]\", \"[gMASK]\", \"[sMASK]\", \"sop\", \"eop\"] + role_special_tokens\n        for token_id in range(vocab_size):\n            piece = tokenizer._convert_id_to_token(token_id)\n            if token_id == 0:\n                piece = \"<unk>\"\n            elif token_id == 1:\n                piece = \"<bos>\"\n            elif token_id == 2:\n                piece = \"<eos>\"\n\n            text = piece.encode(\"utf-8\")\n            score = 0.0\n            # Referencing the tokenizer Python implementation(https://huggingface.co/THUDM/chatglm3-6b/blob/main/tokenization_chatglm.py),\n            # it is only valid if it is less than tokenizer.tokenizer.sp_model.vocab_size()\n            if len(piece) != 0 and token_id < tokenizer.tokenizer.sp_model.vocab_size():\n                score = tokenizer.tokenizer.sp_model.get_score(token_id)\n\n            if token_id >= tokenizer.tokenizer.sp_model.vocab_size():\n                if piece in special_tokens:\n                    toktype = SentencePieceTokenTypes.CONTROL\n                elif len(piece) == 0:\n                    text = f\"[PAD{token_id}]\".encode(\"utf-8\")\n                    toktype = SentencePieceTokenTypes.UNUSED\n                else:\n                    toktype = SentencePieceTokenTypes.USER_DEFINED\n                tokens.append(text)\n                scores.append(score)\n                toktypes.append(toktype)\n                continue\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.tokenizer.sp_model.is_unknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.tokenizer.sp_model.is_control(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.tokenizer.sp_model.is_unused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.tokenizer.sp_model.is_byte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n\n            tokens.append(text)\n            scores.append(score)\n            toktypes.append(toktype)\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        # glm3 needs prefix and suffix formatted as:\n        # prompt = \"[gMASK]sop<|user|>\\n\" + prompt + \"<|assistant|>\"\n        self.gguf_writer.add_tokenizer_pre(\"chatglm-spm\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    @staticmethod\n    def token_bytes_to_string(b):\n        from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n        byte_encoder = bytes_to_unicode()\n        return ''.join([byte_encoder[ord(char)] for char in b.decode('latin-1')])\n\n    @staticmethod\n    def bpe(mergeable_ranks: dict[bytes, int], token: bytes, max_rank: int | None = None) -> list[bytes]:\n        parts = [bytes([b]) for b in token]\n        while True:\n            min_idx = None\n            min_rank = None\n            for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n                rank = mergeable_ranks.get(pair[0] + pair[1])\n                if rank is not None and (min_rank is None or rank < min_rank):\n                    min_idx = i\n                    min_rank = rank\n            if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n                break\n            assert min_idx is not None\n            parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n        return parts\n\n    def set_vocab(self):\n        if \"THUDM/chatglm3-6b\" in self.hparams.get(\"_name_or_path\", \"\"):\n            self.set_vocab_chatglm3()\n            return\n\n        dir_model = self.dir_model\n        hparams = self.hparams\n        tokens: list[str] = []\n        toktypes: list[int] = []\n\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(dir_model, trust_remote_code=True)\n        vocab_size = hparams.get(\"padded_vocab_size\",hparams[\"vocab_size\"])\n        assert max(tokenizer.get_vocab().values()) < vocab_size\n\n        tokens, toktypes, tokpre = self.get_vocab_base()\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=True)\n        # only add special tokens when they were not already loaded from config.json\n        special_vocab._set_special_token(\"eos\", tokenizer.get_added_vocab()[\"<|endoftext|>\"])\n        special_vocab._set_special_token(\"eot\", tokenizer.get_added_vocab()[\"<|user|>\"])\n        # this one is usually not in config.json anyway\n        special_vocab._set_special_token(\"unk\", tokenizer.get_added_vocab()[\"<|endoftext|>\"])\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def set_gguf_parameters(self):\n        n_embed = self.hparams.get(\"hidden_size\", self.hparams.get(\"n_embed\"))\n        n_head = self.hparams.get(\"n_head\", self.hparams.get(\"num_attention_heads\"))\n        n_head_kv = self.hparams.get(\"multi_query_group_num\", self.hparams.get(\"num_key_value_heads\", n_head))\n        self.gguf_writer.add_context_length(self.hparams.get(\"seq_length\", n_embed))\n        self.gguf_writer.add_embedding_length(n_embed)\n        self.gguf_writer.add_feed_forward_length(self.hparams.get(\"ffn_hidden_size\", self.hparams.get(\"intermediate_size\", 4 * n_embed)))\n        self.gguf_writer.add_block_count(self.hparams.get(\"num_layers\", self.hparams[\"num_hidden_layers\"]))\n        self.gguf_writer.add_head_count(n_head)\n        self.gguf_writer.add_head_count_kv(n_head_kv)\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams.get(\"layernorm_epsilon\",1e-5))\n        self.gguf_writer.add_file_type(self.ftype)\n        if \"attention_dim\" in self.hparams:\n            rope_dim = self.hparams[\"attention_dim\"]\n        else:\n            rope_dim = self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"]\n        self.gguf_writer.add_rope_dimension_count(int(rope_dim * self.hparams.get(\"partial_rotary_factor\", 0.5)))\n        self.gguf_writer.add_add_bos_token(False)\n        rope_freq = 10000\n        if \"rope_ratio\" in self.hparams:\n            rope_freq = rope_freq * self.hparams[\"rope_ratio\"]\n        self.gguf_writer.add_rope_freq_base(rope_freq)\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        del bid  # unused\n\n        if name.endswith(\".rotary_pos_emb.inv_freq\") or name.startswith(\"model.vision.\"):\n            return []\n\n        name = name.removeprefix(\"transformer.\")\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"NemotronForCausalLM\")\nclass NemotronModel(Model):\n    model_arch = gguf.MODEL_ARCH.NEMOTRON\n\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        self.gguf_writer.add_pad_token_id(0)\n        self.gguf_writer.add_unk_token_id(1)\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n\n        f_norm_eps = self.find_hparam([\"layer_norm_eps\", \"layer_norm_epsilon\", \"norm_epsilon\", \"norm_eps\"])\n        self.gguf_writer.add_layer_norm_eps(f_norm_eps)\n\n        # * Partial RoPE\n        rot_pct = self.find_hparam([\"partial_rotary_factor\", \"rope_pct\", \"rope_percent\"])\n        n_embd = self.find_hparam([\"hidden_size\", \"n_embd\"])\n        n_head = self.find_hparam([\"num_attention_heads\", \"n_head\"])\n        self.gguf_writer.add_rope_dimension_count(int(rot_pct * n_embd) // n_head)\n\n        # * RopeScaling for Nemotron\n        if \"rope_scaling\" not in self.hparams or self.hparams[\"rope_scaling\"] is None:\n            self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.NONE)\n        else:\n            self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n            self.gguf_writer.add_rope_scaling_factor(self.hparams[\"factor\"])\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # * Adding +1 to LayerNorm's weights here to implement layernorm1p w/o changing anything on the GGML engine side\n        #   model.layers.{l}.input_layernorm.weight\n        #   model.layers.{l}.post_attention_layernorm.weight\n        #   model.norm.weight\n        if name.endswith(\"norm.weight\"):\n            data_torch = data_torch + 1\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n\n@Model.register(\"ExaoneForCausalLM\")\nclass ExaoneModel(Model):\n    model_arch = gguf.MODEL_ARCH.EXAONE\n\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n\n        assert (hparams[\"activation_function\"] == \"silu\")\n\n        max_position_embeddings = hparams[\"max_position_embeddings\"]\n        embed_dim = hparams[\"hidden_size\"]\n        num_heads = hparams[\"num_attention_heads\"]\n        num_kv_heads = hparams.get(\"num_key_value_heads\", num_heads)\n        layer_norm_eps = hparams[\"layer_norm_epsilon\"]\n        intermediate_size = hparams[\"intermediate_size\"] if \"intermediate_size\" in hparams else 4 * embed_dim\n        num_layers = hparams[\"num_layers\"]\n        # ignore for now as EXAONE-3.0-7.8B-Instruct attentino_dropout is 0.0\n        # attention_dropout_rate = hparams[\"attention_dropout\"]\n        # ignore for now as EXAONE-3.0-7.8B-Instruct embed_dropout is 0.0\n        # embed_dropout_rate = hparams[\"embed_dropout\"]\n        self.gguf_writer.add_embedding_length(embed_dim)\n        self.gguf_writer.add_head_count(num_heads)\n        self.gguf_writer.add_head_count_kv(num_kv_heads)\n        self.gguf_writer.add_context_length(max_position_embeddings)\n        self.gguf_writer.add_layer_norm_rms_eps(layer_norm_eps)\n        self.gguf_writer.add_feed_forward_length(intermediate_size)\n        self.gguf_writer.add_block_count(num_layers)\n        self.gguf_writer.add_file_type(self.ftype)\n\n        if (rope_theta := self.hparams.get(\"rope_theta\")) is not None:\n            self.gguf_writer.add_rope_freq_base(rope_theta)\n        rotary_factor = self.find_hparam([\"partial_rotary_factor\", \"rope_pct\"], optional=True)\n        rotary_factor = rotary_factor if rotary_factor is not None else 1.0\n        self.gguf_writer.add_rope_dimension_count(int(rotary_factor * (hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"])))\n        if hparams.get(\"rope_scaling\") is not None and \"factor\" in hparams[\"rope_scaling\"]:\n            if hparams[\"rope_scaling\"].get(\"type\") == \"linear\":\n                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n                self.gguf_writer.add_rope_scaling_factor(hparams[\"rope_scaling\"][\"factor\"])\n\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        if rope_scaling := self.find_hparam([\"rope_scaling\"], optional=True):\n            if rope_scaling.get(\"rope_type\", '').lower() == \"llama3\":\n                base = self.hparams.get(\"rope_theta\", 10000.0)\n                dim = self.hparams.get(\"head_dim\", self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n                freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n\n                factor = rope_scaling.get(\"factor\", 8.0)\n                low_freq_factor = rope_scaling.get(\"low_freq_factor\", 1.0)\n                high_freq_factor = rope_scaling.get(\"high_freq_factor\", 4.0)\n                old_context_len = self.hparams.get(\"original_max_position_embeddings\", 8192)\n\n                low_freq_wavelen = old_context_len / low_freq_factor\n                high_freq_wavelen = old_context_len / high_freq_factor\n                assert low_freq_wavelen != high_freq_wavelen\n\n                rope_factors = []\n                for freq in freqs:\n                    wavelen = 2 * math.pi / freq\n                    if wavelen < high_freq_wavelen:\n                        rope_factors.append(1)\n                    elif wavelen > low_freq_wavelen:\n                        rope_factors.append(factor)\n                    else:\n                        smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n                        rope_factors.append(1 / ((1 - smooth) / factor + smooth))\n\n                yield (self.format_tensor_name(gguf.MODEL_TENSOR.ROPE_FREQS), torch.tensor(rope_factors, dtype=torch.float32))\n\n\n@Model.register(\"GraniteForCausalLM\")\nclass GraniteModel(LlamaModel):\n    \"\"\"Conversion for IBM's GraniteForCausalLM\"\"\"\n    model_arch = gguf.MODEL_ARCH.GRANITE\n\n    def set_gguf_parameters(self):\n        \"\"\"Granite uses standard llama parameters with the following differences:\n\n        - No head_dim support\n        - New multiplier params:\n            - attention_scale\n            - embedding_scale\n            - residual_scale\n        - logits_scaling\n        \"\"\"\n        if head_dim := self.hparams.pop(\"head_dim\", None):\n            logger.warning(\"Ignoring head_dim (%s) from config for Granite\", head_dim)\n        super().set_gguf_parameters()\n        # NOTE: Convert _multiplier params to _scale params for naming\n        #   consistency\n        if attention_scale := self.hparams.get(\"attention_multiplier\"):\n            self.gguf_writer.add_attention_scale(attention_scale)\n            logger.info(\"gguf: (granite) attention_scale = %s\", attention_scale)\n        if embedding_scale := self.hparams.get(\"embedding_multiplier\"):\n            self.gguf_writer.add_embedding_scale(embedding_scale)\n            logger.info(\"gguf: (granite) embedding_scale = %s\", embedding_scale)\n        if residual_scale := self.hparams.get(\"residual_multiplier\"):\n            self.gguf_writer.add_residual_scale(residual_scale)\n            logger.info(\"gguf: (granite) residual_scale = %s\", residual_scale)\n        if logits_scale := self.hparams.get(\"logits_scaling\"):\n            self.gguf_writer.add_logit_scale(logits_scale)\n            logger.info(\"gguf: (granite) logits_scale = %s\", logits_scale)\n\n\n@Model.register(\"GraniteMoeForCausalLM\")\nclass GraniteMoeModel(GraniteModel):\n    \"\"\"Conversion for IBM's GraniteMoeForCausalLM\"\"\"\n    model_arch = gguf.MODEL_ARCH.GRANITE_MOE\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        \"\"\"In modeling_granitemoe, the JetMoe implementation of parallel experts\n        is used. This essentially merges w1 and w3 into a single tensor with 2x\n        the hidden size that is then split during forward. To keep compatibility\n        with existing mixtral support, we pull them apart here.\n        \"\"\"\n\n        if name.endswith(\"block_sparse_moe.input_linear.weight\"):\n            ffn_dim = self.hparams[\"intermediate_size\"]\n            assert data_torch.shape[-2] == 2 * ffn_dim, \"Merged FFN tensor size must be 2 * intermediate_size\"\n            gate, up = data_torch[..., :ffn_dim, :], data_torch[..., ffn_dim:, :]\n            return [\n                (self.format_tensor_name(gguf.MODEL_TENSOR.FFN_GATE_EXP, bid), gate),\n                (self.format_tensor_name(gguf.MODEL_TENSOR.FFN_UP_EXP, bid), up),\n            ]\n\n        return super().modify_tensors(data_torch, name, bid)\n\n\n@Model.register(\"ChameleonForConditionalGeneration\")\n@Model.register(\"ChameleonForCausalLM\")  # obsolete\nclass ChameleonModel(Model):\n    model_arch = gguf.MODEL_ARCH.CHAMELEON\n\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_swin_norm(self.hparams.get(\"swin_norm\", False))\n\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # ignore image tokenizer for now\n        # TODO: remove this once image support is implemented for Chameleon\n        if name.startswith(\"model.vqmodel\"):\n            return []\n\n        n_head = self.hparams[\"num_attention_heads\"]\n        n_kv_head = self.hparams.get(\"num_key_value_heads\")\n        hidden_dim = self.hparams.get(\"hidden_size\")\n\n        if name.endswith((\"q_proj.weight\", \"q_proj.bias\")):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_head)\n        if name.endswith((\"k_proj.weight\", \"k_proj.bias\")):\n            data_torch = LlamaModel.permute(data_torch, n_head, n_kv_head)\n        if name.endswith((\"q_norm.weight\", \"q_norm.bias\")):\n            data_torch = ChameleonModel._reverse_hf_permute(data_torch, n_head, hidden_dim)\n        if name.endswith((\"k_norm.weight\", \"k_norm.bias\")):\n            data_torch = ChameleonModel._reverse_hf_permute(data_torch, n_kv_head, hidden_dim)\n\n        return [(self.map_tensor_name(name), data_torch)]\n\n    # see: https://github.com/huggingface/transformers/blob/72fb02c47dbbe1999ae105319f24631cad6e2e00/src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py#L176-L203\n    @staticmethod\n    def _reverse_hf_permute(data_torch, n_heads, hidden_dim):\n        head_dim = hidden_dim // n_heads\n        data_torch = data_torch[0].view(2, head_dim // 2).t().reshape(1, -1)\n        data_torch = data_torch.repeat_interleave(n_heads, 0)\n        return data_torch\n\n\n###### CONVERSION LOGIC ######\n\n\n# tree of lazy tensors\nclass LazyTorchTensor(gguf.LazyBase):\n    _tensor_type = torch.Tensor\n    # to keep the type-checker happy\n    dtype: torch.dtype\n    shape: torch.Size\n\n    # only used when converting a torch.Tensor to a np.ndarray\n    _dtype_map: dict[torch.dtype, type] = {\n        torch.float16: np.float16,\n        torch.float32: np.float32,\n    }\n\n    # used for safetensors slices\n    # ref: https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/src/lib.rs#L1046\n    # TODO: uncomment U64, U32, and U16, ref: https://github.com/pytorch/pytorch/issues/58734\n    _dtype_str_map: dict[str, torch.dtype] = {\n        \"F64\": torch.float64,\n        \"F32\": torch.float32,\n        \"BF16\": torch.bfloat16,\n        \"F16\": torch.float16,\n        # \"U64\": torch.uint64,\n        \"I64\": torch.int64,\n        # \"U32\": torch.uint32,\n        \"I32\": torch.int32,\n        # \"U16\": torch.uint16,\n        \"I16\": torch.int16,\n        \"U8\": torch.uint8,\n        \"I8\": torch.int8,\n        \"BOOL\": torch.bool,\n        \"F8_E4M3\": torch.float8_e4m3fn,\n        \"F8_E5M2\": torch.float8_e5m2,\n    }\n\n    def numpy(self) -> gguf.LazyNumpyTensor:\n        dtype = self._dtype_map[self.dtype]\n        return gguf.LazyNumpyTensor(\n            meta=gguf.LazyNumpyTensor.meta_with_dtype_and_shape(dtype, self.shape),\n            args=(self,),\n            func=(lambda s: s.numpy())\n        )\n\n    @classmethod\n    def meta_with_dtype_and_shape(cls, dtype: torch.dtype, shape: tuple[int, ...]) -> Tensor:\n        return torch.empty(size=shape, dtype=dtype, device=\"meta\")\n\n    @classmethod\n    def from_safetensors_slice(cls, st_slice: Any) -> Tensor:\n        dtype = cls._dtype_str_map[st_slice.get_dtype()]\n        shape: tuple[int, ...] = tuple(st_slice.get_shape())\n        lazy = cls(meta=cls.meta_with_dtype_and_shape(dtype, shape), args=(st_slice,), func=lambda s: s[:])\n        return cast(torch.Tensor, lazy)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        del types  # unused\n\n        if kwargs is None:\n            kwargs = {}\n\n        if func is torch.Tensor.numpy:\n            return args[0].numpy()\n\n        return cls._wrap_fn(func)(*args, **kwargs)\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Convert a huggingface model to a GGML compatible file\")\n    parser.add_argument(\n        \"--vocab-only\", action=\"store_true\",\n        help=\"extract only the vocab\",\n    )\n    parser.add_argument(\n        \"--outfile\", type=Path,\n        help=\"path to write to; default: based on input. {ftype} will be replaced by the outtype.\",\n    )\n    parser.add_argument(\n        \"--outtype\", type=str, choices=[\"f32\", \"f16\", \"bf16\", \"q8_0\", \"tq1_0\", \"tq2_0\", \"auto\"], default=\"f16\",\n        help=\"output format - use f32 for float32, f16 for float16, bf16 for bfloat16, q8_0 for Q8_0, tq1_0 or tq2_0 for ternary, and auto for the highest-fidelity 16-bit float type depending on the first loaded tensor type\",\n    )\n    parser.add_argument(\n        \"--bigendian\", action=\"store_true\",\n        help=\"model is executed on big endian machine\",\n    )\n    parser.add_argument(\n        \"model\", type=Path,\n        help=\"directory containing model file\",\n        nargs=\"?\",\n    )\n    parser.add_argument(\n        \"--use-temp-file\", action=\"store_true\",\n        help=\"use the tempfile library while processing (helpful when running out of memory, process killed)\",\n    )\n    parser.add_argument(\n        \"--no-lazy\", action=\"store_true\",\n        help=\"use more RAM by computing all outputs before writing (use in case lazy evaluation is broken)\",\n    )\n    parser.add_argument(\n        \"--model-name\", type=str, default=None,\n        help=\"name of the model\",\n    )\n    parser.add_argument(\n        \"--verbose\", action=\"store_true\",\n        help=\"increase output verbosity\",\n    )\n    parser.add_argument(\n        \"--split-max-tensors\", type=int, default=0,\n        help=\"max tensors in each split\",\n    )\n    parser.add_argument(\n        \"--split-max-size\", type=str, default=\"0\",\n        help=\"max size per split N(M|G)\",\n    )\n    parser.add_argument(\n        \"--dry-run\", action=\"store_true\",\n        help=\"only print out a split plan and exit, without writing any new files\",\n    )\n    parser.add_argument(\n        \"--no-tensor-first-split\", action=\"store_true\",\n        help=\"do not add tensors to the first split (disabled by default)\"\n    )\n    parser.add_argument(\n        \"--metadata\", type=Path,\n        help=\"Specify the path for an authorship metadata override file\"\n    )\n    parser.add_argument(\n        \"--print-supported-models\", action=\"store_true\",\n        help=\"Print the supported models\"\n    )\n\n    args = parser.parse_args()\n    if not args.print_supported_models and args.model is None:\n        parser.error(\"the following arguments are required: model\")\n    return args\n\n\ndef split_str_to_n_bytes(split_str: str) -> int:\n    if split_str.endswith(\"K\"):\n        n = int(split_str[:-1]) * 1000\n    elif split_str.endswith(\"M\"):\n        n = int(split_str[:-1]) * 1000 * 1000\n    elif split_str.endswith(\"G\"):\n        n = int(split_str[:-1]) * 1000 * 1000 * 1000\n    elif split_str.isnumeric():\n        n = int(split_str)\n    else:\n        raise ValueError(f\"Invalid split size: {split_str}, must be a number, optionally followed by K, M, or G\")\n\n    if n < 0:\n        raise ValueError(f\"Invalid split size: {split_str}, must be positive\")\n\n    return n\n\n\ndef main() -> None:\n    args = parse_args()\n\n    if args.print_supported_models:\n        logger.error(\"Supported models:\")\n        Model.print_registered_models()\n        sys.exit(0)\n\n    if args.verbose:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    dir_model = args.model\n\n    if not dir_model.is_dir():\n        logger.error(f'Error: {args.model} is not a directory')\n        sys.exit(1)\n\n    ftype_map: dict[str, gguf.LlamaFileType] = {\n        \"f32\": gguf.LlamaFileType.ALL_F32,\n        \"f16\": gguf.LlamaFileType.MOSTLY_F16,\n        \"bf16\": gguf.LlamaFileType.MOSTLY_BF16,\n        \"q8_0\": gguf.LlamaFileType.MOSTLY_Q8_0,\n        \"tq1_0\": gguf.LlamaFileType.MOSTLY_TQ1_0,\n        \"tq2_0\": gguf.LlamaFileType.MOSTLY_TQ2_0,\n        \"auto\": gguf.LlamaFileType.GUESSED,\n    }\n\n    is_split = args.split_max_tensors > 0 or args.split_max_size != \"0\"\n    if args.use_temp_file and is_split:\n        logger.error(\"Error: Cannot use temp file when splitting\")\n        sys.exit(1)\n\n    if args.outfile is not None:\n        fname_out = args.outfile\n    else:\n        fname_out = dir_model\n\n    logger.info(f\"Loading model: {dir_model.name}\")\n\n    hparams = Model.load_hparams(dir_model)\n\n    with torch.inference_mode():\n        output_type = ftype_map[args.outtype]\n        model_architecture = hparams[\"architectures\"][0]\n\n        try:\n            model_class = Model.from_model_architecture(model_architecture)\n        except NotImplementedError:\n            logger.error(f\"Model {model_architecture} is not supported\")\n            sys.exit(1)\n\n        model_instance = model_class(dir_model=dir_model, ftype=output_type, fname_out=fname_out,\n                                     is_big_endian=args.bigendian, use_temp_file=args.use_temp_file,\n                                     eager=args.no_lazy,\n                                     metadata_override=args.metadata, model_name=args.model_name,\n                                     split_max_tensors=args.split_max_tensors,\n                                     split_max_size=split_str_to_n_bytes(args.split_max_size), dry_run=args.dry_run,\n                                     small_first_shard=args.no_tensor_first_split)\n\n        if args.vocab_only:\n            logger.info(\"Exporting model vocab...\")\n            model_instance.write_vocab()\n            logger.info(f\"Model vocab successfully exported to {model_instance.fname_out}\")\n        else:\n            logger.info(\"Exporting model...\")\n            model_instance.write()\n            out_path = f\"{model_instance.fname_out.parent}{os.sep}\" if is_split else model_instance.fname_out\n            logger.info(f\"Model successfully exported to {out_path}\")\n\n\nif __name__ == '__main__':\n    main()\n",
        "imports": [
            "Tensor",
            "_set_vocab_sentencepiece",
            "the",
            "typing",
            "logging",
            "ast",
            "SentencePieceProcessor",
            "annotations",
            "pathlib",
            "Path",
            "argparse",
            "contextlib",
            "sentencepiece",
            "these",
            "__future__",
            "math",
            "first",
            "os",
            "safetensors",
            "itertools",
            "being",
            "TYPE_CHECKING",
            "other",
            "chain",
            "tokeniser_config.json",
            "enum",
            "sys",
            "sentencepiece_model_pb2",
            "json",
            "sha256",
            "hashlib",
            "config",
            "bytes_to_unicode",
            "IntEnum",
            "gguf",
            "config.json",
            "torch",
            "AutoTokenizer",
            "None",
            "meta",
            "Huggingface",
            "numpy",
            "safe_open",
            "re",
            "transformers",
            "transformers.models.gpt2.tokenization_gpt2"
        ],
        "functions": [
            "shuffle_attn_q_weight",
            "from_model_architecture",
            "_set_vocab_none",
            "phantom",
            "_set_vocab_builtin",
            "_reverse_hf_part",
            "register",
            "_set_vocab_sentencepiece",
            "_reverse_hf_permute",
            "shuffle_attn_output_weight",
            "_set_vocab_llama_hf",
            "_make_divisible",
            "split_str_to_n_bytes",
            "_reverse_hf_permute_part",
            "_stack_qk_norm",
            "main",
            "prepare_metadata",
            "get_tensors",
            "get_vocab_base_pre",
            "weight_quant",
            "parse_args",
            "print_registered_models",
            "set_type",
            "find_hparam",
            "map_tensor_name",
            "write_vocab",
            "bpe",
            "_create_vocab_sentencepiece",
            "_set_vocab_qwen",
            "_ffn_mult_to_intermediate_size",
            "does_token_look_special",
            "_find_multiple",
            "generate_extra_tensors",
            "permute",
            "prepare_tensors",
            "match_model_tensor_name",
            "set_vocab_chatglm3",
            "token_bytes_to_string",
            "get_model_part_names",
            "write",
            "func",
            "get_vocab_base",
            "meta_with_dtype_and_shape",
            "format_tensor_name",
            "_set_vocab_gpt2",
            "numpy",
            "__init_subclass__",
            "from_safetensors_slice",
            "__torch_function__",
            "modify_tensors",
            "set_vocab",
            "load_hparams",
            "__init__",
            "set_gguf_parameters",
            "tensor_force_quant"
        ],
        "variables": [
            "rotary_pct",
            "shared_token_embeddings_found",
            "help",
            "fname_out",
            "rope_factors",
            "USER_DEFINED",
            "n_embed",
            "split_max_tensors",
            "_ffn_multipliers",
            "load_merges",
            "experts",
            "rms_eps",
            "epsilon",
            "rotary_factor",
            "text",
            "prompt",
            "ctx_length",
            "up",
            "data_torch",
            "special_token_types",
            "res",
            "embeddings_scale",
            "n_experts",
            "num_layers",
            "remove_whitespaces",
            "optional",
            "is_safetensors",
            "_q_norms",
            "kwargs",
            "time_mix_extra_dim",
            "scale",
            "data",
            "num_key_value_heads",
            "d_conv",
            "k",
            "rope_scaling",
            "qkv_weights",
            "precompiled_charsmap",
            "parser",
            "AnyModel",
            "tensors",
            "key_name",
            "name2",
            "tokenizer_config_file",
            "norms",
            "bpe_tok_path",
            "head_size",
            "vocab_size",
            "merges",
            "merged",
            "tensor_names",
            "module_path",
            "added_vocab",
            "toktypes",
            "long_factors",
            "block_count",
            "ename",
            "int",
            "n_embd",
            "size_label",
            "byte_encoder",
            "mrope_section",
            "q_per_kv",
            "embedding_scale",
            "chkhsh",
            "_k_norms",
            "_position_offset",
            "_tensor_type",
            "meta",
            "d1",
            "new_v",
            "type",
            "rank",
            "index_name",
            "max_vocab_index",
            "toktype",
            "logits_scale",
            "embed_dropout_rate",
            "path",
            "chktxt",
            "previous_token",
            "old_context_len",
            "chat_eos_token_id",
            "hparams",
            "length",
            "chktok",
            "freqs",
            "ctx",
            "ftype",
            "logger",
            "ffn_config",
            "tokenizer_file",
            "attention_scale",
            "output_type",
            "tokenizer_path",
            "UNKNOWN",
            "rope_dims",
            "base",
            "token_type",
            "f_norm_eps",
            "rope_freq",
            "vocab",
            "inner_dim",
            "n_head_closest_log2",
            "chat_eos_token",
            "max_position_embeddings",
            "min_rank",
            "piece",
            "r",
            "tok_embd_name",
            "model_architecture",
            "action",
            "n_ctx",
            "num_attention_heads",
            "d2",
            "split_max_size",
            "num_kv_heads",
            "sentencepiece_model",
            "num_heads",
            "extra",
            "max_name_len",
            "ffn_dim",
            "added_tokens_json",
            "result",
            "rot_pct",
            "high_freq_wavelen",
            "rope_dim",
            "str",
            "_experts",
            "shape_str",
            "ff_dim",
            "lines",
            "size",
            "bool",
            "add_to_gguf",
            "args",
            "pooling_path",
            "qkv_bias",
            "_num_kv_heads",
            "CONTROL",
            "dtype",
            "tokpre",
            "default_pre",
            "hidden_size",
            "n_dims",
            "token_id",
            "added_tokens",
            "pooling",
            "missing",
            "n_ff",
            "vocab_only",
            "tokenizer",
            "head_count",
            "weight_map",
            "model_name",
            "max",
            "pooling_type",
            "theta",
            "low_freq_wavelen",
            "output_name",
            "is_big_endian",
            "endianess",
            "model_arch",
            "use_temp_file",
            "token_content",
            "embed_dim",
            "index_file",
            "first_val",
            "added_tokens_decoder",
            "lazy",
            "hidden_dim",
            "sliding_window",
            "role_special_tokens",
            "n",
            "model_instance",
            "low_freq_factor",
            "time_decay_extra_dim",
            "name",
            "special_vocab",
            "weight",
            "attn_config",
            "min_idx",
            "layer_norm_eps",
            "_tok_embd",
            "clip_qkv",
            "level",
            "n_expert",
            "dir_model_card",
            "q",
            "tensor_map",
            "_num_heads",
            "gguf_writer",
            "is_split",
            "add_prefix",
            "factor",
            "old_eos",
            "A",
            "dim",
            "missing_files",
            "remainder",
            "n_head_kv",
            "UNUSED",
            "dir_model",
            "d_state",
            "use_dt_b_c_norm",
            "metadata",
            "model_type",
            "head_count_kv",
            "data_qtype",
            "chat_template",
            "token",
            "dt_rank",
            "score",
            "scores",
            "seems_special",
            "d_inner",
            "merged_name",
            "encoding",
            "max_pos_embds",
            "eager",
            "short_factors",
            "iscale",
            "rms_norm_eps",
            "max_alibi_bias",
            "out_path",
            "smooth",
            "tokenizer_class",
            "attn_factor",
            "pad_vocab",
            "bid",
            "rescale_every_n_layers",
            "attention_dropout_rate",
            "tokenizer_config_json",
            "tokenizer_json",
            "key",
            "expert_count",
            "v",
            "token_text",
            "reverse_vocab",
            "logit_scale",
            "d_model",
            "intermediate_size",
            "high_freq_factor",
            "first_tensor",
            "orig_max_pos_embds",
            "n_kv_head",
            "vocab_reader",
            "match",
            "n_head",
            "special_tokens",
            "residual_scale",
            "metadata_override",
            "count",
            "__name__",
            "head_dim",
            "mergeable_ranks",
            "parts",
            "exp_tensor_names",
            "part_names",
            "model_class",
            "field",
            "BYTE",
            "multiple_of",
            "tokens",
            "name1",
            "pad_count",
            "token_score",
            "NORMAL",
            "shape",
            "old_dtype",
            "rope_scaling_type",
            "added_tokens_file",
            "func",
            "description",
            "None",
            "num_groups",
            "width_scale",
            "new_name",
            "nargs",
            "wavelen",
            "small_first_shard",
            "qkv",
            "token_len",
            "progress",
            "modules",
            "raw_dtype"
        ]
    },
    {
        "file_name": "convert_hf_to_gguf_update.py",
        "language": "python",
        "source_code": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# This script downloads the tokenizer models of the specified models from Huggingface and\n# generates the get_vocab_base_pre() function for convert_hf_to_gguf.py\n#\n# This is necessary in order to analyze the type of pre-tokenizer used by the model and\n# provide the necessary information to llama.cpp via the GGUF header in order to implement\n# the same pre-tokenizer.\n#\n# ref: https://github.com/ggerganov/llama.cpp/pull/6920\n#\n# Instructions:\n#\n# - Add a new model to the \"models\" list\n# - Run the script with your huggingface token:\n#\n#   python3 convert_hf_to_gguf_update.py <huggingface_token>\n#\n# - The convert_hf_to_gguf.py script will have had its get_vocab_base_pre() function updated\n# - Update llama.cpp with the new pre-tokenizer if necessary\n#\n# TODO: generate tokenizer tests for llama.cpp\n#\n\nimport logging\nimport os\nimport pathlib\nimport re\n\nimport requests\nimport sys\nimport json\nimport shutil\n\nfrom hashlib import sha256\nfrom enum import IntEnum, auto\nfrom transformers import AutoTokenizer\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(\"convert_hf_to_gguf_update\")\nsess = requests.Session()\n\n\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\n\n\n# TODO: this string has to exercise as much pre-tokenizer functionality as possible\n#       will be updated with time - contributions welcome\nCHK_TXT = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n\ud83d\ude80 (normal) \ud83d\ude36\u200d\ud83c\udf2b\ufe0f (multiple emojis concatenated) \u2705 \ud83e\udd99\ud83e\udd99 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 \u1780\u17b6\u1793\u17cb\u178f\u17c2\u1796\u17b7\u179f\u17c1\u179f\u17a2\u17b6\u1785\ud83d\ude01 ?\u6211\u60f3\u5728apple\u5de5\u4f5c1314151\u5929\uff5e ------======= \u043d\u0435\u0449\u043e \u043d\u0430 \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n\nif len(sys.argv) == 2:\n    token = sys.argv[1]\n    if not token.startswith(\"hf_\"):\n        logger.info(\"Huggingface token seems invalid\")\n        logger.info(\"Usage: python convert_hf_to_gguf_update.py <huggingface_token>\")\n        sys.exit(1)\nelse:\n    logger.info(\"Usage: python convert_hf_to_gguf_update.py <huggingface_token>\")\n    sys.exit(1)\n\n# TODO: add models here, base models preferred\nmodels = [\n    {\"name\": \"llama-spm\",        \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/meta-llama/Llama-2-7b-hf\", },\n    {\"name\": \"llama-bpe\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },\n    {\"name\": \"phi-3\",            \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\", },\n    {\"name\": \"deepseek-llm\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\", },\n    {\"name\": \"deepseek-coder\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\", },\n    {\"name\": \"falcon\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/falcon-7b\", },\n    {\"name\": \"bert-bge\",         \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/BAAI/bge-small-en-v1.5\", },\n    {\"name\": \"falcon3\",          \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/Falcon3-7B-Base\", },\n    {\"name\": \"bert-bge-large\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/BAAI/bge-large-zh-v1.5\", },\n    {\"name\": \"mpt\",              \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/mosaicml/mpt-7b\", },\n    {\"name\": \"starcoder\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/bigcode/starcoder2-3b\", },\n    {\"name\": \"gpt-2\",            \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/openai-community/gpt2\", },\n    {\"name\": \"stablelm2\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\", },\n    {\"name\": \"refact\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/smallcloudai/Refact-1_6-base\", },\n    {\"name\": \"command-r\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/CohereForAI/c4ai-command-r-v01\", },\n    {\"name\": \"qwen2\",            \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/Qwen/Qwen1.5-7B\", },\n    {\"name\": \"olmo\",             \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/allenai/OLMo-1.7-7B-hf\", },\n    {\"name\": \"dbrx\",             \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/databricks/dbrx-base\", },\n    {\"name\": \"jina-v1-en\",       \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/jinaai/jina-reranker-v1-tiny-en\", },\n    {\"name\": \"jina-v2-en\",       \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/jinaai/jina-embeddings-v2-base-en\", }, # WPM!\n    {\"name\": \"jina-v2-es\",       \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/jinaai/jina-embeddings-v2-base-es\", },\n    {\"name\": \"jina-v2-de\",       \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/jinaai/jina-embeddings-v2-base-de\", },\n    {\"name\": \"smaug-bpe\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/abacusai/Smaug-Llama-3-70B-Instruct\", },\n    {\"name\": \"poro-chat\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/LumiOpen/Poro-34B-chat\", },\n    {\"name\": \"jina-v2-code\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/jinaai/jina-embeddings-v2-base-code\", },\n    {\"name\": \"viking\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/LumiOpen/Viking-7B\", }, # Also used for Viking 13B and 33B\n    {\"name\": \"gemma\",            \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/google/gemma-2b\", },\n    {\"name\": \"gemma-2\",          \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/google/gemma-2-9b\", },\n    {\"name\": \"jais\",             \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/core42/jais-13b\", },\n    {\"name\": \"t5\",               \"tokt\": TOKENIZER_TYPE.UGM, \"repo\": \"https://huggingface.co/google-t5/t5-small\", },\n    {\"name\": \"codeshell\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/WisdomShell/CodeShell-7B\", },\n    {\"name\": \"tekken\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/mistralai/Mistral-Nemo-Base-2407\", },\n    {\"name\": \"smollm\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/HuggingFaceTB/SmolLM-135M\", },\n    {'name': \"bloom\",            \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/bigscience/bloom\", },\n    {'name': \"gpt3-finnish\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/TurkuNLP/gpt3-finnish-small\", },\n    {\"name\": \"exaone\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\", },\n    {\"name\": \"phi-2\",            \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/microsoft/phi-2\", },\n    {\"name\": \"chameleon\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/facebook/chameleon-7b\", },\n    {\"name\": \"minerva-7b\",       \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/sapienzanlp/Minerva-7B-base-v1.0\", },\n    {\"name\": \"roberta-bpe\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/sentence-transformers/stsb-roberta-base\"},\n    {\"name\": \"gigachat\",         \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct\"},\n    {\"name\": \"megrez\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/Infinigence/Megrez-3B-Instruct\"},\n    {\"name\": \"deepseek-v3\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/DeepSeek-V3\"},\n    {\"name\": \"deepseek-r1-qwen\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"},\n]\n\n\ndef download_file_with_auth(url, token, save_path):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = sess.get(url, headers=headers)\n    response.raise_for_status()\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, 'wb') as downloaded_file:\n        downloaded_file.write(response.content)\n    logger.info(f\"File {save_path} downloaded successfully\")\n\n\ndef download_model(model):\n    name = model[\"name\"]\n    repo = model[\"repo\"]\n    tokt = model[\"tokt\"]\n\n    os.makedirs(f\"models/tokenizers/{name}\", exist_ok=True)\n\n    files = [\"config.json\", \"tokenizer.json\", \"tokenizer_config.json\"]\n\n    if tokt == TOKENIZER_TYPE.SPM:\n        files.append(\"tokenizer.model\")\n\n    if tokt == TOKENIZER_TYPE.UGM:\n        files.append(\"spiece.model\")\n\n    if os.path.isdir(repo):\n        # If repo is a path on the file system, copy the directory\n        for file in files:\n            src_path = os.path.join(repo, file)\n            dst_path = f\"models/tokenizers/{name}/{file}\"\n            if os.path.isfile(dst_path):\n                logger.info(f\"{name}: File {dst_path} already exists - skipping\")\n                continue\n            if os.path.isfile(src_path):\n                shutil.copy2(src_path, dst_path)\n                logger.info(f\"{name}: Copied {src_path} to {dst_path}\")\n            else:\n                logger.warning(f\"{name}: Source file {src_path} does not exist\")\n    else:\n        # If repo is a URL, download the files\n        for file in files:\n            save_path = f\"models/tokenizers/{name}/{file}\"\n            if os.path.isfile(save_path):\n                logger.info(f\"{name}: File {save_path} already exists - skipping\")\n                continue\n            download_file_with_auth(f\"{repo}/resolve/main/{file}\", token, save_path)\n\n\nfor model in models:\n    try:\n        download_model(model)\n    except Exception as e:\n        logger.error(f\"Failed to download model {model['name']}. Error: {e}\")\n\n\n# generate the source code for the convert_hf_to_gguf.py:get_vocab_base_pre() function:\n\nsrc_ifs = \"\"\nfor model in models:\n    name = model[\"name\"]\n    tokt = model[\"tokt\"]\n\n    if tokt == TOKENIZER_TYPE.SPM or tokt == TOKENIZER_TYPE.UGM:\n        continue\n\n    # Skip if the tokenizer folder does not exist or there are other download issues previously\n    if not os.path.exists(f\"models/tokenizers/{name}\"):\n        logger.warning(f\"Directory for tokenizer {name} not found. Skipping...\")\n        continue\n\n    # create the tokenizer\n    try:\n        if name == \"t5\":\n            tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\", use_fast=False)\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\")\n    except OSError as e:\n        logger.error(f\"Error loading tokenizer for model {name}. The model may not exist or is not accessible with the provided token. Error: {e}\")\n        continue  # Skip to the next model if the tokenizer can't be loaded\n\n    chktok = tokenizer.encode(CHK_TXT)\n    chkhsh = sha256(str(chktok).encode()).hexdigest()\n\n    logger.info(f\"model: {name}\")\n    logger.info(f\"tokt: {tokt}\")\n    logger.info(f\"repo: {model['repo']}\")\n    logger.info(f\"chktok: {chktok}\")\n    logger.info(f\"chkhsh: {chkhsh}\")\n\n    # print the \"pre_tokenizer\" content from the tokenizer.json\n    with open(f\"models/tokenizers/{name}/tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n        cfg = json.load(f)\n        normalizer = cfg[\"normalizer\"]\n        logger.info(\"normalizer: \" + json.dumps(normalizer, indent=4))\n        pre_tokenizer = cfg[\"pre_tokenizer\"]\n        logger.info(\"pre_tokenizer: \" + json.dumps(pre_tokenizer, indent=4))\n        if \"ignore_merges\" in cfg[\"model\"]:\n            logger.info(\"ignore_merges: \" + json.dumps(cfg[\"model\"][\"ignore_merges\"], indent=4))\n\n    logger.info(\"\")\n\n    src_ifs += f\"        if chkhsh == \\\"{chkhsh}\\\":\\n\"\n    src_ifs += f\"            # ref: {model['repo']}\\n\"\n    src_ifs += f\"            res = \\\"{name}\\\"\\n\"\n\nsrc_func = f\"\"\"\n    def get_vocab_base_pre(self, tokenizer) -> str:\n        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that\n        # is specific for the BPE pre-tokenizer used by the model\n        # we will use this unique identifier to write a \"tokenizer.ggml.pre\" entry in the GGUF file which we can\n        # use in llama.cpp to implement the same pre-tokenizer\n\n        chktxt = {repr(CHK_TXT)}\n\n        chktok = tokenizer.encode(chktxt)\n        chkhsh = sha256(str(chktok).encode()).hexdigest()\n\n        logger.debug(f\"chktok: {{chktok}}\")\n        logger.debug(f\"chkhsh: {{chkhsh}}\")\n\n        res = None\n\n        # NOTE: if you get an error here, you need to update the convert_hf_to_gguf_update.py script\n        #       or pull the latest version of the model from Huggingface\n        #       don't edit the hashes manually!\n{src_ifs}\n        if res is None:\n            logger.warning(\"\\\\n\")\n            logger.warning(\"**************************************************************************************\")\n            logger.warning(\"** WARNING: The BPE pre-tokenizer was not recognized!\")\n            logger.warning(\"**          There are 2 possible reasons for this:\")\n            logger.warning(\"**          - the model has not been added to convert_hf_to_gguf_update.py yet\")\n            logger.warning(\"**          - the pre-tokenization config has changed upstream\")\n            logger.warning(\"**          Check your model files and convert_hf_to_gguf_update.py and update them accordingly.\")\n            logger.warning(\"** ref:     https://github.com/ggerganov/llama.cpp/pull/6920\")\n            logger.warning(\"**\")\n            logger.warning(f\"** chkhsh:  {{chkhsh}}\")\n            logger.warning(\"**************************************************************************************\")\n            logger.warning(\"\\\\n\")\n            raise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n\n        logger.debug(f\"tokenizer.ggml.pre: {{repr(res)}}\")\n        logger.debug(f\"chkhsh: {{chkhsh}}\")\n\n        return res\n\"\"\"\n\nconvert_py_pth = pathlib.Path(\"convert_hf_to_gguf.py\")\nconvert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nconvert_py = re.sub(\n    r\"(# Marker: Start get_vocab_base_pre)(.+?)( +# Marker: End get_vocab_base_pre)\",\n    lambda m: m.group(1) + src_func + m.group(3),\n    convert_py,\n    flags=re.DOTALL | re.MULTILINE,\n)\n\nconvert_py_pth.write_text(convert_py, encoding=\"utf-8\")\n\nlogger.info(\"+++ convert_hf_to_gguf.py was updated\")\n\n# generate tests for each tokenizer model\n\ntests = [\n    \"ied 4 \u00bd months\",\n    \"F\u00fchrer\",\n    \"\",\n    \" \",\n    \"  \",\n    \"   \",\n    \"\\t\",\n    \"\\n\",\n    \"\\n\\n\",\n    \"\\n\\n\\n\",\n    \"\\t\\n\",\n    \"Hello world\",\n    \" Hello world\",\n    \"Hello World\",\n    \" Hello World\",\n    \" Hello World!\",\n    \"Hello, world!\",\n    \" Hello, world!\",\n    \" this is \ud83e\udd99.cpp\",\n    \"w048 7tuijk dsdfhu\",\n    \"\u043d\u0435\u0449\u043e \u043d\u0430 \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\",\n    \"\u1780\u17b6\u1793\u17cb\u178f\u17c2\u1796\u17b7\u179f\u17c1\u179f\u17a2\u17b6\u1785\u1781\u179b\u1785\u17c1\u1789\",\n    \"\ud83d\ude80 (normal) \ud83d\ude36\u200d\ud83c\udf2b\ufe0f (multiple emojis concatenated) \u2705 (only emoji that has its own token)\",\n    \"Hello\",\n    \" Hello\",\n    \"  Hello\",\n    \"   Hello\",\n    \"    Hello\",\n    \"    Hello\\n    Hello\",\n    \" (\",\n    \"\\n =\",\n    \"' era\",\n    \"Hello, y'all! How are you \ud83d\ude01 ?\u6211\u60f3\u5728apple\u5de5\u4f5c1314151\u5929\uff5e\",\n    \"!!!!!!\",\n    \"3\",\n    \"33\",\n    \"333\",\n    \"3333\",\n    \"33333\",\n    \"333333\",\n    \"3333333\",\n    \"33333333\",\n    \"333333333\",\n    \"C\u1eeda Vi\u1ec7t\", # llama-bpe fails on this\n    \" discards\",\n    CHK_TXT,\n]\n\n# write the tests to ./models/ggml-vocab-{name}.gguf.inp\n# the format is:\n#\n# test0\n# __ggml_vocab_test__\n# test1\n# __ggml_vocab_test__\n# ...\n#\n\n# with each model, encode all tests and write the results in ./models/ggml-vocab-{name}.gguf.out\n# for each test, write the resulting tokens on a separate line\n\nfor model in models:\n    name = model[\"name\"]\n    tokt = model[\"tokt\"]\n\n    # Skip if the tokenizer folder does not exist or there are other download issues previously\n    if not os.path.exists(f\"models/tokenizers/{name}\"):\n        logger.warning(f\"Directory for tokenizer {name} not found. Skipping...\")\n        continue\n\n    # create the tokenizer\n    try:\n        if name == \"t5\":\n            tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\", use_fast=False)\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\")\n    except OSError as e:\n        logger.error(f\"Failed to load tokenizer for model {name}. Error: {e}\")\n        continue  # Skip this model and continue with the next one in the loop\n\n    with open(f\"models/ggml-vocab-{name}.gguf.inp\", \"w\", encoding=\"utf-8\") as f:\n        for text in tests:\n            f.write(f\"{text}\")\n            f.write(\"\\n__ggml_vocab_test__\\n\")\n\n    with open(f\"models/ggml-vocab-{name}.gguf.out\", \"w\") as f:\n        for text in tests:\n            res = tokenizer.encode(text, add_special_tokens=False)\n            for r in res:\n                f.write(f\" {r}\")\n            f.write(\"\\n\")\n\n    logger.info(f\"Tests for {name} written in ./models/ggml-vocab-{name}.gguf.*\")\n\n# generate commands for creating vocab files\n\nlogger.info(\"\\nRun the following commands to generate the vocab files for testing:\\n\")\n\nfor model in models:\n    name = model[\"name\"]\n\n    print(f\"python3 convert_hf_to_gguf.py models/tokenizers/{name}/ --outfile models/ggml-vocab-{name}.gguf --vocab-only\") # noqa: NP100\n\nlogger.info(\"\\n\")\n",
        "imports": [
            "requests",
            "Huggingface",
            "the",
            "enum",
            "sys",
            "json",
            "logging",
            "re",
            "hashlib",
            "os",
            "sha256",
            "transformers",
            "IntEnum",
            "pathlib",
            "shutil",
            "AutoTokenizer"
        ],
        "functions": [
            "download_model",
            "download_file_with_auth",
            "get_vocab_base_pre"
        ],
        "variables": [
            "src_func",
            "normalizer",
            "chktok",
            "logger",
            "SPM",
            "n",
            "src_ifs",
            "exist_ok",
            "models",
            "tokt",
            "headers",
            "res",
            "name",
            "UGM",
            "BPE",
            "src_path",
            "repo",
            "convert_py",
            "CHK_TXT",
            "pre_tokenizer",
            "WPM",
            "indent",
            "files",
            "level",
            "tests",
            "save_path",
            "chkhsh",
            "flags",
            "token",
            "chktxt",
            "dst_path",
            "convert_py_pth",
            "tokenizer",
            "response",
            "encoding",
            "cfg",
            "sess"
        ]
    },
    {
        "file_name": "convert_llama_ggml_to_gguf.py",
        "language": "python",
        "source_code": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\nimport logging\nimport argparse\nimport os\nimport struct\nimport sys\nfrom enum import IntEnum\nfrom pathlib import Path\n\nimport numpy as np\n\nif 'NO_LOCAL_GGUF' not in os.environ:\n    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\nimport gguf\n\nlogger = logging.getLogger(\"ggml-to-gguf\")\n\n\nclass GGMLFormat(IntEnum):\n    GGML = 0\n    GGMF = 1\n    GGJT = 2\n\n\nclass GGMLFType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1\n    MOSTLY_Q4_0          = 2\n    MOSTLY_Q4_1          = 3\n    MOSTLY_Q4_1_SOME_F16 = 4\n    MOSTLY_Q8_0          = 7\n    MOSTLY_Q5_0          = 8\n    MOSTLY_Q5_1          = 9\n    MOSTLY_Q2_K          = 10\n    MOSTLY_Q3_K_S        = 11\n    MOSTLY_Q3_K_M        = 12\n    MOSTLY_Q3_K_L        = 13\n    MOSTLY_Q4_K_S        = 14\n    MOSTLY_Q4_K_M        = 15\n    MOSTLY_Q5_K_S        = 16\n    MOSTLY_Q5_K_M        = 17\n    MOSTLY_Q6_K          = 18\n\n\nclass Hyperparameters:\n    def __init__(self):\n        self.n_vocab = self.n_embd = self.n_mult = self.n_head = 0\n        self.n_layer = self.n_rot = self.n_ff = 0\n        self.ftype = GGMLFType.ALL_F32\n\n    def set_n_ff(self, model):\n        ff_tensor_idx = model.tensor_map.get(b'layers.0.feed_forward.w1.weight')\n        assert ff_tensor_idx is not None, 'Missing layer 0 FF tensor'\n        ff_tensor = model.tensors[ff_tensor_idx]\n        self.n_ff = ff_tensor.dims[1]\n\n    def load(self, data, offset):\n        (\n            self.n_vocab,\n            self.n_embd,\n            self.n_mult,\n            self.n_head,\n            self.n_layer,\n            self.n_rot,\n            ftype,\n        ) = struct.unpack('<7I', data[offset:offset + (4 * 7)])\n        try:\n            self.ftype = GGMLFType(ftype)\n        except ValueError:\n            raise ValueError(f'Invalid ftype {ftype}')\n        return 4 * 7\n\n    def __str__(self):\n        return f'<Hyperparameters: n_vocab={self.n_vocab}, n_embd={self.n_embd}, n_mult={self.n_mult}, n_head={self.n_head}, n_layer={self.n_layer}, n_rot={self.n_rot}, n_ff={self.n_ff}, ftype={self.ftype.name}>'\n\n\nclass Vocab:\n    def __init__(self, load_scores = True):\n        self.items = []\n        self.load_scores = load_scores\n\n    def load(self, data, offset, n_vocab):\n        orig_offset = offset\n        for _ in range(n_vocab):\n            itemlen = struct.unpack('<I', data[offset:offset + 4])[0]\n            assert itemlen < 4096, 'Absurd vocab item length'\n            offset += 4\n            item_text = bytes(data[offset:offset + itemlen])\n            offset += itemlen\n            if self.load_scores:\n                item_score = struct.unpack('<f', data[offset:offset + 4])[0]\n                offset += 4\n            else:\n                item_score = 0.0\n            self.items.append((item_text, item_score))\n        return offset - orig_offset\n\n\nclass Tensor:\n    def __init__(self, use_padding = True):\n        self.name = None\n        self.dims: tuple[int, ...] = ()\n        self.dtype = None\n        self.start_offset = 0\n        self.len_bytes = np.int64(0)\n        self.use_padding = use_padding\n\n    def load(self, data, offset):\n        orig_offset = offset\n        (n_dims, name_len, dtype) = struct.unpack('<3I', data[offset:offset + 12])\n        assert n_dims >= 0 and n_dims <= 4, f'Invalid tensor dimensions {n_dims}'\n        assert name_len < 4096, 'Absurd tensor name length'\n        quant = gguf.GGML_QUANT_SIZES.get(dtype)\n        assert quant is not None, 'Unknown tensor type'\n        (blksize, tysize) = quant\n        offset += 12\n        self.dtype= gguf.GGMLQuantizationType(dtype)\n        self.dims = struct.unpack(f'<{n_dims}I', data[offset:offset + (4 * n_dims)])\n        offset += 4 * n_dims\n        self.name = bytes(data[offset:offset + name_len])\n        offset += name_len\n        pad = ((offset + 31) & ~31) - offset if self.use_padding else 0\n        offset += pad\n        n_elems = np.prod(self.dims)\n        n_bytes = np.int64(np.int64(n_elems) * np.int64(tysize)) // np.int64(blksize)\n        self.start_offset = offset\n        self.len_bytes = n_bytes\n        offset += n_bytes\n        return offset - orig_offset\n\n\nclass GGMLModel:\n\n    file_format: GGMLFormat\n    format_version: int\n\n    def __init__(self):\n        self.hyperparameters = None\n        self.vocab = None\n        self.tensor_map = {}\n        self.tensors = []\n\n    def validate_header(self, data, offset):\n        magic = bytes(data[offset:offset + 4])\n        if magic == b'GGUF':\n            raise ValueError('File is already in GGUF format.')\n        if magic == b'lmgg':\n            self.file_format = GGMLFormat.GGML\n            self.format_version = 1\n            return 4\n        version = struct.unpack('<I', data[offset + 4:offset + 8])[0]\n        if magic == b'fmgg':\n            if version != 1:\n                raise ValueError(f'Cannot handle unexpected GGMF file version {version}')\n            self.file_format = GGMLFormat.GGMF\n            self.format_version = version\n            return 8\n        if magic == b'tjgg':\n            if version < 1 or version > 3:\n                raise ValueError(f'Cannot handle unexpected GGJT file version {version}')\n            self.file_format = GGMLFormat.GGJT\n            self.format_version = version\n            return 8\n        raise ValueError(f\"Unexpected file magic {magic!r}! This doesn't look like a GGML format file.\")\n\n    def validate_conversion(self, ftype):\n        err = ''\n        if (self.file_format < GGMLFormat.GGJT or self.format_version < 2):\n            if ftype not in (GGMLFType.ALL_F32, GGMLFType.MOSTLY_F16):\n                err = 'Quantizations changed in GGJTv2. Can only convert unquantized GGML files older than GGJTv2.'\n        elif (self.file_format == GGMLFormat.GGJT and self.format_version == 2):\n            if ftype in (GGMLFType.MOSTLY_Q4_0, GGMLFType.MOSTLY_Q4_1,\n                         GGMLFType.MOSTLY_Q4_1_SOME_F16, GGMLFType.MOSTLY_Q8_0):\n                err = 'Q4 and Q8 quantizations changed in GGJTv3.'\n        if len(err) > 0:\n            raise ValueError(f'{err} Sorry, your {self.file_format.name}v{self.format_version} file of type {ftype.name} is not eligible for conversion.')\n\n    def load(self, data, offset):\n        offset += self.validate_header(data, offset)\n        hp = Hyperparameters()\n        offset += hp.load(data, offset)\n        logger.info(f'* File format: {self.file_format.name}v{self.format_version} with ftype {hp.ftype.name}')\n        self.validate_conversion(hp.ftype)\n        vocab = Vocab(load_scores = self.file_format > GGMLFormat.GGML)\n        offset += vocab.load(data, offset, hp.n_vocab)\n        tensors: list[Tensor] = []\n        tensor_map = {}\n        while offset < len(data):\n            tensor = Tensor(use_padding = self.file_format > GGMLFormat.GGMF)\n            offset += tensor.load(data, offset)\n            tensor_map[tensor.name] = len(tensors)\n            tensors.append(tensor)\n        self.hyperparameters = hp\n        self.vocab = vocab\n        self.tensors = tensors\n        self.tensor_map = tensor_map\n        hp.set_n_ff(self)\n        return offset\n\n\nclass GGMLToGGUF:\n    def __init__(self, ggml_model, data, cfg, params_override = None, vocab_override = None, special_vocab = None):\n        hp = ggml_model.hyperparameters\n        self.model = ggml_model\n        self.data = data\n        self.cfg = cfg\n        self.params_override = params_override\n        self.vocab_override = vocab_override\n        self.special_vocab = special_vocab\n        if params_override is not None:\n            n_kv_head = params_override.n_head_kv\n        else:\n            if cfg.gqa == 1:\n                n_kv_head = hp.n_head\n            else:\n                gqa = float(cfg.gqa)\n                n_kv_head = None\n                for x in range(1, 256):\n                    if float(hp.n_head) / float(x) == gqa:\n                        n_kv_head = x\n                assert n_kv_head is not None, \"Couldn't determine n_kv_head from GQA param\"\n                logger.info(f'- Guessed n_kv_head = {n_kv_head} based on GQA {cfg.gqa}')\n        self.n_kv_head = n_kv_head\n        self.name_map = gguf.get_tensor_name_map(gguf.MODEL_ARCH.LLAMA, ggml_model.hyperparameters.n_layer)\n\n    def save(self):\n        logger.info('* Preparing to save GGUF file')\n        gguf_writer = gguf.GGUFWriter(\n            self.cfg.output,\n            gguf.MODEL_ARCH_NAMES[gguf.MODEL_ARCH.LLAMA],\n            use_temp_file = False)\n        self.add_params(gguf_writer)\n        self.add_vocab(gguf_writer)\n        if self.special_vocab is not None:\n            self.special_vocab.add_to_gguf(gguf_writer)\n        self.add_tensors(gguf_writer)\n        logger.info(\"    gguf: write header\")\n        gguf_writer.write_header_to_file()\n        logger.info(\"    gguf: write metadata\")\n        gguf_writer.write_kv_data_to_file()\n        logger.info(\"    gguf: write tensors\")\n        gguf_writer.write_tensors_to_file()\n        gguf_writer.close()\n\n    def add_params(self, gguf_writer):\n        hp = self.model.hyperparameters\n        cfg = self.cfg\n        if cfg.desc is not None:\n            desc = cfg.desc\n        else:\n            desc = f'converted from legacy {self.model.file_format.name}v{self.model.format_version} {hp.ftype.name} format'\n        try:\n            # Filenames aren't necessarily valid UTF8.\n            name = cfg.name if cfg.name is not None else cfg.input.name\n        except UnicodeDecodeError:\n            name = None\n        logger.info('* Adding model parameters and KV items')\n        if name is not None:\n            gguf_writer.add_name(name)\n        gguf_writer.add_description(desc)\n        gguf_writer.add_file_type(int(hp.ftype))\n        if self.params_override is not None:\n            po = self.params_override\n            assert po.n_embd == hp.n_embd, 'Model hyperparams mismatch'\n            assert po.n_layer == hp.n_layer, 'Model hyperparams mismatch'\n            assert po.n_head == hp.n_head, 'Model hyperparams mismatch'\n            gguf_writer.add_context_length      (po.n_ctx)\n            gguf_writer.add_embedding_length    (po.n_embd)\n            gguf_writer.add_block_count         (po.n_layer)\n            gguf_writer.add_feed_forward_length (po.n_ff)\n            gguf_writer.add_rope_dimension_count(po.n_embd // po.n_head)\n            gguf_writer.add_head_count          (po.n_head)\n            gguf_writer.add_head_count_kv       (po.n_head_kv)\n            gguf_writer.add_layer_norm_rms_eps  (po.f_norm_eps)\n            return\n        gguf_writer.add_context_length(cfg.context_length)\n        gguf_writer.add_embedding_length(hp.n_embd)\n        gguf_writer.add_block_count(hp.n_layer)\n        gguf_writer.add_feed_forward_length(hp.n_ff)\n        gguf_writer.add_rope_dimension_count(hp.n_embd // hp.n_head)\n        gguf_writer.add_head_count(hp.n_head)\n        gguf_writer.add_head_count_kv(self.n_kv_head)\n        gguf_writer.add_layer_norm_rms_eps(float(cfg.eps))\n\n    def add_vocab(self, gguf_writer):\n        hp = self.model.hyperparameters\n        gguf_writer.add_tokenizer_model('llama')\n        gguf_writer.add_tokenizer_pre('default')\n        tokens = []\n        scores = []\n        toktypes = []\n        if self.vocab_override is not None:\n            vo = self.vocab_override\n            logger.info('* Adding vocab item(s)')\n            for (_, (vbytes, score, ttype)) in enumerate(vo.all_tokens()):\n                tokens.append(vbytes)\n                scores.append(score)\n                toktypes.append(ttype)\n            assert len(tokens) == hp.n_vocab, \\\n                f'Override vocab has a different number of items than hyperparameters - override = {len(tokens)} but n_vocab={hp.n_vocab}'\n            gguf_writer.add_token_list(tokens)\n            gguf_writer.add_token_scores(scores)\n            if len(toktypes) > 0:\n                gguf_writer.add_token_types(toktypes)\n            return\n        logger.info(f'* Adding {hp.n_vocab} vocab item(s)')\n        assert len(self.model.vocab.items) >= 3, 'Cannot handle unexpectedly short model vocab'\n        for (tokid, (vbytes, vscore)) in enumerate(self.model.vocab.items):\n            tt = 1 # Normal\n            # Special handling for UNK, BOS, EOS tokens.\n            if tokid <= 2:\n                if tokid == 0:\n                    vbytes = b'<unk>'\n                    tt = 2\n                elif tokid == 1:\n                    vbytes = b'<s>'\n                    tt = 3\n                else:\n                    vbytes = b'</s>'\n                    tt = 3\n            elif len(vbytes) == 0:\n                tt = 3 # Control\n            elif tokid >= 3 and tokid <= 258 and len(vbytes) == 1:\n                vbytes = bytes(f'<0x{vbytes[0]:02X}>', encoding = 'UTF-8')\n                tt = 6 # Byte\n            else:\n                vbytes = vbytes.replace(b' ', b'\\xe2\\x96\\x81')\n            toktypes.append(tt)\n            tokens.append(vbytes)\n            scores.append(vscore)\n        gguf_writer.add_token_list(tokens)\n        gguf_writer.add_token_scores(scores)\n        gguf_writer.add_token_types(toktypes)\n        gguf_writer.add_unk_token_id(0)\n        gguf_writer.add_bos_token_id(1)\n        gguf_writer.add_eos_token_id(2)\n\n    def add_tensors(self, gguf_writer):\n        tensor_map = self.name_map\n        data = self.data\n        logger.info(f'* Adding {len(self.model.tensors)} tensor(s)')\n        for tensor in self.model.tensors:\n            name = str(tensor.name, 'UTF-8')\n            mapped_name = tensor_map.get_name(name, try_suffixes = (\".weight\", \".bias\"))\n            assert mapped_name is not None, f'Bad name {name}'\n            tempdims = list(tensor.dims[:])\n            if len(tempdims) > 1:\n                temp = tempdims[1]\n                tempdims[1] = tempdims[0]\n                tempdims[0] = temp\n            gguf_writer.add_tensor(\n                mapped_name,\n                data[tensor.start_offset:tensor.start_offset + tensor.len_bytes],\n                raw_shape = tempdims,\n                raw_dtype = tensor.dtype)\n\n\ndef handle_metadata(cfg, hp):\n    import examples.convert_legacy_llama as convert\n\n    assert cfg.model_metadata_dir.is_dir(), 'Metadata dir is not a directory'\n    hf_config_path   = cfg.model_metadata_dir / \"config.json\"\n    orig_config_path = cfg.model_metadata_dir / \"params.json\"\n    # We pass a fake model here. \"original\" mode will check the shapes of some\n    # tensors if information is missing in the .json file: other than that, the\n    # model data isn't used so this should be safe (at least for now).\n    fakemodel = {\n        'tok_embeddings.weight': convert.LazyTensor.__new__(convert.LazyTensor),\n        'layers.0.feed_forward.w1.weight': convert.LazyTensor.__new__(convert.LazyTensor),\n    }\n    fakemodel['tok_embeddings.weight'].shape = [hp.n_vocab]\n    fakemodel['layers.0.feed_forward.w1.weight'].shape = [hp.n_ff]\n    if hf_config_path.exists():\n        params = convert.Params.loadHFTransformerJson(fakemodel, hf_config_path)\n    elif orig_config_path.exists():\n        params = convert.Params.loadOriginalParamsJson(fakemodel, orig_config_path)\n    else:\n        raise ValueError('Unable to load metadata')\n    vocab_path = Path(cfg.vocab_dir if cfg.vocab_dir is not None else cfg.model_metadata_dir)\n    vocab_factory = convert.VocabFactory(vocab_path)\n    vocab, special_vocab = vocab_factory.load_vocab(cfg.vocabtype.split(\",\"), cfg.model_metadata_dir)\n    convert.check_vocab_size(params, vocab)\n    return params, vocab, special_vocab\n\n\ndef handle_args():\n    parser = argparse.ArgumentParser(description = 'Convert GGML models to GGUF')\n    parser.add_argument('--input', '-i', type = Path, required = True,\n                        help = 'Input GGMLv3 filename')\n    parser.add_argument('--output', '-o', type = Path, required = True,\n                        help ='Output GGUF filename')\n    parser.add_argument('--name',\n                        help = 'Set model name')\n    parser.add_argument('--desc',\n                        help = 'Set model description')\n    parser.add_argument('--gqa', type = int, default = 1,\n                        help = 'grouped-query attention factor (use 8 for LLaMA2 70B)')\n    parser.add_argument('--eps', default = '5.0e-06',\n                        help = 'RMS norm eps: Use 1e-6 for LLaMA1 and OpenLLaMA, use 1e-5 for LLaMA2')\n    parser.add_argument('--context-length', '-c', type=int, default = 2048,\n                        help = 'Default max context length: LLaMA1 is typically 2048, LLaMA2 is typically 4096')\n    parser.add_argument('--model-metadata-dir', '-m', type = Path,\n                        help ='Load HuggingFace/.pth vocab and metadata from the specified directory')\n    parser.add_argument(\"--vocab-dir\", type=Path,\n                        help=\"directory containing tokenizer.model, if separate from model file - only meaningful with --model-metadata-dir\")\n    parser.add_argument(\"--vocabtype\", default=\"spm,hfft\",\n                        help=\"vocab format - only meaningful with --model-metadata-dir and/or --vocab-dir (default: spm,hfft)\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"increase output verbosity\")\n    return parser.parse_args()\n\n\ndef main():\n    cfg = handle_args()\n    logging.basicConfig(level=logging.DEBUG if cfg.verbose else logging.INFO)\n    logger.info(f'* Using config: {cfg}')\n    logger.warning('=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===')\n    if cfg.model_metadata_dir is None and (cfg.gqa == 1 or cfg.eps == '5.0e-06'):\n        logger.info('- Note: If converting LLaMA2, specifying \"--eps 1e-5\" is required. 70B models also need \"--gqa 8\".')\n    data = np.memmap(cfg.input, mode = 'r')\n    model = GGMLModel()\n    logger.info('* Scanning GGML input file')\n    offset = model.load(data, 0)  # noqa\n    logger.info(f'* GGML model hyperparameters: {model.hyperparameters}')\n    vocab_override = None\n    params_override = None\n    special_vocab = None\n    if cfg.model_metadata_dir is not None:\n        (params_override, vocab_override, special_vocab) = handle_metadata(cfg, model.hyperparameters)\n        logger.info('!! Note: When overriding params the --gqa, --eps and --context-length options are ignored.')\n        logger.info(f'* Overriding params: {params_override}')\n        logger.info(f'* Overriding vocab: {vocab_override}')\n        logger.info(f'* Special vocab: {special_vocab}')\n    else:\n        logger.warning('\\n=== WARNING === Special tokens may not be converted correctly. Use --model-metadata-dir if possible === WARNING ===\\n')\n        if model.file_format == GGMLFormat.GGML:\n            logger.info('! This is a very old GGML file that does not contain vocab scores. Strongly recommend using model metadata!')\n    converter = GGMLToGGUF(\n        model, data, cfg,\n        params_override = params_override,\n        vocab_override = vocab_override,\n        special_vocab = special_vocab\n    )\n    converter.save()\n    logger.info(f'* Successful completion. Output saved to: {cfg.output}')\n\n\nif __name__ == '__main__':\n    main()\n",
        "imports": [
            "examples.convert_legacy_llama",
            "numpy",
            "__future__",
            "struct",
            "model",
            "the",
            "enum",
            "sys",
            "GQA",
            "logging",
            "pathlib",
            "os",
            "annotations",
            "IntEnum",
            "Path",
            "gguf",
            "argparse",
            "legacy"
        ],
        "functions": [
            "__str__",
            "handle_args",
            "add_params",
            "handle_metadata",
            "validate_header",
            "add_tensors",
            "save",
            "validate_conversion",
            "load",
            "add_vocab",
            "set_n_ff",
            "main",
            "__init__"
        ],
        "variables": [
            "ALL_F32",
            "MOSTLY_Q8_0",
            "help",
            "file_format",
            "MOSTLY_Q3_K_S",
            "magic",
            "tt",
            "fakemodel",
            "GGMF",
            "data",
            "MOSTLY_Q2_K",
            "parser",
            "converter",
            "err",
            "tensors",
            "pad",
            "tokid",
            "vocab_override",
            "MOSTLY_Q5_0",
            "toktypes",
            "n_embd",
            "tempdims",
            "WARNING",
            "MOSTLY_Q6_K",
            "vocab_factory",
            "hyperparameters",
            "start_offset",
            "type",
            "items",
            "len_bytes",
            "default",
            "orig_offset",
            "n_vocab",
            "logger",
            "ftype",
            "hf_config_path",
            "n_elems",
            "n_layer",
            "vocab",
            "quant",
            "action",
            "version",
            "ff_tensor_idx",
            "GGML",
            "raw_shape",
            "MOSTLY_Q5_K_S",
            "dtype",
            "n_ff",
            "params",
            "use_padding",
            "use_temp_file",
            "item_text",
            "ff_tensor",
            "mapped_name",
            "MOSTLY_Q4_K_M",
            "n",
            "item_score",
            "name",
            "special_vocab",
            "MOSTLY_Q4_0",
            "MOSTLY_Q3_K_L",
            "desc",
            "offset",
            "n_bytes",
            "format_version",
            "vocab_path",
            "GGJT",
            "tensor_map",
            "dims",
            "gguf_writer",
            "vbytes",
            "override",
            "MOSTLY_Q4_1_SOME_F16",
            "MOSTLY_Q4_1",
            "temp",
            "tensor",
            "MOSTLY_F16",
            "params_override",
            "MOSTLY_Q5_1",
            "scores",
            "MOSTLY_Q4_K_S",
            "cfg",
            "load_scores",
            "itemlen",
            "name_map",
            "n_kv_head",
            "n_head",
            "__name__",
            "vo",
            "MOSTLY_Q3_K_M",
            "po",
            "tokens",
            "level",
            "MOSTLY_Q5_K_M",
            "shape",
            "model",
            "hp",
            "gqa",
            "orig_config_path",
            "raw_dtype"
        ]
    },
    {
        "file_name": "convert_lora_to_gguf.py",
        "language": "python",
        "source_code": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nimport logging\nimport argparse\nimport os\nimport sys\nimport json\nfrom math import prod\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, Sequence, SupportsIndex, cast\nfrom transformers import AutoConfig\n\nimport torch\n\nif TYPE_CHECKING:\n    from torch import Tensor\n\nif 'NO_LOCAL_GGUF' not in os.environ:\n    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\nimport gguf\n\n# reuse model definitions from convert_hf_to_gguf.py\nfrom convert_hf_to_gguf import LazyTorchTensor, Model\n\nlogger = logging.getLogger(\"lora-to-gguf\")\n\n\n@dataclass\nclass PartialLoraTensor:\n    A: Tensor | None = None\n    B: Tensor | None = None\n\n\n# magic to support tensor shape modifications and splitting\nclass LoraTorchTensor:\n    _lora_A: Tensor  # (n_rank, row_size)\n    _lora_B: Tensor  # (col_size, n_rank)\n    _rank: int\n\n    def __init__(self, A: Tensor, B: Tensor):\n        assert len(A.shape) == len(B.shape)\n        assert A.shape[-2] == B.shape[-1]\n        if A.dtype != B.dtype:\n            A = A.to(torch.float32)\n            B = B.to(torch.float32)\n        self._lora_A = A\n        self._lora_B = B\n        self._rank = B.shape[-1]\n\n    def get_lora_A_B(self) -> tuple[Tensor, Tensor]:\n        return (self._lora_A, self._lora_B)\n\n    def __getitem__(\n        self,\n        indices: (\n            SupportsIndex\n            | slice\n            | tuple[SupportsIndex | slice | Tensor, ...]  # TODO: add ellipsis in the type signature\n        ),\n    ) -> LoraTorchTensor:\n        shape = self.shape\n        if isinstance(indices, SupportsIndex):\n            if len(shape) > 2:\n                return LoraTorchTensor(self._lora_A[indices], self._lora_B[indices])\n            else:\n                raise NotImplementedError  # can't return a vector\n        elif isinstance(indices, slice):\n            if len(shape) > 2:\n                return LoraTorchTensor(self._lora_A[indices], self._lora_B[indices])\n            else:\n                return LoraTorchTensor(self._lora_A, self._lora_B[indices])\n        elif isinstance(indices, tuple):\n            assert len(indices) > 0\n            if indices[-1] is Ellipsis:\n                return self[indices[:-1]]\n            # expand ellipsis\n            indices = tuple(\n                u\n                for v in (\n                    (\n                        (slice(None, None) for _ in range(len(indices) - 1))\n                        if i is Ellipsis\n                        else (i,)\n                    )\n                    for i in indices\n                )\n                for u in v\n            )\n\n            if len(indices) < len(shape):\n                indices = (*indices, *(slice(None, None) for _ in range(len(indices), len(shape))))\n\n            # TODO: make sure this is correct\n            indices_A = (\n                *(\n                    (\n                        j.__index__() % self._lora_A.shape[i]\n                        if isinstance(j, SupportsIndex)\n                        else slice(None, None)\n                    )\n                    for i, j in enumerate(indices[:-2])\n                ),\n                slice(None, None),\n                indices[-1],\n            )\n            indices_B = indices[:-1]\n            return LoraTorchTensor(self._lora_A[indices_A], self._lora_B[indices_B])\n        else:\n            raise NotImplementedError  # unknown indice type\n\n    @property\n    def dtype(self) -> torch.dtype:\n        assert self._lora_A.dtype == self._lora_B.dtype\n        return self._lora_A.dtype\n\n    @property\n    def shape(self) -> tuple[int, ...]:\n        assert len(self._lora_A.shape) == len(self._lora_B.shape)\n        return (*self._lora_B.shape[:-1], self._lora_A.shape[-1])\n\n    def size(self, dim=None):\n        assert dim is None\n        return self.shape\n\n    def reshape(self, *shape: int | tuple[int, ...]) -> LoraTorchTensor:\n        if isinstance(shape[0], tuple):\n            new_shape: tuple[int, ...] = shape[0]\n        else:\n            new_shape = cast(tuple[int, ...], shape)\n        orig_shape = self.shape\n        if len(new_shape) < 2:\n            raise NotImplementedError  # can't become a vector\n\n        # expand -1 in the shape\n        if any(dim == -1 for dim in new_shape):\n            n_elems = prod(orig_shape)\n            n_new_elems = prod(dim if dim != -1 else 1 for dim in new_shape)\n            assert n_elems % n_new_elems == 0\n            new_shape = (*(dim if dim != -1 else n_elems // n_new_elems for dim in new_shape),)\n\n        if new_shape[-1] != orig_shape[-1]:\n            raise NotImplementedError  # can't reshape the row size trivially\n\n        shape_A = (*(1 for _ in new_shape[:-2]), self._rank, orig_shape[-1])\n        shape_B = (*new_shape[:-1], self._rank)\n        return LoraTorchTensor(\n            self._lora_A.reshape(shape_A),\n            self._lora_B.reshape(shape_B),\n        )\n\n    def reshape_as(self, other: Tensor) -> LoraTorchTensor:\n        return self.reshape(*other.shape)\n\n    def view(self, *size: int) -> LoraTorchTensor:\n        return self.reshape(*size)\n\n    def permute(self, *dims: int) -> LoraTorchTensor:\n        shape = self.shape\n        dims = tuple(dim - len(shape) if dim >= 0 else dim for dim in dims)\n        if dims[-1] == -1:\n            # TODO: support higher dimensional A shapes bigger than 1\n            assert all(dim == 1 for dim in self._lora_A.shape[:-2])\n            return LoraTorchTensor(self._lora_A, self._lora_B.permute(*dims))\n        if len(shape) == 2 and dims[-1] == -2 and dims[-2] == -1:\n            return LoraTorchTensor(self._lora_B.permute(*dims), self._lora_A.permute(*dims))\n        else:\n            # TODO: compose the above two\n            raise NotImplementedError\n\n    def transpose(self, dim0: int, dim1: int) -> LoraTorchTensor:\n        shape = self.shape\n        dims = [i for i in range(len(shape))]\n        dims[dim0], dims[dim1] = dims[dim1], dims[dim0]\n        return self.permute(*dims)\n\n    def swapaxes(self, axis0: int, axis1: int) -> LoraTorchTensor:\n        return self.transpose(axis0, axis1)\n\n    def to(self, *args, **kwargs):\n        return LoraTorchTensor(self._lora_A.to(*args, **kwargs), self._lora_B.to(*args, **kwargs))\n\n    @classmethod\n    def __torch_function__(cls, func: Callable, types, args=(), kwargs=None):\n        del types  # unused\n\n        if kwargs is None:\n            kwargs = {}\n\n        if func is torch.permute:\n            return type(args[0]).permute(*args, **kwargs)\n        elif func is torch.reshape:\n            return type(args[0]).reshape(*args, **kwargs)\n        elif func is torch.stack:\n            assert isinstance(args[0], Sequence)\n            dim = kwargs.get(\"dim\", 0)\n            assert dim == 0\n            return LoraTorchTensor(\n                torch.stack([a._lora_A for a in args[0]], dim),\n                torch.stack([b._lora_B for b in args[0]], dim),\n            )\n        elif func is torch.cat:\n            assert isinstance(args[0], Sequence)\n            dim = kwargs.get(\"dim\", 0)\n            assert dim == 0\n            if len(args[0][0].shape) > 2:\n                return LoraTorchTensor(\n                    torch.cat([a._lora_A for a in args[0]], dim),\n                    torch.cat([b._lora_B for b in args[0]], dim),\n                )\n            elif all(torch.equal(args[0][0]._lora_A, t._lora_A) for t in args[0][1:]):\n                return LoraTorchTensor(\n                    args[0][0]._lora_A,\n                    torch.cat([b._lora_B for b in args[0]], dim),\n                )\n            else:\n                raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n\ndef get_base_tensor_name(lora_tensor_name: str) -> str:\n    base_name = lora_tensor_name.replace(\"base_model.model.\", \"\")\n    base_name = base_name.replace(\".lora_A.weight\", \".weight\")\n    base_name = base_name.replace(\".lora_B.weight\", \".weight\")\n    # models produced by mergekit-extract-lora have token embeddings in the adapter\n    base_name = base_name.replace(\".lora_embedding_A\", \".weight\")\n    base_name = base_name.replace(\".lora_embedding_B\", \".weight\")\n    return base_name\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Convert a Hugging Face PEFT LoRA adapter to a GGUF file\")\n    parser.add_argument(\n        \"--outfile\", type=Path,\n        help=\"path to write to; default: based on input. {ftype} will be replaced by the outtype.\",\n    )\n    parser.add_argument(\n        \"--outtype\", type=str, choices=[\"f32\", \"f16\", \"bf16\", \"q8_0\", \"auto\"], default=\"f16\",\n        help=\"output format - use f32 for float32, f16 for float16, bf16 for bfloat16, q8_0 for Q8_0, auto for the highest-fidelity 16-bit float type depending on the first loaded tensor type\",\n    )\n    parser.add_argument(\n        \"--bigendian\", action=\"store_true\",\n        help=\"model is executed on big endian machine\",\n    )\n    parser.add_argument(\n        \"--no-lazy\", action=\"store_true\",\n        help=\"use more RAM by computing all outputs before writing (use in case lazy evaluation is broken)\",\n    )\n    parser.add_argument(\n        \"--verbose\", action=\"store_true\",\n        help=\"increase output verbosity\",\n    )\n    parser.add_argument(\n        \"--dry-run\", action=\"store_true\",\n        help=\"only print out what will be done, without writing any new files\",\n    )\n    parser.add_argument(\n        \"--base\", type=Path,\n        help=\"directory containing Hugging Face model config files (config.json, tokenizer.json) for the base model that the adapter is based on - only config is needed, actual model weights are not required. If base model is unspecified, it will be loaded from Hugging Face hub based on the adapter config\",\n    )\n    parser.add_argument(\n        \"--base-model-id\", type=str,\n        help=\"the model ID of the base model, if it is not available locally or in the adapter config. If specified, it will ignore --base and load the base model config from the Hugging Face hub (Example: 'meta-llama/Llama-3.2-1B-Instruct')\",\n    )\n    parser.add_argument(\n        \"lora_path\", type=Path,\n        help=\"directory containing Hugging Face PEFT LoRA config (adapter_model.json) and weights (adapter_model.safetensors or adapter_model.bin)\",\n    )\n\n    return parser.parse_args()\n\n\ndef load_hparams_from_hf(hf_model_id: str) -> dict[str, Any]:\n    # normally, adapter does not come with base model config, we need to load it from AutoConfig\n    config = AutoConfig.from_pretrained(hf_model_id)\n    return config.to_dict()\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)\n\n    ftype_map: dict[str, gguf.LlamaFileType] = {\n        \"f32\": gguf.LlamaFileType.ALL_F32,\n        \"f16\": gguf.LlamaFileType.MOSTLY_F16,\n        \"bf16\": gguf.LlamaFileType.MOSTLY_BF16,\n        \"q8_0\": gguf.LlamaFileType.MOSTLY_Q8_0,\n        \"auto\": gguf.LlamaFileType.GUESSED,\n    }\n\n    ftype = ftype_map[args.outtype]\n\n    dir_base_model: Path | None = args.base\n    dir_lora: Path = args.lora_path\n    base_model_id: str | None = args.base_model_id\n    lora_config = dir_lora / \"adapter_config.json\"\n    input_model = dir_lora / \"adapter_model.safetensors\"\n\n    if args.outfile is not None:\n        fname_out = args.outfile\n    else:\n        # output in the same directory as the model by default\n        fname_out = dir_lora\n\n    if os.path.exists(input_model):\n        # lazy import load_file only if lora is in safetensors format.\n        from safetensors.torch import load_file\n\n        lora_model = load_file(input_model, device=\"cpu\")\n    else:\n        input_model = os.path.join(dir_lora, \"adapter_model.bin\")\n        lora_model = torch.load(input_model, map_location=\"cpu\", weights_only=True)\n\n    # load LoRA config\n    with open(lora_config, \"r\") as f:\n        lparams: dict[str, Any] = json.load(f)\n\n    # load base model\n    if base_model_id is not None:\n        logger.info(f\"Loading base model from Hugging Face: {base_model_id}\")\n        hparams = load_hparams_from_hf(base_model_id)\n    elif dir_base_model is None:\n        if \"base_model_name_or_path\" in lparams:\n            model_id = lparams[\"base_model_name_or_path\"]\n            logger.info(f\"Loading base model from Hugging Face: {model_id}\")\n            try:\n                hparams = load_hparams_from_hf(model_id)\n            except OSError as e:\n                logger.error(f\"Failed to load base model config: {e}\")\n                logger.error(\"Please try downloading the base model and add its path to --base\")\n                sys.exit(1)\n        else:\n            logger.error(\"'base_model_name_or_path' is not found in adapter_config.json\")\n            logger.error(\"Base model config is required. Please download the base model and add its path to --base\")\n            sys.exit(1)\n    else:\n        logger.info(f\"Loading base model: {dir_base_model.name}\")\n        hparams = Model.load_hparams(dir_base_model)\n\n    with torch.inference_mode():\n        try:\n            model_class = Model.from_model_architecture(hparams[\"architectures\"][0])\n        except NotImplementedError:\n            logger.error(f\"Model {hparams['architectures'][0]} is not supported\")\n            sys.exit(1)\n\n        class LoraModel(model_class):\n            model_arch = model_class.model_arch\n\n            lora_alpha: float\n\n            def __init__(self, *args, dir_lora_model: Path, lora_alpha: float, **kwargs):\n\n                super().__init__(*args, **kwargs)\n\n                self.dir_model_card = dir_lora_model\n                self.lora_alpha = float(lora_alpha)\n\n            def set_vocab(self):\n                pass\n\n            def set_type(self):\n                self.gguf_writer.add_type(gguf.GGUFType.ADAPTER)\n                self.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, \"lora\")\n\n            def set_gguf_parameters(self):\n                self.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, self.lora_alpha)\n\n            def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n                # Never add extra tensors (e.g. rope_freqs) for LoRA adapters\n                return ()\n\n            def get_tensors(self) -> Iterator[tuple[str, Tensor]]:\n                tensor_map: dict[str, PartialLoraTensor] = {}\n\n                for name, tensor in lora_model.items():\n                    if self.lazy:\n                        tensor = LazyTorchTensor.from_eager(tensor)\n                    base_name = get_base_tensor_name(name)\n                    # note: mergekit-extract-lora also adds token embeddings to the adapter\n                    is_lora_a = \".lora_A.weight\" in name or \".lora_embedding_A\" in name\n                    is_lora_b = \".lora_B.weight\" in name or \".lora_embedding_B\" in name\n                    if not is_lora_a and not is_lora_b:\n                        if \".base_layer.weight\" in name:\n                            continue\n                        # mergekit-extract-lora add these layernorm to the adapter, we need to keep them\n                        if \"_layernorm\" in name or \".norm\" in name:\n                            yield (base_name, tensor)\n                            continue\n                        logger.error(f\"Unexpected name '{name}': Not a lora_A or lora_B tensor\")\n                        if \".embed_tokens.weight\" in name or \".lm_head.weight\" in name:\n                            logger.error(\"Embeddings is present in the adapter. This can be due to new tokens added during fine tuning\")\n                            logger.error(\"Please refer to https://github.com/ggerganov/llama.cpp/pull/9948\")\n                        sys.exit(1)\n\n                    if base_name in tensor_map:\n                        if is_lora_a:\n                            tensor_map[base_name].A = tensor\n                        else:\n                            tensor_map[base_name].B = tensor\n                    else:\n                        if is_lora_a:\n                            tensor_map[base_name] = PartialLoraTensor(A=tensor)\n                        else:\n                            tensor_map[base_name] = PartialLoraTensor(B=tensor)\n\n                for name, tensor in tensor_map.items():\n                    assert tensor.A is not None\n                    assert tensor.B is not None\n                    yield (name, cast(torch.Tensor, LoraTorchTensor(tensor.A, tensor.B)))\n\n            def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n                dest = list(super().modify_tensors(data_torch, name, bid))\n                # some archs may have the same tensor for lm_head and output (tie word embeddings)\n                # in this case, adapters targeting lm_head will fail when using llama-export-lora\n                # therefore, we ignore them for now\n                # see: https://github.com/ggerganov/llama.cpp/issues/9065\n                if name == \"lm_head.weight\" and len(dest) == 0:\n                    raise ValueError(\"lm_head is present in adapter, but is ignored in base model\")\n                for dest_name, dest_data in dest:\n                    # mergekit-extract-lora add these layernorm to the adapter\n                    if \"_norm\" in dest_name:\n                        assert dest_data.dim() == 1\n                        yield (dest_name, dest_data)\n                        continue\n\n                    # otherwise, we must get the lora_A and lora_B tensors\n                    assert isinstance(dest_data, LoraTorchTensor)\n                    lora_a, lora_b = dest_data.get_lora_A_B()\n\n                    # note: mergekit-extract-lora flip and transpose A and B\n                    # here we only need to transpose token_embd.lora_a, see llm_build_inp_embd()\n                    if \"token_embd.weight\" in dest_name:\n                        lora_a = lora_a.T\n\n                    yield (dest_name + \".lora_a\", lora_a)\n                    yield (dest_name + \".lora_b\", lora_b)\n\n        alpha: float = lparams[\"lora_alpha\"]\n\n        model_instance = LoraModel(\n            dir_base_model,\n            ftype,\n            fname_out,\n            is_big_endian=args.bigendian,\n            use_temp_file=False,\n            eager=args.no_lazy,\n            dry_run=args.dry_run,\n            dir_lora_model=dir_lora,\n            lora_alpha=alpha,\n            hparams=hparams,\n        )\n\n        logger.info(\"Exporting model...\")\n        model_instance.write()\n        logger.info(f\"Model successfully exported to {model_instance.fname_out}\")\n",
        "imports": [
            "convert_hf_to_gguf",
            "Tensor",
            "the",
            "typing",
            "logging",
            "prod",
            "annotations",
            "AutoConfig",
            "pathlib",
            "Path",
            "safetensors.torch",
            "argparse",
            "dataclass",
            "__future__",
            "math",
            "os",
            "TYPE_CHECKING",
            "dataclasses",
            "sys",
            "json",
            "load_file",
            "Hugging",
            "gguf",
            "torch",
            "transformers",
            "LazyTorchTensor",
            "convert_hf_to_gguf.py"
        ],
        "functions": [
            "get_base_tensor_name",
            "view",
            "parse_args",
            "reshape_as",
            "get_tensors",
            "load_hparams_from_hf",
            "set_type",
            "get_lora_A_B",
            "to",
            "reshape",
            "generate_extra_tensors",
            "permute",
            "swapaxes",
            "size",
            "dtype",
            "shape",
            "__getitem__",
            "__torch_function__",
            "modify_tensors",
            "transpose",
            "set_vocab",
            "__init__",
            "set_gguf_parameters"
        ],
        "variables": [
            "help",
            "is_lora_a",
            "fname_out",
            "hparams",
            "model_arch",
            "is_big_endian",
            "use_temp_file",
            "ftype",
            "logger",
            "dir_lora_model",
            "model_instance",
            "lora_b",
            "n_elems",
            "Path",
            "name",
            "lora_config",
            "kwargs",
            "lora_alpha",
            "B",
            "lora_a",
            "action",
            "parser",
            "is_lora_b",
            "dir_model_card",
            "dest",
            "__name__",
            "float",
            "dims",
            "A",
            "dim",
            "base_name",
            "indices_B",
            "orig_shape",
            "model_class",
            "lora_model",
            "indices_A",
            "shape_B",
            "dry_run",
            "args",
            "config",
            "description",
            "dtype",
            "level",
            "shape",
            "model_id",
            "new_shape",
            "_lora_B",
            "indices",
            "_lora_A",
            "None",
            "tensor",
            "_rank",
            "input_model",
            "n_new_elems",
            "type",
            "shape_A",
            "eager"
        ]
    },
    {
        "file_name": "abspath.c",
        "language": "cpp",
        "source_code": "#include \"git-compat-util.h\"\n#include \"abspath.h\"\n#include \"strbuf.h\"\n\n/*\n * Do not use this for inspecting *tracked* content.  When path is a\n * symlink to a directory, we do not want to say it is a directory when\n * dealing with tracked content in the working tree.\n */\nint is_directory(const char *path)\n{\n\tstruct stat st;\n\treturn (!stat(path, &st) && S_ISDIR(st.st_mode));\n}\n\n/* removes the last path component from 'path' except if 'path' is root */\nstatic void strip_last_component(struct strbuf *path)\n{\n\tsize_t offset = offset_1st_component(path->buf);\n\tsize_t len = path->len;\n\n\t/* Find start of the last component */\n\twhile (offset < len && !is_dir_sep(path->buf[len - 1]))\n\t\tlen--;\n\t/* Skip sequences of multiple path-separators */\n\twhile (offset < len && is_dir_sep(path->buf[len - 1]))\n\t\tlen--;\n\n\tstrbuf_setlen(path, len);\n}\n\n/* get (and remove) the next component in 'remaining' and place it in 'next' */\nstatic void get_next_component(struct strbuf *next, struct strbuf *remaining)\n{\n\tchar *start = NULL;\n\tchar *end = NULL;\n\n\tstrbuf_reset(next);\n\n\t/* look for the next component */\n\t/* Skip sequences of multiple path-separators */\n\tfor (start = remaining->buf; is_dir_sep(*start); start++)\n\t\t; /* nothing */\n\t/* Find end of the path component */\n\tfor (end = start; *end && !is_dir_sep(*end); end++)\n\t\t; /* nothing */\n\n\tstrbuf_add(next, start, end - start);\n\t/* remove the component from 'remaining' */\n\tstrbuf_remove(remaining, 0, end - remaining->buf);\n}\n\n/* copies root part from remaining to resolved, canonicalizing it on the way */\nstatic void get_root_part(struct strbuf *resolved, struct strbuf *remaining)\n{\n\tint offset = offset_1st_component(remaining->buf);\n\n\tstrbuf_reset(resolved);\n\tstrbuf_add(resolved, remaining->buf, offset);\n#ifdef GIT_WINDOWS_NATIVE\n\tconvert_slashes(resolved->buf);\n#endif\n\tstrbuf_remove(remaining, 0, offset);\n}\n\n/* We allow \"recursive\" symbolic links. Only within reason, though. */\n#ifndef MAXSYMLINKS\n#define MAXSYMLINKS 32\n#endif\n\n/*\n * If set, any number of trailing components may be missing; otherwise, only one\n * may be.\n */\n#define REALPATH_MANY_MISSING (1 << 0)\n/* Should we die if there's an error? */\n#define REALPATH_DIE_ON_ERROR (1 << 1)\n\nstatic char *strbuf_realpath_1(struct strbuf *resolved, const char *path,\n\t\t\t       int flags)\n{\n\tstruct strbuf remaining = STRBUF_INIT;\n\tstruct strbuf next = STRBUF_INIT;\n\tstruct strbuf symlink = STRBUF_INIT;\n\tchar *retval = NULL;\n\tint num_symlinks = 0;\n\tstruct stat st;\n\n\tif (!*path) {\n\t\tif (flags & REALPATH_DIE_ON_ERROR)\n\t\t\tdie(\"The empty string is not a valid path\");\n\t\telse\n\t\t\tgoto error_out;\n\t}\n\n\tstrbuf_addstr(&remaining, path);\n\tget_root_part(resolved, &remaining);\n\n\tif (!resolved->len) {\n\t\t/* relative path; can use CWD as the initial resolved path */\n\t\tif (strbuf_getcwd(resolved)) {\n\t\t\tif (flags & REALPATH_DIE_ON_ERROR)\n\t\t\t\tdie_errno(\"unable to get current working directory\");\n\t\t\telse\n\t\t\t\tgoto error_out;\n\t\t}\n\t}\n\n\t/* Iterate over the remaining path components */\n\twhile (remaining.len > 0) {\n\t\tget_next_component(&next, &remaining);\n\n\t\tif (next.len == 0) {\n\t\t\tcontinue; /* empty component */\n\t\t} else if (next.len == 1 && !strcmp(next.buf, \".\")) {\n\t\t\tcontinue; /* '.' component */\n\t\t} else if (next.len == 2 && !strcmp(next.buf, \"..\")) {\n\t\t\t/* '..' component; strip the last path component */\n\t\t\tstrip_last_component(resolved);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* append the next component and resolve resultant path */\n\t\tif (!is_dir_sep(resolved->buf[resolved->len - 1]))\n\t\t\tstrbuf_addch(resolved, '/');\n\t\tstrbuf_addbuf(resolved, &next);\n\n\t\tif (lstat(resolved->buf, &st)) {\n\t\t\t/* error out unless this was the last component */\n\t\t\tif (errno != ENOENT ||\n\t\t\t   (!(flags & REALPATH_MANY_MISSING) && remaining.len)) {\n\t\t\t\tif (flags & REALPATH_DIE_ON_ERROR)\n\t\t\t\t\tdie_errno(\"Invalid path '%s'\",\n\t\t\t\t\t\t  resolved->buf);\n\t\t\t\telse\n\t\t\t\t\tgoto error_out;\n\t\t\t}\n\t\t} else if (S_ISLNK(st.st_mode)) {\n\t\t\tssize_t len;\n\t\t\tstrbuf_reset(&symlink);\n\n\t\t\tif (num_symlinks++ > MAXSYMLINKS) {\n\t\t\t\terrno = ELOOP;\n\n\t\t\t\tif (flags & REALPATH_DIE_ON_ERROR)\n\t\t\t\t\tdie(\"More than %d nested symlinks \"\n\t\t\t\t\t    \"on path '%s'\", MAXSYMLINKS, path);\n\t\t\t\telse\n\t\t\t\t\tgoto error_out;\n\t\t\t}\n\n\t\t\tlen = strbuf_readlink(&symlink, resolved->buf,\n\t\t\t\t\t      st.st_size);\n\t\t\tif (len < 0) {\n\t\t\t\tif (flags & REALPATH_DIE_ON_ERROR)\n\t\t\t\t\tdie_errno(\"Invalid symlink '%s'\",\n\t\t\t\t\t\t  resolved->buf);\n\t\t\t\telse\n\t\t\t\t\tgoto error_out;\n\t\t\t}\n\n\t\t\tif (is_absolute_path(symlink.buf)) {\n\t\t\t\t/* absolute symlink; set resolved to root */\n\t\t\t\tget_root_part(resolved, &symlink);\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * relative symlink\n\t\t\t\t * strip off the last component since it will\n\t\t\t\t * be replaced with the contents of the symlink\n\t\t\t\t */\n\t\t\t\tstrip_last_component(resolved);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * if there are still remaining components to resolve\n\t\t\t * then append them to symlink\n\t\t\t */\n\t\t\tif (remaining.len) {\n\t\t\t\tstrbuf_addch(&symlink, '/');\n\t\t\t\tstrbuf_addbuf(&symlink, &remaining);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * use the symlink as the remaining components that\n\t\t\t * need to be resolved\n\t\t\t */\n\t\t\tstrbuf_swap(&symlink, &remaining);\n\t\t}\n\t}\n\n\tretval = resolved->buf;\n\nerror_out:\n\tstrbuf_release(&remaining);\n\tstrbuf_release(&next);\n\tstrbuf_release(&symlink);\n\n\tif (!retval)\n\t\tstrbuf_reset(resolved);\n\n\treturn retval;\n}\n\n/*\n * Return the real path (i.e., absolute path, with symlinks resolved\n * and extra slashes removed) equivalent to the specified path.  (If\n * you want an absolute path but don't mind links, use\n * absolute_path().)  Places the resolved realpath in the provided strbuf.\n *\n * The directory part of path (i.e., everything up to the last\n * dir_sep) must denote a valid, existing directory, but the last\n * component need not exist.  If die_on_error is set, then die with an\n * informative error message if there is a problem.  Otherwise, return\n * NULL on errors (without generating any output).\n */\nchar *strbuf_realpath(struct strbuf *resolved, const char *path,\n\t\t      int die_on_error)\n{\n\treturn strbuf_realpath_1(resolved, path,\n\t\t\t\t die_on_error ? REALPATH_DIE_ON_ERROR : 0);\n}\n\n/*\n * Just like strbuf_realpath, but allows an arbitrary number of path\n * components to be missing.\n */\nchar *strbuf_realpath_forgiving(struct strbuf *resolved, const char *path,\n\t\t\t\tint die_on_error)\n{\n\treturn strbuf_realpath_1(resolved, path,\n\t\t\t\t ((die_on_error ? REALPATH_DIE_ON_ERROR : 0) |\n\t\t\t\t  REALPATH_MANY_MISSING));\n}\n\nchar *real_pathdup(const char *path, int die_on_error)\n{\n\tstruct strbuf realpath = STRBUF_INIT;\n\tchar *retval = NULL;\n\n\tif (strbuf_realpath(&realpath, path, die_on_error))\n\t\tretval = strbuf_detach(&realpath, NULL);\n\n\tstrbuf_release(&realpath);\n\n\treturn retval;\n}\n\n/*\n * Use this to get an absolute path from a relative one. If you want\n * to resolve links, you should use strbuf_realpath.\n */\nconst char *absolute_path(const char *path)\n{\n\tstatic struct strbuf sb = STRBUF_INIT;\n\tstrbuf_reset(&sb);\n\tstrbuf_add_absolute_path(&sb, path);\n\treturn sb.buf;\n}\n\nchar *absolute_pathdup(const char *path)\n{\n\tstruct strbuf sb = STRBUF_INIT;\n\tstrbuf_add_absolute_path(&sb, path);\n\treturn strbuf_detach(&sb, NULL);\n}\n\nchar *prefix_filename(const char *pfx, const char *arg)\n{\n\tstruct strbuf path = STRBUF_INIT;\n\tsize_t pfx_len = pfx ? strlen(pfx) : 0;\n\n\tif (!pfx_len)\n\t\t; /* nothing to prefix */\n\telse if (is_absolute_path(arg))\n\t\tpfx_len = 0;\n\telse\n\t\tstrbuf_add(&path, pfx, pfx_len);\n\n\tstrbuf_addstr(&path, arg);\n#ifdef GIT_WINDOWS_NATIVE\n\tconvert_slashes(path.buf + pfx_len);\n#endif\n\treturn strbuf_detach(&path, NULL);\n}\n\nchar *prefix_filename_except_for_dash(const char *pfx, const char *arg)\n{\n\tif (!strcmp(arg, \"-\"))\n\t\treturn xstrdup(arg);\n\treturn prefix_filename(pfx, arg);\n}\n\nvoid strbuf_add_absolute_path(struct strbuf *sb, const char *path)\n{\n\tif (!*path)\n\t\tdie(\"The empty string is not a valid path\");\n\tif (!is_absolute_path(path)) {\n\t\tstruct stat cwd_stat, pwd_stat;\n\t\tsize_t orig_len = sb->len;\n\t\tchar *cwd = xgetcwd();\n\t\tchar *pwd = getenv(\"PWD\");\n\t\tif (pwd && strcmp(pwd, cwd) &&\n\t\t    !stat(cwd, &cwd_stat) &&\n\t\t    (cwd_stat.st_dev || cwd_stat.st_ino) &&\n\t\t    !stat(pwd, &pwd_stat) &&\n\t\t    pwd_stat.st_dev == cwd_stat.st_dev &&\n\t\t    pwd_stat.st_ino == cwd_stat.st_ino)\n\t\t\tstrbuf_addstr(sb, pwd);\n\t\telse\n\t\t\tstrbuf_addstr(sb, cwd);\n\t\tif (sb->len > orig_len && !is_dir_sep(sb->buf[sb->len - 1]))\n\t\t\tstrbuf_addch(sb, '/');\n\t\tfree(cwd);\n\t}\n\tstrbuf_addstr(sb, path);\n}\n\nvoid strbuf_add_real_path(struct strbuf *sb, const char *path)\n{\n\tif (sb->len) {\n\t\tstruct strbuf resolved = STRBUF_INIT;\n\t\tstrbuf_realpath(&resolved, path, 1);\n\t\tstrbuf_addbuf(sb, &resolved);\n\t\tstrbuf_release(&resolved);\n\t} else\n\t\tstrbuf_realpath(sb, path, 1);\n}\n",
        "imports": [
            "abspath.h",
            "strbuf.h"
        ],
        "functions": [
            "if",
            "get_next_component",
            "is_directory",
            "get_root_part",
            "strbuf_add_absolute_path",
            "strip_last_component",
            "strbuf_add_real_path"
        ],
        "variables": [
            "pwd",
            "offset",
            "pfx_len",
            "orig_len",
            "remaining",
            "end",
            "sb",
            "num_symlinks",
            "retval",
            "errno",
            "next",
            "symlink",
            "path",
            "cwd",
            "resolved",
            "len",
            "start",
            "realpath"
        ]
    },
    {
        "file_name": "add-interactive.c",
        "language": "cpp",
        "source_code": "#define DISABLE_SIGN_COMPARE_WARNINGS\n\n#include \"git-compat-util.h\"\n#include \"add-interactive.h\"\n#include \"color.h\"\n#include \"config.h\"\n#include \"diffcore.h\"\n#include \"gettext.h\"\n#include \"hash.h\"\n#include \"hex.h\"\n#include \"preload-index.h\"\n#include \"read-cache-ll.h\"\n#include \"repository.h\"\n#include \"revision.h\"\n#include \"refs.h\"\n#include \"string-list.h\"\n#include \"lockfile.h\"\n#include \"dir.h\"\n#include \"run-command.h\"\n#include \"prompt.h\"\n#include \"tree.h\"\n\nstatic void init_color(struct repository *r, struct add_i_state *s,\n\t\t       const char *section_and_slot, char *dst,\n\t\t       const char *default_color)\n{\n\tchar *key = xstrfmt(\"color.%s\", section_and_slot);\n\tconst char *value;\n\n\tif (!s->use_color)\n\t\tdst[0] = '\\0';\n\telse if (repo_config_get_value(r, key, &value) ||\n\t\t color_parse(value, dst))\n\t\tstrlcpy(dst, default_color, COLOR_MAXLEN);\n\n\tfree(key);\n}\n\nvoid init_add_i_state(struct add_i_state *s, struct repository *r)\n{\n\tconst char *value;\n\n\ts->r = r;\n\n\tif (repo_config_get_value(r, \"color.interactive\", &value))\n\t\ts->use_color = -1;\n\telse\n\t\ts->use_color =\n\t\t\tgit_config_colorbool(\"color.interactive\", value);\n\ts->use_color = want_color(s->use_color);\n\n\tinit_color(r, s, \"interactive.header\", s->header_color, GIT_COLOR_BOLD);\n\tinit_color(r, s, \"interactive.help\", s->help_color, GIT_COLOR_BOLD_RED);\n\tinit_color(r, s, \"interactive.prompt\", s->prompt_color,\n\t\t   GIT_COLOR_BOLD_BLUE);\n\tinit_color(r, s, \"interactive.error\", s->error_color,\n\t\t   GIT_COLOR_BOLD_RED);\n\n\tinit_color(r, s, \"diff.frag\", s->fraginfo_color,\n\t\t   diff_get_color(s->use_color, DIFF_FRAGINFO));\n\tinit_color(r, s, \"diff.context\", s->context_color, \"fall back\");\n\tif (!strcmp(s->context_color, \"fall back\"))\n\t\tinit_color(r, s, \"diff.plain\", s->context_color,\n\t\t\t   diff_get_color(s->use_color, DIFF_CONTEXT));\n\tinit_color(r, s, \"diff.old\", s->file_old_color,\n\t\tdiff_get_color(s->use_color, DIFF_FILE_OLD));\n\tinit_color(r, s, \"diff.new\", s->file_new_color,\n\t\tdiff_get_color(s->use_color, DIFF_FILE_NEW));\n\n\tstrlcpy(s->reset_color,\n\t\ts->use_color ? GIT_COLOR_RESET : \"\", COLOR_MAXLEN);\n\n\tFREE_AND_NULL(s->interactive_diff_filter);\n\trepo_config_get_string(r, \"interactive.difffilter\",\n\t\t\t       &s->interactive_diff_filter);\n\n\tFREE_AND_NULL(s->interactive_diff_algorithm);\n\trepo_config_get_string(r, \"diff.algorithm\",\n\t\t\t       &s->interactive_diff_algorithm);\n\n\trepo_config_get_bool(r, \"interactive.singlekey\", &s->use_single_key);\n\tif (s->use_single_key)\n\t\tsetbuf(stdin, NULL);\n}\n\nvoid clear_add_i_state(struct add_i_state *s)\n{\n\tFREE_AND_NULL(s->interactive_diff_filter);\n\tFREE_AND_NULL(s->interactive_diff_algorithm);\n\tmemset(s, 0, sizeof(*s));\n\ts->use_color = -1;\n}\n\n/*\n * A \"prefix item list\" is a list of items that are identified by a string, and\n * a unique prefix (if any) is determined for each item.\n *\n * It is implemented in the form of a pair of `string_list`s, the first one\n * duplicating the strings, with the `util` field pointing at a structure whose\n * first field must be `size_t prefix_length`.\n *\n * That `prefix_length` field will be computed by `find_unique_prefixes()`; It\n * will be set to zero if no valid, unique prefix could be found.\n *\n * The second `string_list` is called `sorted` and does _not_ duplicate the\n * strings but simply reuses the first one's, with the `util` field pointing at\n * the `string_item_list` of the first `string_list`. It  will be populated and\n * sorted by `find_unique_prefixes()`.\n */\nstruct prefix_item_list {\n\tstruct string_list items;\n\tstruct string_list sorted;\n\tint *selected; /* for multi-selections */\n\tsize_t min_length, max_length;\n};\n#define PREFIX_ITEM_LIST_INIT { \\\n\t.items = STRING_LIST_INIT_DUP, \\\n\t.sorted = STRING_LIST_INIT_NODUP, \\\n\t.min_length = 1, \\\n\t.max_length = 4, \\\n}\n\nstatic void prefix_item_list_clear(struct prefix_item_list *list)\n{\n\tstring_list_clear(&list->items, 1);\n\tstring_list_clear(&list->sorted, 0);\n\tFREE_AND_NULL(list->selected);\n}\n\nstatic void extend_prefix_length(struct string_list_item *p,\n\t\t\t\t const char *other_string, size_t max_length)\n{\n\tsize_t *len = p->util;\n\n\tif (!*len || memcmp(p->string, other_string, *len))\n\t\treturn;\n\n\tfor (;;) {\n\t\tchar c = p->string[*len];\n\n\t\t/*\n\t\t * Is `p` a strict prefix of `other`? Or have we exhausted the\n\t\t * maximal length of the prefix? Or is the current character a\n\t\t * multi-byte UTF-8 one? If so, there is no valid, unique\n\t\t * prefix.\n\t\t */\n\t\tif (!c || ++*len > max_length || !isascii(c)) {\n\t\t\t*len = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (c != other_string[*len - 1])\n\t\t\tbreak;\n\t}\n}\n\nstatic void find_unique_prefixes(struct prefix_item_list *list)\n{\n\tsize_t i;\n\n\tif (list->sorted.nr == list->items.nr)\n\t\treturn;\n\n\tstring_list_clear(&list->sorted, 0);\n\t/* Avoid reallocating incrementally */\n\tlist->sorted.items = xmalloc(st_mult(sizeof(*list->sorted.items),\n\t\t\t\t\t     list->items.nr));\n\tlist->sorted.nr = list->sorted.alloc = list->items.nr;\n\n\tfor (i = 0; i < list->items.nr; i++) {\n\t\tlist->sorted.items[i].string = list->items.items[i].string;\n\t\tlist->sorted.items[i].util = list->items.items + i;\n\t}\n\n\tstring_list_sort(&list->sorted);\n\n\tfor (i = 0; i < list->sorted.nr; i++) {\n\t\tstruct string_list_item *sorted_item = list->sorted.items + i;\n\t\tstruct string_list_item *item = sorted_item->util;\n\t\tsize_t *len = item->util;\n\n\t\t*len = 0;\n\t\twhile (*len < list->min_length) {\n\t\t\tchar c = item->string[(*len)++];\n\n\t\t\tif (!c || !isascii(c)) {\n\t\t\t\t*len = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (i > 0)\n\t\t\textend_prefix_length(item, sorted_item[-1].string,\n\t\t\t\t\t     list->max_length);\n\t\tif (i + 1 < list->sorted.nr)\n\t\t\textend_prefix_length(item, sorted_item[1].string,\n\t\t\t\t\t     list->max_length);\n\t}\n}\n\nstatic ssize_t find_unique(const char *string, struct prefix_item_list *list)\n{\n\tint index = string_list_find_insert_index(&list->sorted, string, 1);\n\tstruct string_list_item *item;\n\n\tif (list->items.nr != list->sorted.nr)\n\t\tBUG(\"prefix_item_list in inconsistent state (%\"PRIuMAX\n\t\t    \" vs %\"PRIuMAX\")\",\n\t\t    (uintmax_t)list->items.nr, (uintmax_t)list->sorted.nr);\n\n\tif (index < 0)\n\t\titem = list->sorted.items[-1 - index].util;\n\telse if (index > 0 &&\n\t\t starts_with(list->sorted.items[index - 1].string, string))\n\t\treturn -1;\n\telse if (index + 1 < list->sorted.nr &&\n\t\t starts_with(list->sorted.items[index + 1].string, string))\n\t\treturn -1;\n\telse if (index < list->sorted.nr &&\n\t\t starts_with(list->sorted.items[index].string, string))\n\t\titem = list->sorted.items[index].util;\n\telse\n\t\treturn -1;\n\treturn item - list->items.items;\n}\n\nstruct list_options {\n\tint columns;\n\tconst char *header;\n\tvoid (*print_item)(int i, int selected, struct string_list_item *item,\n\t\t\t   void *print_item_data);\n\tvoid *print_item_data;\n};\n\nstatic void list(struct add_i_state *s, struct string_list *list, int *selected,\n\t\t struct list_options *opts)\n{\n\tint i, last_lf = 0;\n\n\tif (!list->nr)\n\t\treturn;\n\n\tif (opts->header)\n\t\tcolor_fprintf_ln(stdout, s->header_color,\n\t\t\t\t \"%s\", opts->header);\n\n\tfor (i = 0; i < list->nr; i++) {\n\t\topts->print_item(i, selected ? selected[i] : 0, list->items + i,\n\t\t\t\t opts->print_item_data);\n\n\t\tif ((opts->columns) && ((i + 1) % (opts->columns))) {\n\t\t\tputchar('\\t');\n\t\t\tlast_lf = 0;\n\t\t}\n\t\telse {\n\t\t\tputchar('\\n');\n\t\t\tlast_lf = 1;\n\t\t}\n\t}\n\n\tif (!last_lf)\n\t\tputchar('\\n');\n}\nstruct list_and_choose_options {\n\tstruct list_options list_opts;\n\n\tconst char *prompt;\n\tenum {\n\t\tSINGLETON = (1<<0),\n\t\tIMMEDIATE = (1<<1),\n\t} flags;\n\tvoid (*print_help)(struct add_i_state *s);\n};\n\n#define LIST_AND_CHOOSE_ERROR (-1)\n#define LIST_AND_CHOOSE_QUIT  (-2)\n\n/*\n * Returns the selected index in singleton mode, the number of selected items\n * otherwise.\n *\n * If an error occurred, returns `LIST_AND_CHOOSE_ERROR`. Upon EOF,\n * `LIST_AND_CHOOSE_QUIT` is returned.\n */\nstatic ssize_t list_and_choose(struct add_i_state *s,\n\t\t\t       struct prefix_item_list *items,\n\t\t\t       struct list_and_choose_options *opts)\n{\n\tint singleton = opts->flags & SINGLETON;\n\tint immediate = opts->flags & IMMEDIATE;\n\n\tstruct strbuf input = STRBUF_INIT;\n\tssize_t res = singleton ? LIST_AND_CHOOSE_ERROR : 0;\n\n\tif (!singleton) {\n\t\tfree(items->selected);\n\t\tCALLOC_ARRAY(items->selected, items->items.nr);\n\t}\n\n\tif (singleton && !immediate)\n\t\tBUG(\"singleton requires immediate\");\n\n\tfind_unique_prefixes(items);\n\n\tfor (;;) {\n\t\tchar *p;\n\n\t\tstrbuf_reset(&input);\n\n\t\tlist(s, &items->items, items->selected, &opts->list_opts);\n\n\t\tcolor_fprintf(stdout, s->prompt_color, \"%s\", opts->prompt);\n\t\tfputs(singleton ? \"> \" : \">> \", stdout);\n\t\tfflush(stdout);\n\n\t\tif (git_read_line_interactively(&input) == EOF) {\n\t\t\tputchar('\\n');\n\t\t\tif (immediate)\n\t\t\t\tres = LIST_AND_CHOOSE_QUIT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!input.len)\n\t\t\tbreak;\n\n\t\tif (!strcmp(input.buf, \"?\")) {\n\t\t\topts->print_help(s);\n\t\t\tcontinue;\n\t\t}\n\n\t\tp = input.buf;\n\t\tfor (;;) {\n\t\t\tsize_t sep = strcspn(p, \" \\t\\r\\n,\");\n\t\t\tint choose = 1;\n\t\t\t/* `from` is inclusive, `to` is exclusive */\n\t\t\tssize_t from = -1, to = -1;\n\n\t\t\tif (!sep) {\n\t\t\t\tif (!*p)\n\t\t\t\t\tbreak;\n\t\t\t\tp++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Input that begins with '-'; de-select */\n\t\t\tif (*p == '-') {\n\t\t\t\tchoose = 0;\n\t\t\t\tp++;\n\t\t\t\tsep--;\n\t\t\t}\n\n\t\t\tif (sep == 1 && *p == '*') {\n\t\t\t\tfrom = 0;\n\t\t\t\tto = items->items.nr;\n\t\t\t} else if (isdigit(*p)) {\n\t\t\t\tchar *endp;\n\t\t\t\t/*\n\t\t\t\t * A range can be specified like 5-7 or 5-.\n\t\t\t\t *\n\t\t\t\t * Note: `from` is 0-based while the user input\n\t\t\t\t * is 1-based, hence we have to decrement by\n\t\t\t\t * one. We do not have to decrement `to` even\n\t\t\t\t * if it is 0-based because it is an exclusive\n\t\t\t\t * boundary.\n\t\t\t\t */\n\t\t\t\tfrom = strtoul(p, &endp, 10) - 1;\n\t\t\t\tif (endp == p + sep)\n\t\t\t\t\tto = from + 1;\n\t\t\t\telse if (*endp == '-') {\n\t\t\t\t\tif (isdigit(*(++endp)))\n\t\t\t\t\t\tto = strtoul(endp, &endp, 10);\n\t\t\t\t\telse\n\t\t\t\t\t\tto = items->items.nr;\n\t\t\t\t\t/* extra characters after the range? */\n\t\t\t\t\tif (endp != p + sep)\n\t\t\t\t\t\tfrom = -1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (p[sep])\n\t\t\t\tp[sep++] = '\\0';\n\t\t\tif (from < 0) {\n\t\t\t\tfrom = find_unique(p, items);\n\t\t\t\tif (from >= 0)\n\t\t\t\t\tto = from + 1;\n\t\t\t}\n\n\t\t\tif (from < 0 || from >= items->items.nr ||\n\t\t\t    (singleton && from + 1 != to)) {\n\t\t\t\tcolor_fprintf_ln(stderr, s->error_color,\n\t\t\t\t\t\t _(\"Huh (%s)?\"), p);\n\t\t\t\tbreak;\n\t\t\t} else if (singleton) {\n\t\t\t\tres = from;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (to > items->items.nr)\n\t\t\t\tto = items->items.nr;\n\n\t\t\tfor (; from < to; from++)\n\t\t\t\tif (items->selected[from] != choose) {\n\t\t\t\t\titems->selected[from] = choose;\n\t\t\t\t\tres += choose ? +1 : -1;\n\t\t\t\t}\n\n\t\t\tp += sep;\n\t\t}\n\n\t\tif ((immediate && res != LIST_AND_CHOOSE_ERROR) ||\n\t\t    !strcmp(input.buf, \"*\"))\n\t\t\tbreak;\n\t}\n\n\tstrbuf_release(&input);\n\treturn res;\n}\n\nstruct adddel {\n\tuintmax_t add, del;\n\tunsigned seen:1, unmerged:1, binary:1;\n};\n\nstruct file_item {\n\tsize_t prefix_length;\n\tstruct adddel index, worktree;\n};\n\nstatic void add_file_item(struct string_list *files, const char *name)\n{\n\tstruct file_item *item = xcalloc(1, sizeof(*item));\n\n\tstring_list_append(files, name)->util = item;\n}\n\nstruct pathname_entry {\n\tstruct hashmap_entry ent;\n\tconst char *name;\n\tstruct file_item *item;\n};\n\nstatic int pathname_entry_cmp(const void *cmp_data UNUSED,\n\t\t\t      const struct hashmap_entry *he1,\n\t\t\t      const struct hashmap_entry *he2,\n\t\t\t      const void *name)\n{\n\tconst struct pathname_entry *e1 =\n\t\tcontainer_of(he1, const struct pathname_entry, ent);\n\tconst struct pathname_entry *e2 =\n\t\tcontainer_of(he2, const struct pathname_entry, ent);\n\n\treturn strcmp(e1->name, name ? (const char *)name : e2->name);\n}\n\nstruct collection_status {\n\tenum { FROM_WORKTREE = 0, FROM_INDEX = 1 } mode;\n\n\tconst char *reference;\n\n\tunsigned skip_unseen:1;\n\tsize_t unmerged_count, binary_count;\n\tstruct string_list *files;\n\tstruct hashmap file_map;\n};\n\nstatic void collect_changes_cb(struct diff_queue_struct *q,\n\t\t\t       struct diff_options *options,\n\t\t\t       void *data)\n{\n\tstruct collection_status *s = data;\n\tstruct diffstat_t stat = { 0 };\n\tint i;\n\n\tif (!q->nr)\n\t\treturn;\n\n\tcompute_diffstat(options, &stat, q);\n\n\tfor (i = 0; i < stat.nr; i++) {\n\t\tconst char *name = stat.files[i]->name;\n\t\tint hash = strhash(name);\n\t\tstruct pathname_entry *entry;\n\t\tstruct file_item *file_item;\n\t\tstruct adddel *adddel, *other_adddel;\n\n\t\tentry = hashmap_get_entry_from_hash(&s->file_map, hash, name,\n\t\t\t\t\t\t    struct pathname_entry, ent);\n\t\tif (!entry) {\n\t\t\tif (s->skip_unseen)\n\t\t\t\tcontinue;\n\n\t\t\tadd_file_item(s->files, name);\n\n\t\t\tCALLOC_ARRAY(entry, 1);\n\t\t\thashmap_entry_init(&entry->ent, hash);\n\t\t\tentry->name = s->files->items[s->files->nr - 1].string;\n\t\t\tentry->item = s->files->items[s->files->nr - 1].util;\n\t\t\thashmap_add(&s->file_map, &entry->ent);\n\t\t}\n\n\t\tfile_item = entry->item;\n\t\tadddel = s->mode == FROM_INDEX ?\n\t\t\t&file_item->index : &file_item->worktree;\n\t\tother_adddel = s->mode == FROM_INDEX ?\n\t\t\t&file_item->worktree : &file_item->index;\n\t\tadddel->seen = 1;\n\t\tadddel->add = stat.files[i]->added;\n\t\tadddel->del = stat.files[i]->deleted;\n\t\tif (stat.files[i]->is_binary) {\n\t\t\tif (!other_adddel->binary)\n\t\t\t\ts->binary_count++;\n\t\t\tadddel->binary = 1;\n\t\t}\n\t\tif (stat.files[i]->is_unmerged) {\n\t\t\tif (!other_adddel->unmerged)\n\t\t\t\ts->unmerged_count++;\n\t\t\tadddel->unmerged = 1;\n\t\t}\n\t}\n\tfree_diffstat_info(&stat);\n}\n\nenum modified_files_filter {\n\tNO_FILTER = 0,\n\tWORKTREE_ONLY = 1,\n\tINDEX_ONLY = 2,\n};\n\nstatic int get_modified_files(struct repository *r,\n\t\t\t      enum modified_files_filter filter,\n\t\t\t      struct prefix_item_list *files,\n\t\t\t      const struct pathspec *ps,\n\t\t\t      size_t *unmerged_count,\n\t\t\t      size_t *binary_count)\n{\n\tstruct object_id head_oid;\n\tint is_initial = !refs_resolve_ref_unsafe(get_main_ref_store(r),\n\t\t\t\t\t\t  \"HEAD\", RESOLVE_REF_READING,\n\t\t\t\t\t\t  &head_oid, NULL);\n\tstruct collection_status s = { 0 };\n\tint i;\n\n\tdiscard_index(r->index);\n\tif (repo_read_index_preload(r, ps, 0) < 0)\n\t\treturn error(_(\"could not read index\"));\n\n\tprefix_item_list_clear(files);\n\ts.files = &files->items;\n\thashmap_init(&s.file_map, pathname_entry_cmp, NULL, 0);\n\n\tfor (i = 0; i < 2; i++) {\n\t\tstruct rev_info rev;\n\t\tstruct setup_revision_opt opt = { 0 };\n\n\t\tif (filter == INDEX_ONLY)\n\t\t\ts.mode = (i == 0) ? FROM_INDEX : FROM_WORKTREE;\n\t\telse\n\t\t\ts.mode = (i == 0) ? FROM_WORKTREE : FROM_INDEX;\n\t\ts.skip_unseen = filter && i;\n\n\t\topt.def = is_initial ?\n\t\t\tempty_tree_oid_hex(r->hash_algo) : oid_to_hex(&head_oid);\n\n\t\trepo_init_revisions(r, &rev, NULL);\n\t\tsetup_revisions(0, NULL, &rev, &opt);\n\n\t\trev.diffopt.output_format = DIFF_FORMAT_CALLBACK;\n\t\trev.diffopt.format_callback = collect_changes_cb;\n\t\trev.diffopt.format_callback_data = &s;\n\n\t\tif (ps)\n\t\t\tcopy_pathspec(&rev.prune_data, ps);\n\n\t\tif (s.mode == FROM_INDEX)\n\t\t\trun_diff_index(&rev, DIFF_INDEX_CACHED);\n\t\telse {\n\t\t\trev.diffopt.flags.ignore_dirty_submodules = 1;\n\t\t\trun_diff_files(&rev, 0);\n\t\t}\n\n\t\trelease_revisions(&rev);\n\t}\n\thashmap_clear_and_free(&s.file_map, struct pathname_entry, ent);\n\tif (unmerged_count)\n\t\t*unmerged_count = s.unmerged_count;\n\tif (binary_count)\n\t\t*binary_count = s.binary_count;\n\n\t/* While the diffs are ordered already, we ran *two* diffs... */\n\tstring_list_sort(&files->items);\n\n\treturn 0;\n}\n\nstatic void render_adddel(struct strbuf *buf,\n\t\t\t\tstruct adddel *ad, const char *no_changes)\n{\n\tif (ad->binary)\n\t\tstrbuf_addstr(buf, _(\"binary\"));\n\telse if (ad->seen)\n\t\tstrbuf_addf(buf, \"+%\"PRIuMAX\"/-%\"PRIuMAX,\n\t\t\t    (uintmax_t)ad->add, (uintmax_t)ad->del);\n\telse\n\t\tstrbuf_addstr(buf, no_changes);\n}\n\n/* filters out prefixes which have special meaning to list_and_choose() */\nstatic int is_valid_prefix(const char *prefix, size_t prefix_len)\n{\n\treturn prefix_len && prefix &&\n\t\t/*\n\t\t * We expect `prefix` to be NUL terminated, therefore this\n\t\t * `strcspn()` call is okay, even if it might do much more\n\t\t * work than strictly necessary.\n\t\t */\n\t\tstrcspn(prefix, \" \\t\\r\\n,\") >= prefix_len &&\t/* separators */\n\t\t*prefix != '-' &&\t\t\t\t/* deselection */\n\t\t!isdigit(*prefix) &&\t\t\t\t/* selection */\n\t\t(prefix_len != 1 ||\n\t\t (*prefix != '*' &&\t\t\t\t/* \"all\" wildcard */\n\t\t  *prefix != '?'));\t\t\t\t/* prompt help */\n}\n\nstruct print_file_item_data {\n\tconst char *modified_fmt, *color, *reset;\n\tstruct strbuf buf, name, index, worktree;\n\tunsigned only_names:1;\n};\n\nstatic void print_file_item(int i, int selected, struct string_list_item *item,\n\t\t\t    void *print_file_item_data)\n{\n\tstruct file_item *c = item->util;\n\tstruct print_file_item_data *d = print_file_item_data;\n\tconst char *highlighted = NULL;\n\n\tstrbuf_reset(&d->index);\n\tstrbuf_reset(&d->worktree);\n\tstrbuf_reset(&d->buf);\n\n\t/* Format the item with the prefix highlighted. */\n\tif (c->prefix_length > 0 &&\n\t    is_valid_prefix(item->string, c->prefix_length)) {\n\t\tstrbuf_reset(&d->name);\n\t\tstrbuf_addf(&d->name, \"%s%.*s%s%s\", d->color,\n\t\t\t    (int)c->prefix_length, item->string, d->reset,\n\t\t\t    item->string + c->prefix_length);\n\t\thighlighted = d->name.buf;\n\t}\n\n\tif (d->only_names) {\n\t\tprintf(\"%c%2d: %s\", selected ? '*' : ' ', i + 1,\n\t\t       highlighted ? highlighted : item->string);\n\t\treturn;\n\t}\n\n\trender_adddel(&d->worktree, &c->worktree, _(\"nothing\"));\n\trender_adddel(&d->index, &c->index, _(\"unchanged\"));\n\n\tstrbuf_addf(&d->buf, d->modified_fmt, d->index.buf, d->worktree.buf,\n\t\t    highlighted ? highlighted : item->string);\n\n\tprintf(\"%c%2d: %s\", selected ? '*' : ' ', i + 1, d->buf.buf);\n}\n\nstatic int run_status(struct add_i_state *s, const struct pathspec *ps,\n\t\t      struct prefix_item_list *files,\n\t\t      struct list_and_choose_options *opts)\n{\n\tif (get_modified_files(s->r, NO_FILTER, files, ps, NULL, NULL) < 0)\n\t\treturn -1;\n\n\tlist(s, &files->items, NULL, &opts->list_opts);\n\tputchar('\\n');\n\n\treturn 0;\n}\n\nstatic int run_update(struct add_i_state *s, const struct pathspec *ps,\n\t\t      struct prefix_item_list *files,\n\t\t      struct list_and_choose_options *opts)\n{\n\tint res = 0, fd;\n\tsize_t count, i;\n\tstruct lock_file index_lock;\n\n\tif (get_modified_files(s->r, WORKTREE_ONLY, files, ps, NULL, NULL) < 0)\n\t\treturn -1;\n\n\tif (!files->items.nr) {\n\t\tputchar('\\n');\n\t\treturn 0;\n\t}\n\n\topts->prompt = N_(\"Update\");\n\tcount = list_and_choose(s, files, opts);\n\tif (count <= 0) {\n\t\tputchar('\\n');\n\t\treturn 0;\n\t}\n\n\tfd = repo_hold_locked_index(s->r, &index_lock, LOCK_REPORT_ON_ERROR);\n\tif (fd < 0) {\n\t\tputchar('\\n');\n\t\treturn -1;\n\t}\n\n\tfor (i = 0; i < files->items.nr; i++) {\n\t\tconst char *name = files->items.items[i].string;\n\t\tstruct stat st;\n\n\t\tif (!files->selected[i])\n\t\t\tcontinue;\n\t\tif (lstat(name, &st) && is_missing_file_error(errno)) {\n\t\t\tif (remove_file_from_index(s->r->index, name) < 0) {\n\t\t\t\tres = error(_(\"could not stage '%s'\"), name);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else if (add_file_to_index(s->r->index, name, 0) < 0) {\n\t\t\tres = error(_(\"could not stage '%s'\"), name);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!res && write_locked_index(s->r->index, &index_lock, COMMIT_LOCK) < 0)\n\t\tres = error(_(\"could not write index\"));\n\n\tif (!res)\n\t\tprintf(Q_(\"updated %d path\\n\",\n\t\t\t  \"updated %d paths\\n\", count), (int)count);\n\n\tputchar('\\n');\n\treturn res;\n}\n\nstatic void revert_from_diff(struct diff_queue_struct *q,\n\t\t\t     struct diff_options *opt, void *data UNUSED)\n{\n\tint i, add_flags = ADD_CACHE_OK_TO_ADD | ADD_CACHE_OK_TO_REPLACE;\n\n\tfor (i = 0; i < q->nr; i++) {\n\t\tstruct diff_filespec *one = q->queue[i]->one;\n\t\tstruct cache_entry *ce;\n\n\t\tif (!(one->mode && !is_null_oid(&one->oid))) {\n\t\t\tremove_file_from_index(opt->repo->index, one->path);\n\t\t\tprintf(_(\"note: %s is untracked now.\\n\"), one->path);\n\t\t} else {\n\t\t\tce = make_cache_entry(opt->repo->index, one->mode,\n\t\t\t\t\t      &one->oid, one->path, 0, 0);\n\t\t\tif (!ce)\n\t\t\t\tdie(_(\"make_cache_entry failed for path '%s'\"),\n\t\t\t\t    one->path);\n\t\t\tadd_index_entry(opt->repo->index, ce, add_flags);\n\t\t}\n\t}\n}\n\nstatic int run_revert(struct add_i_state *s, const struct pathspec *ps,\n\t\t      struct prefix_item_list *files,\n\t\t      struct list_and_choose_options *opts)\n{\n\tint res = 0, fd;\n\tsize_t count, i, j;\n\n\tstruct object_id oid;\n\tint is_initial = !refs_resolve_ref_unsafe(get_main_ref_store(s->r),\n\t\t\t\t\t\t  \"HEAD\", RESOLVE_REF_READING,\n\t\t\t\t\t\t  &oid,\n\t\t\t\t\t\t  NULL);\n\tstruct lock_file index_lock;\n\tconst char **paths;\n\tstruct tree *tree;\n\tstruct diff_options diffopt = { NULL };\n\n\tif (get_modified_files(s->r, INDEX_ONLY, files, ps, NULL, NULL) < 0)\n\t\treturn -1;\n\n\tif (!files->items.nr) {\n\t\tputchar('\\n');\n\t\treturn 0;\n\t}\n\n\topts->prompt = N_(\"Revert\");\n\tcount = list_and_choose(s, files, opts);\n\tif (count <= 0)\n\t\tgoto finish_revert;\n\n\tfd = repo_hold_locked_index(s->r, &index_lock, LOCK_REPORT_ON_ERROR);\n\tif (fd < 0) {\n\t\tres = -1;\n\t\tgoto finish_revert;\n\t}\n\n\tif (is_initial)\n\t\toidcpy(&oid, s->r->hash_algo->empty_tree);\n\telse {\n\t\ttree = parse_tree_indirect(&oid);\n\t\tif (!tree) {\n\t\t\tres = error(_(\"Could not parse HEAD^{tree}\"));\n\t\t\tgoto finish_revert;\n\t\t}\n\t\toidcpy(&oid, &tree->object.oid);\n\t}\n\n\tALLOC_ARRAY(paths, count + 1);\n\tfor (i = j = 0; i < files->items.nr; i++)\n\t\tif (files->selected[i])\n\t\t\tpaths[j++] = files->items.items[i].string;\n\tpaths[j] = NULL;\n\n\tparse_pathspec(&diffopt.pathspec, 0,\n\t\t       PATHSPEC_PREFER_FULL | PATHSPEC_LITERAL_PATH,\n\t\t       NULL, paths);\n\n\tdiffopt.output_format = DIFF_FORMAT_CALLBACK;\n\tdiffopt.format_callback = revert_from_diff;\n\tdiffopt.flags.override_submodule_config = 1;\n\tdiffopt.repo = s->r;\n\n\tif (do_diff_cache(&oid, &diffopt)) {\n\t\tdiff_free(&diffopt);\n\t\tres = -1;\n\t} else {\n\t\tdiffcore_std(&diffopt);\n\t\tdiff_flush(&diffopt);\n\t}\n\tfree(paths);\n\n\tif (!res && write_locked_index(s->r->index, &index_lock,\n\t\t\t\t       COMMIT_LOCK) < 0)\n\t\tres = -1;\n\telse\n\t\tres = repo_refresh_and_write_index(s->r, REFRESH_QUIET, 0, 1,\n\t\t\t\t\t\t   NULL, NULL, NULL);\n\n\tif (!res)\n\t\tprintf(Q_(\"reverted %d path\\n\",\n\t\t\t  \"reverted %d paths\\n\", count), (int)count);\n\nfinish_revert:\n\tputchar('\\n');\n\treturn res;\n}\n\nstatic int get_untracked_files(struct repository *r,\n\t\t\t       struct prefix_item_list *files,\n\t\t\t       const struct pathspec *ps)\n{\n\tstruct dir_struct dir = { 0 };\n\tsize_t i;\n\tstruct strbuf buf = STRBUF_INIT;\n\n\tif (repo_read_index(r) < 0)\n\t\treturn error(_(\"could not read index\"));\n\n\tprefix_item_list_clear(files);\n\tsetup_standard_excludes(&dir);\n\tadd_pattern_list(&dir, EXC_CMDL, \"--exclude option\");\n\tfill_directory(&dir, r->index, ps);\n\n\tfor (i = 0; i < dir.nr; i++) {\n\t\tstruct dir_entry *ent = dir.entries[i];\n\n\t\tif (index_name_is_other(r->index, ent->name, ent->len)) {\n\t\t\tstrbuf_reset(&buf);\n\t\t\tstrbuf_add(&buf, ent->name, ent->len);\n\t\t\tadd_file_item(&files->items, buf.buf);\n\t\t}\n\t}\n\n\tstrbuf_release(&buf);\n\tdir_clear(&dir);\n\treturn 0;\n}\n\nstatic int run_add_untracked(struct add_i_state *s, const struct pathspec *ps,\n\t\t      struct prefix_item_list *files,\n\t\t      struct list_and_choose_options *opts)\n{\n\tstruct print_file_item_data *d = opts->list_opts.print_item_data;\n\tint res = 0, fd;\n\tsize_t count, i;\n\tstruct lock_file index_lock;\n\n\tif (get_untracked_files(s->r, files, ps) < 0)\n\t\treturn -1;\n\n\tif (!files->items.nr) {\n\t\tprintf(_(\"No untracked files.\\n\"));\n\t\tgoto finish_add_untracked;\n\t}\n\n\topts->prompt = N_(\"Add untracked\");\n\td->only_names = 1;\n\tcount = list_and_choose(s, files, opts);\n\td->only_names = 0;\n\tif (count <= 0)\n\t\tgoto finish_add_untracked;\n\n\tfd = repo_hold_locked_index(s->r, &index_lock, LOCK_REPORT_ON_ERROR);\n\tif (fd < 0) {\n\t\tres = -1;\n\t\tgoto finish_add_untracked;\n\t}\n\n\tfor (i = 0; i < files->items.nr; i++) {\n\t\tconst char *name = files->items.items[i].string;\n\t\tif (files->selected[i] &&\n\t\t    add_file_to_index(s->r->index, name, 0) < 0) {\n\t\t\tres = error(_(\"could not stage '%s'\"), name);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!res &&\n\t    write_locked_index(s->r->index, &index_lock, COMMIT_LOCK) < 0)\n\t\tres = error(_(\"could not write index\"));\n\n\tif (!res)\n\t\tprintf(Q_(\"added %d path\\n\",\n\t\t\t  \"added %d paths\\n\", count), (int)count);\n\nfinish_add_untracked:\n\tputchar('\\n');\n\treturn res;\n}\n\nstatic int run_patch(struct add_i_state *s, const struct pathspec *ps,\n\t\t     struct prefix_item_list *files,\n\t\t     struct list_and_choose_options *opts)\n{\n\tint res = 0;\n\tssize_t count, i, j;\n\tsize_t unmerged_count = 0, binary_count = 0;\n\n\tif (get_modified_files(s->r, WORKTREE_ONLY, files, ps,\n\t\t\t       &unmerged_count, &binary_count) < 0)\n\t\treturn -1;\n\n\tif (unmerged_count || binary_count) {\n\t\tfor (i = j = 0; i < files->items.nr; i++) {\n\t\t\tstruct file_item *item = files->items.items[i].util;\n\n\t\t\tif (item->index.binary || item->worktree.binary) {\n\t\t\t\tfree(item);\n\t\t\t\tfree(files->items.items[i].string);\n\t\t\t} else if (item->index.unmerged ||\n\t\t\t\t item->worktree.unmerged) {\n\t\t\t\tcolor_fprintf_ln(stderr, s->error_color,\n\t\t\t\t\t\t _(\"ignoring unmerged: %s\"),\n\t\t\t\t\t\t files->items.items[i].string);\n\t\t\t\tfree(item);\n\t\t\t\tfree(files->items.items[i].string);\n\t\t\t} else\n\t\t\t\tfiles->items.items[j++] = files->items.items[i];\n\t\t}\n\t\tfiles->items.nr = j;\n\t}\n\n\tif (!files->items.nr) {\n\t\tif (binary_count)\n\t\t\tfprintf(stderr, _(\"Only binary files changed.\\n\"));\n\t\telse\n\t\t\tfprintf(stderr, _(\"No changes.\\n\"));\n\t\treturn 0;\n\t}\n\n\topts->prompt = N_(\"Patch update\");\n\tcount = list_and_choose(s, files, opts);\n\tif (count > 0) {\n\t\tstruct strvec args = STRVEC_INIT;\n\t\tstruct pathspec ps_selected = { 0 };\n\n\t\tfor (i = 0; i < files->items.nr; i++)\n\t\t\tif (files->selected[i])\n\t\t\t\tstrvec_push(&args,\n\t\t\t\t\t    files->items.items[i].string);\n\t\tparse_pathspec(&ps_selected,\n\t\t\t       PATHSPEC_ALL_MAGIC & ~PATHSPEC_LITERAL,\n\t\t\t       PATHSPEC_LITERAL_PATH, \"\", args.v);\n\t\tres = run_add_p(s->r, ADD_P_ADD, NULL, &ps_selected);\n\t\tstrvec_clear(&args);\n\t\tclear_pathspec(&ps_selected);\n\t}\n\n\treturn res;\n}\n\nstatic int run_diff(struct add_i_state *s, const struct pathspec *ps,\n\t\t    struct prefix_item_list *files,\n\t\t    struct list_and_choose_options *opts)\n{\n\tint res = 0;\n\tssize_t count, i;\n\n\tstruct object_id oid;\n\tint is_initial = !refs_resolve_ref_unsafe(get_main_ref_store(s->r),\n\t\t\t\t\t\t  \"HEAD\", RESOLVE_REF_READING,\n\t\t\t\t\t\t  &oid,\n\t\t\t\t\t\t  NULL);\n\tif (get_modified_files(s->r, INDEX_ONLY, files, ps, NULL, NULL) < 0)\n\t\treturn -1;\n\n\tif (!files->items.nr) {\n\t\tputchar('\\n');\n\t\treturn 0;\n\t}\n\n\topts->prompt = N_(\"Review diff\");\n\topts->flags = IMMEDIATE;\n\tcount = list_and_choose(s, files, opts);\n\topts->flags = 0;\n\tif (count > 0) {\n\t\tstruct child_process cmd = CHILD_PROCESS_INIT;\n\n\t\tstrvec_pushl(&cmd.args, \"git\", \"diff\", \"-p\", \"--cached\",\n\t\t\t     oid_to_hex(!is_initial ? &oid :\n\t\t\t\t\ts->r->hash_algo->empty_tree),\n\t\t\t     \"--\", NULL);\n\t\tfor (i = 0; i < files->items.nr; i++)\n\t\t\tif (files->selected[i])\n\t\t\t\tstrvec_push(&cmd.args,\n\t\t\t\t\t    files->items.items[i].string);\n\t\tres = run_command(&cmd);\n\t}\n\n\tputchar('\\n');\n\treturn res;\n}\n\nstatic int run_help(struct add_i_state *s, const struct pathspec *ps UNUSED,\n\t\t    struct prefix_item_list *files UNUSED,\n\t\t    struct list_and_choose_options *opts UNUSED)\n{\n\tcolor_fprintf_ln(stdout, s->help_color, \"status        - %s\",\n\t\t\t _(\"show paths with changes\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"update        - %s\",\n\t\t\t _(\"add working tree state to the staged set of changes\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"revert        - %s\",\n\t\t\t _(\"revert staged set of changes back to the HEAD version\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"patch         - %s\",\n\t\t\t _(\"pick hunks and update selectively\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"diff          - %s\",\n\t\t\t _(\"view diff between HEAD and index\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"add untracked - %s\",\n\t\t\t _(\"add contents of untracked files to the staged set of changes\"));\n\n\treturn 0;\n}\n\nstatic void choose_prompt_help(struct add_i_state *s)\n{\n\tcolor_fprintf_ln(stdout, s->help_color, \"%s\",\n\t\t\t _(\"Prompt help:\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"1          - %s\",\n\t\t\t _(\"select a single item\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"3-5        - %s\",\n\t\t\t _(\"select a range of items\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"2-3,6-9    - %s\",\n\t\t\t _(\"select multiple ranges\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"foo        - %s\",\n\t\t\t _(\"select item based on unique prefix\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"-...       - %s\",\n\t\t\t _(\"unselect specified items\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"*          - %s\",\n\t\t\t _(\"choose all items\"));\n\tcolor_fprintf_ln(stdout, s->help_color, \"           - %s\",\n\t\t\t _(\"(empty) finish selecting\"));\n}\n\ntypedef int (*command_t)(struct add_i_state *s, const struct pathspec *ps,\n\t\t\t struct prefix_item_list *files,\n\t\t\t struct list_and_choose_options *opts);\n\nstruct command_item {\n\tsize_t prefix_length;\n\tcommand_t command;\n};\n\nstruct print_command_item_data {\n\tconst char *color, *reset;\n};\n\nstatic void print_command_item(int i, int selected UNUSED,\n\t\t\t       struct string_list_item *item,\n\t\t\t       void *print_command_item_data)\n{\n\tstruct print_command_item_data *d = print_command_item_data;\n\tstruct command_item *util = item->util;\n\n\tif (!util->prefix_length ||\n\t    !is_valid_prefix(item->string, util->prefix_length))\n\t\tprintf(\" %2d: %s\", i + 1, item->string);\n\telse\n\t\tprintf(\" %2d: %s%.*s%s%s\", i + 1,\n\t\t       d->color, (int)util->prefix_length, item->string,\n\t\t       d->reset, item->string + util->prefix_length);\n}\n\nstatic void command_prompt_help(struct add_i_state *s)\n{\n\tconst char *help_color = s->help_color;\n\tcolor_fprintf_ln(stdout, help_color, \"%s\", _(\"Prompt help:\"));\n\tcolor_fprintf_ln(stdout, help_color, \"1          - %s\",\n\t\t\t _(\"select a numbered item\"));\n\tcolor_fprintf_ln(stdout, help_color, \"foo        - %s\",\n\t\t\t _(\"select item based on unique prefix\"));\n\tcolor_fprintf_ln(stdout, help_color, \"           - %s\",\n\t\t\t _(\"(empty) select nothing\"));\n}\n\nint run_add_i(struct repository *r, const struct pathspec *ps)\n{\n\tstruct add_i_state s = { NULL };\n\tstruct print_command_item_data data = { \"[\", \"]\" };\n\tstruct list_and_choose_options main_loop_opts = {\n\t\t{ 4, N_(\"*** Commands ***\"), print_command_item, &data },\n\t\tN_(\"What now\"), SINGLETON | IMMEDIATE, command_prompt_help\n\t};\n\tstruct {\n\t\tconst char *string;\n\t\tcommand_t command;\n\t} command_list[] = {\n\t\t{ \"status\", run_status },\n\t\t{ \"update\", run_update },\n\t\t{ \"revert\", run_revert },\n\t\t{ \"add untracked\", run_add_untracked },\n\t\t{ \"patch\", run_patch },\n\t\t{ \"diff\", run_diff },\n\t\t{ \"quit\", NULL },\n\t\t{ \"help\", run_help },\n\t};\n\tstruct prefix_item_list commands = PREFIX_ITEM_LIST_INIT;\n\n\tstruct print_file_item_data print_file_item_data = {\n\t\t\"%12s %12s %s\", NULL, NULL,\n\t\tSTRBUF_INIT, STRBUF_INIT, STRBUF_INIT, STRBUF_INIT\n\t};\n\tstruct list_and_choose_options opts = {\n\t\t{ 0, NULL, print_file_item, &print_file_item_data },\n\t\tNULL, 0, choose_prompt_help\n\t};\n\tstruct strbuf header = STRBUF_INIT;\n\tstruct prefix_item_list files = PREFIX_ITEM_LIST_INIT;\n\tssize_t i;\n\tint res = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(command_list); i++) {\n\t\tstruct command_item *util = xcalloc(1, sizeof(*util));\n\t\tutil->command = command_list[i].command;\n\t\tstring_list_append(&commands.items, command_list[i].string)\n\t\t\t->util = util;\n\t}\n\n\tinit_add_i_state(&s, r);\n\n\t/*\n\t * When color was asked for, use the prompt color for\n\t * highlighting, otherwise use square brackets.\n\t */\n\tif (s.use_color) {\n\t\tdata.color = s.prompt_color;\n\t\tdata.reset = s.reset_color;\n\t}\n\tprint_file_item_data.color = data.color;\n\tprint_file_item_data.reset = data.reset;\n\n\tstrbuf_addstr(&header, \"     \");\n\tstrbuf_addf(&header, print_file_item_data.modified_fmt,\n\t\t    _(\"staged\"), _(\"unstaged\"), _(\"path\"));\n\topts.list_opts.header = header.buf;\n\n\tdiscard_index(r->index);\n\tif (repo_read_index(r) < 0 ||\n\t    repo_refresh_and_write_index(r, REFRESH_QUIET, 0, 1,\n\t\t\t\t\t NULL, NULL, NULL) < 0)\n\t\twarning(_(\"could not refresh index\"));\n\n\tres = run_status(&s, ps, &files, &opts);\n\n\tfor (;;) {\n\t\tstruct command_item *util;\n\n\t\ti = list_and_choose(&s, &commands, &main_loop_opts);\n\t\tif (i < 0 || i >= commands.items.nr)\n\t\t\tutil = NULL;\n\t\telse\n\t\t\tutil = commands.items.items[i].util;\n\n\t\tif (i == LIST_AND_CHOOSE_QUIT || (util && !util->command)) {\n\t\t\tprintf(_(\"Bye.\\n\"));\n\t\t\tres = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (util)\n\t\t\tres = util->command(&s, ps, &files, &opts);\n\t}\n\n\tprefix_item_list_clear(&files);\n\tstrbuf_release(&print_file_item_data.buf);\n\tstrbuf_release(&print_file_item_data.name);\n\tstrbuf_release(&print_file_item_data.index);\n\tstrbuf_release(&print_file_item_data.worktree);\n\tstrbuf_release(&header);\n\tprefix_item_list_clear(&commands);\n\tclear_add_i_state(&s);\n\n\treturn res;\n}\n",
        "imports": [
            "lockfile.h",
            "hash.h",
            "dir.h",
            "config.h",
            "repository.h",
            "prompt.h",
            "tree.h",
            "refs.h",
            "color.h",
            "hex.h",
            "revision.h",
            "diffcore.h",
            "gettext.h"
        ],
        "functions": [
            "if",
            "init_add_i_state",
            "clear_add_i_state",
            "is_valid_prefix",
            "run_add_i",
            "command_prompt_help",
            "add_file_item",
            "choose_prompt_help",
            "prefix_item_list_clear",
            "find_unique_prefixes",
            "find_unique"
        ],
        "variables": [
            "binary",
            "opt",
            "sorted_item",
            "add_flags",
            "fd",
            "prompt",
            "mode",
            "key",
            "header",
            "res",
            "i",
            "e1",
            "name",
            "tree",
            "FROM_WORKTREE",
            "sep",
            "r",
            "file_item",
            "d",
            "stat",
            "c",
            "item",
            "binary_count",
            "highlighted",
            "repo",
            "one",
            "count",
            "ps_selected",
            "ignore_dirty_submodules",
            "reset",
            "ent",
            "unmerged_count",
            "data",
            "len",
            "to",
            "format_callback_data",
            "dir",
            "singleton",
            "skip_unseen",
            "immediate",
            "files",
            "override_submodule_config",
            "args",
            "use_color",
            "input",
            "e2",
            "del",
            "last_lf",
            "hash",
            "help_color",
            "commands",
            "command",
            "color",
            "add",
            "util",
            "string",
            "nr",
            "output_format",
            "cmd",
            "unmerged",
            "diffopt",
            "only_names",
            "flags",
            "seen",
            "buf",
            "p",
            "format_callback",
            "s",
            "from",
            "choose",
            "index"
        ]
    },
    {
        "file_name": "add-patch.c",
        "language": "cpp",
        "source_code": "#define USE_THE_REPOSITORY_VARIABLE\n#define DISABLE_SIGN_COMPARE_WARNINGS\n\n#include \"git-compat-util.h\"\n#include \"add-interactive.h\"\n#include \"advice.h\"\n#include \"editor.h\"\n#include \"environment.h\"\n#include \"gettext.h\"\n#include \"object-name.h\"\n#include \"pager.h\"\n#include \"read-cache-ll.h\"\n#include \"repository.h\"\n#include \"strbuf.h\"\n#include \"sigchain.h\"\n#include \"run-command.h\"\n#include \"strvec.h\"\n#include \"pathspec.h\"\n#include \"color.h\"\n#include \"compat/terminal.h\"\n#include \"prompt.h\"\n\nenum prompt_mode_type {\n\tPROMPT_MODE_CHANGE = 0, PROMPT_DELETION, PROMPT_ADDITION, PROMPT_HUNK,\n\tPROMPT_MODE_MAX, /* must be last */\n};\n\nstruct patch_mode {\n\t/*\n\t * The magic constant 4 is chosen such that all patch modes\n\t * provide enough space for three command-line arguments followed by a\n\t * trailing `NULL`.\n\t */\n\tconst char *diff_cmd[4], *apply_args[4], *apply_check_args[4];\n\tunsigned is_reverse:1, index_only:1, apply_for_checkout:1;\n\tconst char *prompt_mode[PROMPT_MODE_MAX];\n\tconst char *edit_hunk_hint, *help_patch_text;\n};\n\nstatic struct patch_mode patch_mode_add = {\n\t.diff_cmd = { \"diff-files\", NULL },\n\t.apply_args = { \"--cached\", NULL },\n\t.apply_check_args = { \"--cached\", NULL },\n\t.prompt_mode = {\n\t\tN_(\"Stage mode change [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Stage deletion [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Stage addition [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Stage this hunk [y,n,q,a,d%s,?]? \")\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for staging.\"),\n\t.help_patch_text =\n\t\tN_(\"y - stage this hunk\\n\"\n\t\t   \"n - do not stage this hunk\\n\"\n\t\t   \"q - quit; do not stage this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - stage this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not stage this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\")\n};\n\nstatic struct patch_mode patch_mode_stash = {\n\t.diff_cmd = { \"diff-index\", \"HEAD\", NULL },\n\t.apply_args = { \"--cached\", NULL },\n\t.apply_check_args = { \"--cached\", NULL },\n\t.prompt_mode = {\n\t\tN_(\"Stash mode change [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Stash deletion [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Stash addition [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Stash this hunk [y,n,q,a,d%s,?]? \"),\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for stashing.\"),\n\t.help_patch_text =\n\t\tN_(\"y - stash this hunk\\n\"\n\t\t   \"n - do not stash this hunk\\n\"\n\t\t   \"q - quit; do not stash this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - stash this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not stash this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\"),\n};\n\nstatic struct patch_mode patch_mode_reset_head = {\n\t.diff_cmd = { \"diff-index\", \"--cached\", NULL },\n\t.apply_args = { \"-R\", \"--cached\", NULL },\n\t.apply_check_args = { \"-R\", \"--cached\", NULL },\n\t.is_reverse = 1,\n\t.index_only = 1,\n\t.prompt_mode = {\n\t\tN_(\"Unstage mode change [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Unstage deletion [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Unstage addition [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Unstage this hunk [y,n,q,a,d%s,?]? \"),\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for unstaging.\"),\n\t.help_patch_text =\n\t\tN_(\"y - unstage this hunk\\n\"\n\t\t   \"n - do not unstage this hunk\\n\"\n\t\t   \"q - quit; do not unstage this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - unstage this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not unstage this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\"),\n};\n\nstatic struct patch_mode patch_mode_reset_nothead = {\n\t.diff_cmd = { \"diff-index\", \"-R\", \"--cached\", NULL },\n\t.apply_args = { \"--cached\", NULL },\n\t.apply_check_args = { \"--cached\", NULL },\n\t.index_only = 1,\n\t.prompt_mode = {\n\t\tN_(\"Apply mode change to index [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply deletion to index [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply addition to index [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply this hunk to index [y,n,q,a,d%s,?]? \"),\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for applying.\"),\n\t.help_patch_text =\n\t\tN_(\"y - apply this hunk to index\\n\"\n\t\t   \"n - do not apply this hunk to index\\n\"\n\t\t   \"q - quit; do not apply this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - apply this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not apply this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\"),\n};\n\nstatic struct patch_mode patch_mode_checkout_index = {\n\t.diff_cmd = { \"diff-files\", NULL },\n\t.apply_args = { \"-R\", NULL },\n\t.apply_check_args = { \"-R\", NULL },\n\t.is_reverse = 1,\n\t.prompt_mode = {\n\t\tN_(\"Discard mode change from worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard deletion from worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard addition from worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard this hunk from worktree [y,n,q,a,d%s,?]? \"),\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for discarding.\"),\n\t.help_patch_text =\n\t\tN_(\"y - discard this hunk from worktree\\n\"\n\t\t   \"n - do not discard this hunk from worktree\\n\"\n\t\t   \"q - quit; do not discard this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - discard this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not discard this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\"),\n};\n\nstatic struct patch_mode patch_mode_checkout_head = {\n\t.diff_cmd = { \"diff-index\", NULL },\n\t.apply_for_checkout = 1,\n\t.apply_check_args = { \"-R\", NULL },\n\t.is_reverse = 1,\n\t.prompt_mode = {\n\t\tN_(\"Discard mode change from index and worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard deletion from index and worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard addition from index and worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard this hunk from index and worktree [y,n,q,a,d%s,?]? \"),\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for discarding.\"),\n\t.help_patch_text =\n\t\tN_(\"y - discard this hunk from index and worktree\\n\"\n\t\t   \"n - do not discard this hunk from index and worktree\\n\"\n\t\t   \"q - quit; do not discard this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - discard this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not discard this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\"),\n};\n\nstatic struct patch_mode patch_mode_checkout_nothead = {\n\t.diff_cmd = { \"diff-index\", \"-R\", NULL },\n\t.apply_for_checkout = 1,\n\t.apply_check_args = { NULL },\n\t.prompt_mode = {\n\t\tN_(\"Apply mode change to index and worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply deletion to index and worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply addition to index and worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply this hunk to index and worktree [y,n,q,a,d%s,?]? \"),\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for applying.\"),\n\t.help_patch_text =\n\t\tN_(\"y - apply this hunk to index and worktree\\n\"\n\t\t   \"n - do not apply this hunk to index and worktree\\n\"\n\t\t   \"q - quit; do not apply this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - apply this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not apply this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\"),\n};\n\nstatic struct patch_mode patch_mode_worktree_head = {\n\t.diff_cmd = { \"diff-index\", NULL },\n\t.apply_args = { \"-R\", NULL },\n\t.apply_check_args = { \"-R\", NULL },\n\t.is_reverse = 1,\n\t.prompt_mode = {\n\t\tN_(\"Discard mode change from worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard deletion from worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard addition from worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Discard this hunk from worktree [y,n,q,a,d%s,?]? \"),\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for discarding.\"),\n\t.help_patch_text =\n\t\tN_(\"y - discard this hunk from worktree\\n\"\n\t\t   \"n - do not discard this hunk from worktree\\n\"\n\t\t   \"q - quit; do not discard this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - discard this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not discard this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\"),\n};\n\nstatic struct patch_mode patch_mode_worktree_nothead = {\n\t.diff_cmd = { \"diff-index\", \"-R\", NULL },\n\t.apply_args = { NULL },\n\t.apply_check_args = { NULL },\n\t.prompt_mode = {\n\t\tN_(\"Apply mode change to worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply deletion to worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply addition to worktree [y,n,q,a,d%s,?]? \"),\n\t\tN_(\"Apply this hunk to worktree [y,n,q,a,d%s,?]? \"),\n\t},\n\t.edit_hunk_hint = N_(\"If the patch applies cleanly, the edited hunk \"\n\t\t\t     \"will immediately be marked for applying.\"),\n\t.help_patch_text =\n\t\tN_(\"y - apply this hunk to worktree\\n\"\n\t\t   \"n - do not apply this hunk to worktree\\n\"\n\t\t   \"q - quit; do not apply this hunk or any of the remaining \"\n\t\t\t\"ones\\n\"\n\t\t   \"a - apply this hunk and all later hunks in the file\\n\"\n\t\t   \"d - do not apply this hunk or any of the later hunks in \"\n\t\t\t\"the file\\n\"),\n};\n\nstruct hunk_header {\n\tunsigned long old_offset, old_count, new_offset, new_count;\n\t/*\n\t * Start/end offsets to the extra text after the second `@@` in the\n\t * hunk header, e.g. the function signature. This is expected to\n\t * include the newline.\n\t */\n\tsize_t extra_start, extra_end, colored_extra_start, colored_extra_end;\n\tunsigned suppress_colored_line_range:1;\n};\n\nstruct hunk {\n\tsize_t start, end, colored_start, colored_end, splittable_into;\n\tssize_t delta;\n\tenum { UNDECIDED_HUNK = 0, SKIP_HUNK, USE_HUNK } use;\n\tstruct hunk_header header;\n};\n\nstruct add_p_state {\n\tstruct add_i_state s;\n\tstruct strbuf answer, buf;\n\n\t/* parsed diff */\n\tstruct strbuf plain, colored;\n\tstruct file_diff {\n\t\tstruct hunk head;\n\t\tstruct hunk *hunk;\n\t\tsize_t hunk_nr, hunk_alloc;\n\t\tunsigned deleted:1, added:1, mode_change:1,binary:1;\n\t} *file_diff;\n\tsize_t file_diff_nr;\n\n\t/* patch mode */\n\tstruct patch_mode *mode;\n\tconst char *revision;\n};\n\nstatic void add_p_state_clear(struct add_p_state *s)\n{\n\tsize_t i;\n\n\tstrbuf_release(&s->answer);\n\tstrbuf_release(&s->buf);\n\tstrbuf_release(&s->plain);\n\tstrbuf_release(&s->colored);\n\tfor (i = 0; i < s->file_diff_nr; i++)\n\t\tfree(s->file_diff[i].hunk);\n\tfree(s->file_diff);\n\tclear_add_i_state(&s->s);\n}\n\n__attribute__((format (printf, 2, 3)))\nstatic void err(struct add_p_state *s, const char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tfputs(s->s.error_color, stdout);\n\tvprintf(fmt, args);\n\tputs(s->s.reset_color);\n\tva_end(args);\n}\n\nLAST_ARG_MUST_BE_NULL\nstatic void setup_child_process(struct add_p_state *s,\n\t\t\t\tstruct child_process *cp, ...)\n{\n\tva_list ap;\n\tconst char *arg;\n\n\tva_start(ap, cp);\n\twhile ((arg = va_arg(ap, const char *)))\n\t\tstrvec_push(&cp->args, arg);\n\tva_end(ap);\n\n\tcp->git_cmd = 1;\n\tstrvec_pushf(&cp->env,\n\t\t     INDEX_ENVIRONMENT \"=%s\", s->s.r->index_file);\n}\n\nstatic int parse_range(const char **p,\n\t\t       unsigned long *offset, unsigned long *count)\n{\n\tchar *pend;\n\n\t*offset = strtoul(*p, &pend, 10);\n\tif (pend == *p)\n\t\treturn -1;\n\tif (*pend != ',') {\n\t\t*count = 1;\n\t\t*p = pend;\n\t\treturn 0;\n\t}\n\t*count = strtoul(pend + 1, (char **)p, 10);\n\treturn *p == pend + 1 ? -1 : 0;\n}\n\nstatic int parse_hunk_header(struct add_p_state *s, struct hunk *hunk)\n{\n\tstruct hunk_header *header = &hunk->header;\n\tconst char *line = s->plain.buf + hunk->start, *p = line;\n\tchar *eol = memchr(p, '\\n', s->plain.len - hunk->start);\n\n\tif (!eol)\n\t\teol = s->plain.buf + s->plain.len;\n\n\tif (!skip_prefix(p, \"@@ -\", &p) ||\n\t    parse_range(&p, &header->old_offset, &header->old_count) < 0 ||\n\t    !skip_prefix(p, \" +\", &p) ||\n\t    parse_range(&p, &header->new_offset, &header->new_count) < 0 ||\n\t    !skip_prefix(p, \" @@\", &p))\n\t\treturn error(_(\"could not parse hunk header '%.*s'\"),\n\t\t\t     (int)(eol - line), line);\n\n\thunk->start = eol - s->plain.buf + (*eol == '\\n');\n\theader->extra_start = p - s->plain.buf;\n\theader->extra_end = hunk->start;\n\n\tif (!s->colored.len) {\n\t\theader->colored_extra_start = header->colored_extra_end = 0;\n\t\treturn 0;\n\t}\n\n\t/* Now find the extra text in the colored diff */\n\tline = s->colored.buf + hunk->colored_start;\n\teol = memchr(line, '\\n', s->colored.len - hunk->colored_start);\n\tif (!eol)\n\t\teol = s->colored.buf + s->colored.len;\n\tp = memmem(line, eol - line, \"@@ -\", 4);\n\tif (p && (p = memmem(p + 4, eol - p - 4, \" @@\", 3))) {\n\t\theader->colored_extra_start = p + 3 - s->colored.buf;\n\t} else {\n\t\t/* could not parse colored hunk header, leave as-is */\n\t\theader->colored_extra_start = hunk->colored_start;\n\t\theader->suppress_colored_line_range = 1;\n\t}\n\thunk->colored_start = eol - s->colored.buf + (*eol == '\\n');\n\theader->colored_extra_end = hunk->colored_start;\n\n\treturn 0;\n}\n\nstatic int is_octal(const char *p, size_t len)\n{\n\tif (!len)\n\t\treturn 0;\n\n\twhile (len--)\n\t\tif (*p < '0' || *(p++) > '7')\n\t\t\treturn 0;\n\treturn 1;\n}\n\nstatic void complete_file(char marker, struct hunk *hunk)\n{\n\tif (marker == '-' || marker == '+')\n\t\t/*\n\t\t * Last hunk ended in non-context line (i.e. it\n\t\t * appended lines to the file, so there are no\n\t\t * trailing context lines).\n\t\t */\n\t\thunk->splittable_into++;\n}\n\n/* Empty context lines may omit the leading ' ' */\nstatic int normalize_marker(const char *p)\n{\n\treturn p[0] == '\\n' || (p[0] == '\\r' && p[1] == '\\n') ? ' ' : p[0];\n}\n\nstatic int parse_diff(struct add_p_state *s, const struct pathspec *ps)\n{\n\tstruct strvec args = STRVEC_INIT;\n\tconst char *diff_algorithm = s->s.interactive_diff_algorithm;\n\tstruct strbuf *plain = &s->plain, *colored = NULL;\n\tstruct child_process cp = CHILD_PROCESS_INIT;\n\tchar *p, *pend, *colored_p = NULL, *colored_pend = NULL, marker = '\\0';\n\tsize_t file_diff_alloc = 0, i, color_arg_index;\n\tstruct file_diff *file_diff = NULL;\n\tstruct hunk *hunk = NULL;\n\tint res;\n\n\tstrvec_pushv(&args, s->mode->diff_cmd);\n\tif (diff_algorithm)\n\t\tstrvec_pushf(&args, \"--diff-algorithm=%s\", diff_algorithm);\n\tif (s->revision) {\n\t\tstruct object_id oid;\n\t\tstrvec_push(&args,\n\t\t\t    /* could be on an unborn branch */\n\t\t\t    !strcmp(\"HEAD\", s->revision) &&\n\t\t\t    repo_get_oid(the_repository, \"HEAD\", &oid) ?\n\t\t\t    empty_tree_oid_hex(the_repository->hash_algo) : s->revision);\n\t}\n\tcolor_arg_index = args.nr;\n\t/* Use `--no-color` explicitly, just in case `diff.color = always`. */\n\tstrvec_pushl(&args, \"--no-color\", \"--ignore-submodules=dirty\", \"-p\",\n\t\t     \"--\", NULL);\n\tfor (i = 0; i < ps->nr; i++)\n\t\tstrvec_push(&args, ps->items[i].original);\n\n\tsetup_child_process(s, &cp, NULL);\n\tstrvec_pushv(&cp.args, args.v);\n\tres = capture_command(&cp, plain, 0);\n\tif (res) {\n\t\tstrvec_clear(&args);\n\t\treturn error(_(\"could not parse diff\"));\n\t}\n\tif (!plain->len) {\n\t\tstrvec_clear(&args);\n\t\treturn 0;\n\t}\n\tstrbuf_complete_line(plain);\n\n\tif (want_color_fd(1, -1)) {\n\t\tstruct child_process colored_cp = CHILD_PROCESS_INIT;\n\t\tconst char *diff_filter = s->s.interactive_diff_filter;\n\n\t\tsetup_child_process(s, &colored_cp, NULL);\n\t\txsnprintf((char *)args.v[color_arg_index], 8, \"--color\");\n\t\tstrvec_pushv(&colored_cp.args, args.v);\n\t\tcolored = &s->colored;\n\t\tres = capture_command(&colored_cp, colored, 0);\n\t\tstrvec_clear(&args);\n\t\tif (res)\n\t\t\treturn error(_(\"could not parse colored diff\"));\n\n\t\tif (diff_filter) {\n\t\t\tstruct child_process filter_cp = CHILD_PROCESS_INIT;\n\n\t\t\tsetup_child_process(s, &filter_cp,\n\t\t\t\t\t    diff_filter, NULL);\n\t\t\tfilter_cp.git_cmd = 0;\n\t\t\tfilter_cp.use_shell = 1;\n\t\t\tstrbuf_reset(&s->buf);\n\t\t\tif (pipe_command(&filter_cp,\n\t\t\t\t\t colored->buf, colored->len,\n\t\t\t\t\t &s->buf, colored->len,\n\t\t\t\t\t NULL, 0) < 0)\n\t\t\t\treturn error(_(\"failed to run '%s'\"),\n\t\t\t\t\t     diff_filter);\n\t\t\tstrbuf_swap(colored, &s->buf);\n\t\t}\n\n\t\tstrbuf_complete_line(colored);\n\t\tcolored_p = colored->buf;\n\t\tcolored_pend = colored_p + colored->len;\n\t}\n\tstrvec_clear(&args);\n\n\t/* parse files and hunks */\n\tp = plain->buf;\n\tpend = p + plain->len;\n\twhile (p != pend) {\n\t\tchar *eol = memchr(p, '\\n', pend - p);\n\t\tconst char *deleted = NULL, *mode_change = NULL;\n\t\tchar ch = normalize_marker(p);\n\n\t\tif (!eol)\n\t\t\teol = pend;\n\n\t\tif (starts_with(p, \"diff \") ||\n\t\t    starts_with(p, \"* Unmerged path \")) {\n\t\t\tcomplete_file(marker, hunk);\n\t\t\tALLOC_GROW_BY(s->file_diff, s->file_diff_nr, 1,\n\t\t\t\t   file_diff_alloc);\n\t\t\tfile_diff = s->file_diff + s->file_diff_nr - 1;\n\t\t\thunk = &file_diff->head;\n\t\t\thunk->start = p - plain->buf;\n\t\t\tif (colored_p)\n\t\t\t\thunk->colored_start = colored_p - colored->buf;\n\t\t\tmarker = '\\0';\n\t\t} else if (p == plain->buf)\n\t\t\tBUG(\"diff starts with unexpected line:\\n\"\n\t\t\t    \"%.*s\\n\", (int)(eol - p), p);\n\t\telse if (file_diff->deleted)\n\t\t\t; /* keep the rest of the file in a single \"hunk\" */\n\t\telse if (starts_with(p, \"@@ \") ||\n\t\t\t (hunk == &file_diff->head &&\n\t\t\t  (skip_prefix(p, \"deleted file\", &deleted)))) {\n\t\t\tif (marker == '-' || marker == '+')\n\t\t\t\t/*\n\t\t\t\t * Should not happen; previous hunk did not end\n\t\t\t\t * in a context line? Handle it anyway.\n\t\t\t\t */\n\t\t\t\thunk->splittable_into++;\n\n\t\t\tALLOC_GROW_BY(file_diff->hunk, file_diff->hunk_nr, 1,\n\t\t\t\t   file_diff->hunk_alloc);\n\t\t\thunk = file_diff->hunk + file_diff->hunk_nr - 1;\n\n\t\t\thunk->start = p - plain->buf;\n\t\t\tif (colored)\n\t\t\t\thunk->colored_start = colored_p - colored->buf;\n\n\t\t\tif (deleted)\n\t\t\t\tfile_diff->deleted = 1;\n\t\t\telse if (parse_hunk_header(s, hunk) < 0)\n\t\t\t\treturn -1;\n\n\t\t\t/*\n\t\t\t * Start counting into how many hunks this one can be\n\t\t\t * split\n\t\t\t */\n\t\t\tmarker = ch;\n\t\t} else if (hunk == &file_diff->head &&\n\t\t\t   starts_with(p, \"new file\")) {\n\t\t\tfile_diff->added = 1;\n\t\t} else if (hunk == &file_diff->head &&\n\t\t\t   skip_prefix(p, \"old mode \", &mode_change) &&\n\t\t\t   is_octal(mode_change, eol - mode_change)) {\n\t\t\tif (file_diff->mode_change)\n\t\t\t\tBUG(\"double mode change?\\n\\n%.*s\",\n\t\t\t\t    (int)(eol - plain->buf), plain->buf);\n\t\t\tif (file_diff->hunk_nr)\n\t\t\t\tBUG(\"mode change in the middle?\\n\\n%.*s\",\n\t\t\t\t    (int)(eol - plain->buf), plain->buf);\n\n\t\t\t/*\n\t\t\t * Do *not* change `hunk`: the mode change pseudo-hunk\n\t\t\t * is _part of_ the header \"hunk\".\n\t\t\t */\n\t\t\tfile_diff->mode_change = 1;\n\t\t\tALLOC_GROW_BY(file_diff->hunk, file_diff->hunk_nr, 1,\n\t\t\t\t   file_diff->hunk_alloc);\n\t\t\tfile_diff->hunk->start = p - plain->buf;\n\t\t\tif (colored_p)\n\t\t\t\tfile_diff->hunk->colored_start =\n\t\t\t\t\tcolored_p - colored->buf;\n\t\t} else if (hunk == &file_diff->head &&\n\t\t\t   skip_prefix(p, \"new mode \", &mode_change) &&\n\t\t\t   is_octal(mode_change, eol - mode_change)) {\n\n\t\t\t/*\n\t\t\t * Extend the \"mode change\" pseudo-hunk to include also\n\t\t\t * the \"new mode\" line.\n\t\t\t */\n\t\t\tif (!file_diff->mode_change)\n\t\t\t\tBUG(\"'new mode' without 'old mode'?\\n\\n%.*s\",\n\t\t\t\t    (int)(eol - plain->buf), plain->buf);\n\t\t\tif (file_diff->hunk_nr != 1)\n\t\t\t\tBUG(\"mode change in the middle?\\n\\n%.*s\",\n\t\t\t\t    (int)(eol - plain->buf), plain->buf);\n\t\t\tif (p - plain->buf != file_diff->hunk->end)\n\t\t\t\tBUG(\"'new mode' does not immediately follow \"\n\t\t\t\t    \"'old mode'?\\n\\n%.*s\",\n\t\t\t\t    (int)(eol - plain->buf), plain->buf);\n\t\t} else if (hunk == &file_diff->head &&\n\t\t\t   starts_with(p, \"Binary files \"))\n\t\t\tfile_diff->binary = 1;\n\n\t\tif (!!file_diff->deleted + !!file_diff->added +\n\t\t    !!file_diff->mode_change > 1)\n\t\t\tBUG(\"diff can only contain delete *or* add *or* a \"\n\t\t\t    \"mode change?!?\\n%.*s\",\n\t\t\t    (int)(eol - (plain->buf + file_diff->head.start)),\n\t\t\t    plain->buf + file_diff->head.start);\n\n\t\tif ((marker == '-' || marker == '+') && ch == ' ')\n\t\t\thunk->splittable_into++;\n\t\tif (marker && ch != '\\\\')\n\t\t\tmarker = ch;\n\n\t\tp = eol == pend ? pend : eol + 1;\n\t\thunk->end = p - plain->buf;\n\n\t\tif (colored) {\n\t\t\tchar *colored_eol = memchr(colored_p, '\\n',\n\t\t\t\t\t\t   colored_pend - colored_p);\n\t\t\tif (colored_eol)\n\t\t\t\tcolored_p = colored_eol + 1;\n\t\t\telse if (p != pend)\n\t\t\t\t/* non-colored has more lines? */\n\t\t\t\tgoto mismatched_output;\n\t\t\telse if (colored_p == colored_pend)\n\t\t\t\t/* last line has no matching colored one? */\n\t\t\t\tgoto mismatched_output;\n\t\t\telse\n\t\t\t\tcolored_p = colored_pend;\n\n\t\t\thunk->colored_end = colored_p - colored->buf;\n\t\t}\n\n\t\tif (mode_change) {\n\t\t\tif (file_diff->hunk_nr != 1)\n\t\t\t\tBUG(\"mode change in hunk #%d???\",\n\t\t\t\t    (int)file_diff->hunk_nr);\n\t\t\t/* Adjust the end of the \"mode change\" pseudo-hunk */\n\t\t\tfile_diff->hunk->end = hunk->end;\n\t\t\tif (colored)\n\t\t\t\tfile_diff->hunk->colored_end = hunk->colored_end;\n\t\t}\n\t}\n\tcomplete_file(marker, hunk);\n\n\t/* non-colored shorter than colored? */\n\tif (colored_p != colored_pend) {\nmismatched_output:\n\t\terror(_(\"mismatched output from interactive.diffFilter\"));\n\t\tadvise(_(\"Your filter must maintain a one-to-one correspondence\\n\"\n\t\t\t \"between its input and output lines.\"));\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic size_t find_next_line(struct strbuf *sb, size_t offset)\n{\n\tchar *eol;\n\n\tif (offset >= sb->len)\n\t\tBUG(\"looking for next line beyond buffer (%d >= %d)\\n%s\",\n\t\t    (int)offset, (int)sb->len, sb->buf);\n\n\teol = memchr(sb->buf + offset, '\\n', sb->len - offset);\n\tif (!eol)\n\t\treturn sb->len;\n\treturn eol - sb->buf + 1;\n}\n\nstatic void render_hunk(struct add_p_state *s, struct hunk *hunk,\n\t\t\tssize_t delta, int colored, struct strbuf *out)\n{\n\tstruct hunk_header *header = &hunk->header;\n\n\tif (hunk->header.old_offset != 0 || hunk->header.new_offset != 0) {\n\t\t/*\n\t\t * Generate the hunk header dynamically, except for special\n\t\t * hunks (such as the diff header).\n\t\t */\n\t\tconst char *p;\n\t\tsize_t len;\n\t\tunsigned long old_offset = header->old_offset;\n\t\tunsigned long new_offset = header->new_offset;\n\n\t\tif (!colored) {\n\t\t\tp = s->plain.buf + header->extra_start;\n\t\t\tlen = header->extra_end - header->extra_start;\n\t\t} else if (header->suppress_colored_line_range) {\n\t\t\tstrbuf_add(out,\n\t\t\t\t   s->colored.buf + header->colored_extra_start,\n\t\t\t\t   header->colored_extra_end -\n\t\t\t\t   header->colored_extra_start);\n\n\t\t\tstrbuf_add(out, s->colored.buf + hunk->colored_start,\n\t\t\t\t   hunk->colored_end - hunk->colored_start);\n\t\t\treturn;\n\t\t} else {\n\t\t\tstrbuf_addstr(out, s->s.fraginfo_color);\n\t\t\tp = s->colored.buf + header->colored_extra_start;\n\t\t\tlen = header->colored_extra_end\n\t\t\t\t- header->colored_extra_start;\n\t\t}\n\n\t\tif (s->mode->is_reverse)\n\t\t\told_offset -= delta;\n\t\telse\n\t\t\tnew_offset += delta;\n\n\t\tstrbuf_addf(out, \"@@ -%lu\", old_offset);\n\t\tif (header->old_count != 1)\n\t\t\tstrbuf_addf(out, \",%lu\", header->old_count);\n\t\tstrbuf_addf(out, \" +%lu\", new_offset);\n\t\tif (header->new_count != 1)\n\t\t\tstrbuf_addf(out, \",%lu\", header->new_count);\n\t\tstrbuf_addstr(out, \" @@\");\n\n\t\tif (len)\n\t\t\tstrbuf_add(out, p, len);\n\t\telse if (colored)\n\t\t\tstrbuf_addf(out, \"%s\\n\", s->s.reset_color);\n\t\telse\n\t\t\tstrbuf_addch(out, '\\n');\n\t}\n\n\tif (colored)\n\t\tstrbuf_add(out, s->colored.buf + hunk->colored_start,\n\t\t\t   hunk->colored_end - hunk->colored_start);\n\telse\n\t\tstrbuf_add(out, s->plain.buf + hunk->start,\n\t\t\t   hunk->end - hunk->start);\n}\n\nstatic void render_diff_header(struct add_p_state *s,\n\t\t\t       struct file_diff *file_diff, int colored,\n\t\t\t       struct strbuf *out)\n{\n\t/*\n\t * If there was a mode change, the first hunk is a pseudo hunk that\n\t * corresponds to the mode line in the header. If the user did not want\n\t * to stage that \"hunk\", we actually have to cut it out from the header.\n\t */\n\tint skip_mode_change =\n\t\tfile_diff->mode_change && file_diff->hunk->use != USE_HUNK;\n\tstruct hunk *head = &file_diff->head, *first = file_diff->hunk;\n\n\tif (!skip_mode_change) {\n\t\trender_hunk(s, head, 0, colored, out);\n\t\treturn;\n\t}\n\n\tif (colored) {\n\t\tconst char *p = s->colored.buf;\n\n\t\tstrbuf_add(out, p + head->colored_start,\n\t\t\t    first->colored_start - head->colored_start);\n\t\tstrbuf_add(out, p + first->colored_end,\n\t\t\t    head->colored_end - first->colored_end);\n\t} else {\n\t\tconst char *p = s->plain.buf;\n\n\t\tstrbuf_add(out, p + head->start, first->start - head->start);\n\t\tstrbuf_add(out, p + first->end, head->end - first->end);\n\t}\n}\n\n/* Coalesce hunks again that were split */\nstatic int merge_hunks(struct add_p_state *s, struct file_diff *file_diff,\n\t\t       size_t *hunk_index, int use_all, struct hunk *merged)\n{\n\tsize_t i = *hunk_index, delta;\n\tstruct hunk *hunk = file_diff->hunk + i;\n\t/* `header` corresponds to the merged hunk */\n\tstruct hunk_header *header = &merged->header, *next;\n\n\tif (!use_all && hunk->use != USE_HUNK)\n\t\treturn 0;\n\n\t*merged = *hunk;\n\t/* We simply skip the colored part (if any) when merging hunks */\n\tmerged->colored_start = merged->colored_end = 0;\n\n\tfor (; i + 1 < file_diff->hunk_nr; i++) {\n\t\thunk++;\n\t\tnext = &hunk->header;\n\n\t\t/*\n\t\t * Stop merging hunks when:\n\t\t *\n\t\t * - the hunk is not selected for use, or\n\t\t * - the hunk does not overlap with the already-merged hunk(s)\n\t\t */\n\t\tif ((!use_all && hunk->use != USE_HUNK) ||\n\t\t    header->new_offset >= next->new_offset + merged->delta ||\n\t\t    header->new_offset + header->new_count\n\t\t    < next->new_offset + merged->delta)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If the hunks were not edited, and overlap, we can simply\n\t\t * extend the line range.\n\t\t */\n\t\tif (merged->start < hunk->start && merged->end > hunk->start) {\n\t\t\tmerged->end = hunk->end;\n\t\t\tmerged->colored_end = hunk->colored_end;\n\t\t\tdelta = 0;\n\t\t} else {\n\t\t\tconst char *plain = s->plain.buf;\n\t\t\tsize_t  overlapping_line_count = header->new_offset\n\t\t\t\t+ header->new_count - merged->delta\n\t\t\t\t- next->new_offset;\n\t\t\tsize_t overlap_end = hunk->start;\n\t\t\tsize_t overlap_start = overlap_end;\n\t\t\tsize_t overlap_next, len, j;\n\n\t\t\t/*\n\t\t\t * One of the hunks was edited: the modified hunk was\n\t\t\t * appended to the strbuf `s->plain`.\n\t\t\t *\n\t\t\t * Let's ensure that at least the last context line of\n\t\t\t * the first hunk overlaps with the corresponding line\n\t\t\t * of the second hunk, and then merge.\n\t\t\t */\n\t\t\tfor (j = 0; j < overlapping_line_count; j++) {\n\t\t\t\toverlap_next = find_next_line(&s->plain,\n\t\t\t\t\t\t\t      overlap_end);\n\n\t\t\t\tif (overlap_next > hunk->end)\n\t\t\t\t\tBUG(\"failed to find %d context lines \"\n\t\t\t\t\t    \"in:\\n%.*s\",\n\t\t\t\t\t    (int)overlapping_line_count,\n\t\t\t\t\t    (int)(hunk->end - hunk->start),\n\t\t\t\t\t    plain + hunk->start);\n\n\t\t\t\tif (normalize_marker(&plain[overlap_end]) != ' ')\n\t\t\t\t\treturn error(_(\"expected context line \"\n\t\t\t\t\t\t       \"#%d in\\n%.*s\"),\n\t\t\t\t\t\t     (int)(j + 1),\n\t\t\t\t\t\t     (int)(hunk->end\n\t\t\t\t\t\t\t   - hunk->start),\n\t\t\t\t\t\t     plain + hunk->start);\n\n\t\t\t\toverlap_start = overlap_end;\n\t\t\t\toverlap_end = overlap_next;\n\t\t\t}\n\t\t\tlen = overlap_end - overlap_start;\n\n\t\t\tif (len > merged->end - merged->start ||\n\t\t\t    memcmp(plain + merged->end - len,\n\t\t\t\t   plain + overlap_start, len))\n\t\t\t\treturn error(_(\"hunks do not overlap:\\n%.*s\\n\"\n\t\t\t\t\t       \"\\tdoes not end with:\\n%.*s\"),\n\t\t\t\t\t     (int)(merged->end - merged->start),\n\t\t\t\t\t     plain + merged->start,\n\t\t\t\t\t     (int)len, plain + overlap_start);\n\n\t\t\t/*\n\t\t\t * Since the start-end ranges are not adjacent, we\n\t\t\t * cannot simply take the union of the ranges. To\n\t\t\t * address that, we temporarily append the union of the\n\t\t\t * lines to the `plain` strbuf.\n\t\t\t */\n\t\t\tif (merged->end != s->plain.len) {\n\t\t\t\tsize_t start = s->plain.len;\n\n\t\t\t\tstrbuf_add(&s->plain, plain + merged->start,\n\t\t\t\t\t   merged->end - merged->start);\n\t\t\t\tplain = s->plain.buf;\n\t\t\t\tmerged->start = start;\n\t\t\t\tmerged->end = s->plain.len;\n\t\t\t}\n\n\t\t\tstrbuf_add(&s->plain,\n\t\t\t\t   plain + overlap_end,\n\t\t\t\t   hunk->end - overlap_end);\n\t\t\tmerged->end = s->plain.len;\n\t\t\tmerged->splittable_into += hunk->splittable_into;\n\t\t\tdelta = merged->delta;\n\t\t\tmerged->delta += hunk->delta;\n\t\t}\n\n\t\theader->old_count = next->old_offset + next->old_count\n\t\t\t- header->old_offset;\n\t\theader->new_count = next->new_offset + delta\n\t\t\t+ next->new_count - header->new_offset;\n\t}\n\n\tif (i == *hunk_index)\n\t\treturn 0;\n\n\t*hunk_index = i;\n\treturn 1;\n}\n\nstatic void reassemble_patch(struct add_p_state *s,\n\t\t\t     struct file_diff *file_diff, int use_all,\n\t\t\t     struct strbuf *out)\n{\n\tstruct hunk *hunk;\n\tsize_t save_len = s->plain.len, i;\n\tssize_t delta = 0;\n\n\trender_diff_header(s, file_diff, 0, out);\n\n\tfor (i = file_diff->mode_change; i < file_diff->hunk_nr; i++) {\n\t\tstruct hunk merged = { 0 };\n\n\t\thunk = file_diff->hunk + i;\n\t\tif (!use_all && hunk->use != USE_HUNK)\n\t\t\tdelta += hunk->header.old_count\n\t\t\t\t- hunk->header.new_count;\n\t\telse {\n\t\t\t/* merge overlapping hunks into a temporary hunk */\n\t\t\tif (merge_hunks(s, file_diff, &i, use_all, &merged))\n\t\t\t\thunk = &merged;\n\n\t\t\trender_hunk(s, hunk, delta, 0, out);\n\n\t\t\t/*\n\t\t\t * In case `merge_hunks()` used `plain` as a scratch\n\t\t\t * pad (this happens when an edited hunk had to be\n\t\t\t * coalesced with another hunk).\n\t\t\t */\n\t\t\tstrbuf_setlen(&s->plain, save_len);\n\n\t\t\tdelta += hunk->delta;\n\t\t}\n\t}\n}\n\nstatic int split_hunk(struct add_p_state *s, struct file_diff *file_diff,\n\t\t       size_t hunk_index)\n{\n\tint colored = !!s->colored.len, first = 1;\n\tstruct hunk *hunk = file_diff->hunk + hunk_index;\n\tsize_t splittable_into;\n\tsize_t end, colored_end, current, colored_current = 0, context_line_count;\n\tstruct hunk_header remaining, *header;\n\tchar marker, ch;\n\n\tif (hunk_index >= file_diff->hunk_nr)\n\t\tBUG(\"invalid hunk index: %d (must be >= 0 and < %d)\",\n\t\t    (int)hunk_index, (int)file_diff->hunk_nr);\n\n\tif (hunk->splittable_into < 2)\n\t\treturn 0;\n\tsplittable_into = hunk->splittable_into;\n\n\tend = hunk->end;\n\tcolored_end = hunk->colored_end;\n\n\tremaining = hunk->header;\n\n\tfile_diff->hunk_nr += splittable_into - 1;\n\tALLOC_GROW(file_diff->hunk, file_diff->hunk_nr, file_diff->hunk_alloc);\n\tif (hunk_index + splittable_into < file_diff->hunk_nr)\n\t\tmemmove(file_diff->hunk + hunk_index + splittable_into,\n\t\t\tfile_diff->hunk + hunk_index + 1,\n\t\t\t(file_diff->hunk_nr - hunk_index - splittable_into)\n\t\t\t* sizeof(*hunk));\n\thunk = file_diff->hunk + hunk_index;\n\thunk->splittable_into = 1;\n\tmemset(hunk + 1, 0, (splittable_into - 1) * sizeof(*hunk));\n\n\theader = &hunk->header;\n\theader->old_count = header->new_count = 0;\n\n\tcurrent = hunk->start;\n\tif (colored)\n\t\tcolored_current = hunk->colored_start;\n\tmarker = '\\0';\n\tcontext_line_count = 0;\n\n\twhile (splittable_into > 1) {\n\t\tch = normalize_marker(&s->plain.buf[current]);\n\n\t\tif (!ch)\n\t\t\tBUG(\"buffer overrun while splitting hunks\");\n\n\t\t/*\n\t\t * Is this the first context line after a chain of +/- lines?\n\t\t * Then record the start of the next split hunk.\n\t\t */\n\t\tif ((marker == '-' || marker == '+') && ch == ' ') {\n\t\t\tfirst = 0;\n\t\t\thunk[1].start = current;\n\t\t\tif (colored)\n\t\t\t\thunk[1].colored_start = colored_current;\n\t\t\tcontext_line_count = 0;\n\t\t}\n\n\t\t/*\n\t\t * Was the previous line a +/- one? Alternatively, is this the\n\t\t * first line (and not a +/- one)?\n\t\t *\n\t\t * Then just increment the appropriate counter and continue\n\t\t * with the next line.\n\t\t */\n\t\tif (marker != ' ' || (ch != '-' && ch != '+')) {\nnext_hunk_line:\n\t\t\t/* Comment lines are attached to the previous line */\n\t\t\tif (ch == '\\\\')\n\t\t\t\tch = marker ? marker : ' ';\n\n\t\t\t/* current hunk not done yet */\n\t\t\tif (ch == ' ')\n\t\t\t\tcontext_line_count++;\n\t\t\telse if (ch == '-')\n\t\t\t\theader->old_count++;\n\t\t\telse if (ch == '+')\n\t\t\t\theader->new_count++;\n\t\t\telse\n\t\t\t\tBUG(\"unhandled diff marker: '%c'\", ch);\n\t\t\tmarker = ch;\n\t\t\tcurrent = find_next_line(&s->plain, current);\n\t\t\tif (colored)\n\t\t\t\tcolored_current =\n\t\t\t\t\tfind_next_line(&s->colored,\n\t\t\t\t\t\t       colored_current);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We got us the start of a new hunk!\n\t\t *\n\t\t * This is a context line, so it is shared with the previous\n\t\t * hunk, if any.\n\t\t */\n\n\t\tif (first) {\n\t\t\tif (header->old_count || header->new_count)\n\t\t\t\tBUG(\"counts are off: %d/%d\",\n\t\t\t\t    (int)header->old_count,\n\t\t\t\t    (int)header->new_count);\n\n\t\t\theader->old_count = context_line_count;\n\t\t\theader->new_count = context_line_count;\n\t\t\tcontext_line_count = 0;\n\t\t\tfirst = 0;\n\t\t\tgoto next_hunk_line;\n\t\t}\n\n\t\tremaining.old_offset += header->old_count;\n\t\tremaining.old_count -= header->old_count;\n\t\tremaining.new_offset += header->new_count;\n\t\tremaining.new_count -= header->new_count;\n\n\t\t/* initialize next hunk header's offsets */\n\t\thunk[1].header.old_offset =\n\t\t\theader->old_offset + header->old_count;\n\t\thunk[1].header.new_offset =\n\t\t\theader->new_offset + header->new_count;\n\n\t\t/* add one split hunk */\n\t\theader->old_count += context_line_count;\n\t\theader->new_count += context_line_count;\n\n\t\thunk->end = current;\n\t\tif (colored)\n\t\t\thunk->colored_end = colored_current;\n\n\t\thunk++;\n\t\thunk->splittable_into = 1;\n\t\thunk->use = hunk[-1].use;\n\t\theader = &hunk->header;\n\n\t\theader->old_count = header->new_count = context_line_count;\n\t\tcontext_line_count = 0;\n\n\t\tsplittable_into--;\n\t\tmarker = ch;\n\t}\n\n\t/* last hunk simply gets the rest */\n\tif (header->old_offset != remaining.old_offset)\n\t\tBUG(\"miscounted old_offset: %lu != %lu\",\n\t\t    header->old_offset, remaining.old_offset);\n\tif (header->new_offset != remaining.new_offset)\n\t\tBUG(\"miscounted new_offset: %lu != %lu\",\n\t\t    header->new_offset, remaining.new_offset);\n\theader->old_count = remaining.old_count;\n\theader->new_count = remaining.new_count;\n\thunk->end = end;\n\tif (colored)\n\t\thunk->colored_end = colored_end;\n\n\treturn 0;\n}\n\nstatic void recolor_hunk(struct add_p_state *s, struct hunk *hunk)\n{\n\tconst char *plain = s->plain.buf;\n\tsize_t current, eol, next;\n\n\tif (!s->colored.len)\n\t\treturn;\n\n\thunk->colored_start = s->colored.len;\n\tfor (current = hunk->start; current < hunk->end; ) {\n\t\tfor (eol = current; eol < hunk->end; eol++)\n\t\t\tif (plain[eol] == '\\n')\n\t\t\t\tbreak;\n\t\tnext = eol + (eol < hunk->end);\n\t\tif (eol > current && plain[eol - 1] == '\\r')\n\t\t\teol--;\n\n\t\tstrbuf_addstr(&s->colored,\n\t\t\t      plain[current] == '-' ?\n\t\t\t      s->s.file_old_color :\n\t\t\t      plain[current] == '+' ?\n\t\t\t      s->s.file_new_color :\n\t\t\t      s->s.context_color);\n\t\tstrbuf_add(&s->colored, plain + current, eol - current);\n\t\tstrbuf_addstr(&s->colored, s->s.reset_color);\n\t\tif (next > eol)\n\t\t\tstrbuf_add(&s->colored, plain + eol, next - eol);\n\t\tcurrent = next;\n\t}\n\thunk->colored_end = s->colored.len;\n}\n\nstatic int edit_hunk_manually(struct add_p_state *s, struct hunk *hunk)\n{\n\tsize_t i;\n\n\tstrbuf_reset(&s->buf);\n\tstrbuf_commented_addf(&s->buf, comment_line_str,\n\t\t\t      _(\"Manual hunk edit mode -- see bottom for \"\n\t\t\t\t\"a quick guide.\\n\"));\n\trender_hunk(s, hunk, 0, 0, &s->buf);\n\tstrbuf_commented_addf(&s->buf, comment_line_str,\n\t\t\t      _(\"---\\n\"\n\t\t\t\t\"To remove '%c' lines, make them ' ' lines \"\n\t\t\t\t\"(context).\\n\"\n\t\t\t\t\"To remove '%c' lines, delete them.\\n\"\n\t\t\t\t\"Lines starting with %s will be removed.\\n\"),\n\t\t\t      s->mode->is_reverse ? '+' : '-',\n\t\t\t      s->mode->is_reverse ? '-' : '+',\n\t\t\t      comment_line_str);\n\tstrbuf_commented_addf(&s->buf, comment_line_str, \"%s\",\n\t\t\t      _(s->mode->edit_hunk_hint));\n\t/*\n\t * TRANSLATORS: 'it' refers to the patch mentioned in the previous\n\t * messages.\n\t */\n\tstrbuf_commented_addf(&s->buf, comment_line_str,\n\t\t\t      _(\"If it does not apply cleanly, you will be \"\n\t\t\t\t\"given an opportunity to\\n\"\n\t\t\t\t\"edit again.  If all lines of the hunk are \"\n\t\t\t\t\"removed, then the edit is\\n\"\n\t\t\t\t\"aborted and the hunk is left unchanged.\\n\"));\n\n\tif (strbuf_edit_interactively(the_repository, &s->buf,\n\t\t\t\t      \"addp-hunk-edit.diff\", NULL) < 0)\n\t\treturn -1;\n\n\t/* strip out commented lines */\n\thunk->start = s->plain.len;\n\tfor (i = 0; i < s->buf.len; ) {\n\t\tsize_t next = find_next_line(&s->buf, i);\n\n\t\tif (!starts_with(s->buf.buf + i, comment_line_str))\n\t\t\tstrbuf_add(&s->plain, s->buf.buf + i, next - i);\n\t\ti = next;\n\t}\n\n\thunk->end = s->plain.len;\n\tif (hunk->end == hunk->start)\n\t\t/* The user aborted editing by deleting everything */\n\t\treturn 0;\n\n\trecolor_hunk(s, hunk);\n\n\t/*\n\t * If the hunk header is intact, parse it, otherwise simply use the\n\t * hunk header prior to editing (which will adjust `hunk->start` to\n\t * skip the hunk header).\n\t */\n\tif (s->plain.buf[hunk->start] == '@' &&\n\t    parse_hunk_header(s, hunk) < 0)\n\t\treturn error(_(\"could not parse hunk header\"));\n\n\treturn 1;\n}\n\nstatic ssize_t recount_edited_hunk(struct add_p_state *s, struct hunk *hunk,\n\t\t\t\t   size_t orig_old_count, size_t orig_new_count)\n{\n\tstruct hunk_header *header = &hunk->header;\n\tsize_t i;\n\n\theader->old_count = header->new_count = 0;\n\tfor (i = hunk->start; i < hunk->end; ) {\n\t\tswitch(normalize_marker(&s->plain.buf[i])) {\n\t\tcase '-':\n\t\t\theader->old_count++;\n\t\t\tbreak;\n\t\tcase '+':\n\t\t\theader->new_count++;\n\t\t\tbreak;\n\t\tcase ' ':\n\t\t\theader->old_count++;\n\t\t\theader->new_count++;\n\t\t\tbreak;\n\t\t}\n\n\t\ti = find_next_line(&s->plain, i);\n\t}\n\n\treturn orig_old_count - orig_new_count\n\t\t- header->old_count + header->new_count;\n}\n\nstatic int run_apply_check(struct add_p_state *s,\n\t\t\t   struct file_diff *file_diff)\n{\n\tstruct child_process cp = CHILD_PROCESS_INIT;\n\n\tstrbuf_reset(&s->buf);\n\treassemble_patch(s, file_diff, 1, &s->buf);\n\n\tsetup_child_process(s, &cp,\n\t\t\t    \"apply\", \"--check\", NULL);\n\tstrvec_pushv(&cp.args, s->mode->apply_check_args);\n\tif (pipe_command(&cp, s->buf.buf, s->buf.len, NULL, 0, NULL, 0))\n\t\treturn error(_(\"'git apply --cached' failed\"));\n\n\treturn 0;\n}\n\nstatic int read_single_character(struct add_p_state *s)\n{\n\tif (s->s.use_single_key) {\n\t\tint res = read_key_without_echo(&s->answer);\n\t\tprintf(\"%s\\n\", res == EOF ? \"\" : s->answer.buf);\n\t\treturn res;\n\t}\n\n\tif (git_read_line_interactively(&s->answer) == EOF)\n\t\treturn EOF;\n\treturn 0;\n}\n\nstatic int prompt_yesno(struct add_p_state *s, const char *prompt)\n{\n\tfor (;;) {\n\t\tcolor_fprintf(stdout, s->s.prompt_color, \"%s\", _(prompt));\n\t\tfflush(stdout);\n\t\tif (read_single_character(s) == EOF)\n\t\t\treturn -1;\n\t\t/* do not limit to 1-byte input to allow 'no' etc. */\n\t\tswitch (tolower(s->answer.buf[0])) {\n\t\tcase 'n': return 0;\n\t\tcase 'y': return 1;\n\t\t}\n\t}\n}\n\nstatic int edit_hunk_loop(struct add_p_state *s,\n\t\t\t  struct file_diff *file_diff, struct hunk *hunk)\n{\n\tsize_t plain_len = s->plain.len, colored_len = s->colored.len;\n\tstruct hunk backup;\n\n\tbackup = *hunk;\n\n\tfor (;;) {\n\t\tint res = edit_hunk_manually(s, hunk);\n\t\tif (res == 0) {\n\t\t\t/* abandoned */\n\t\t\t*hunk = backup;\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (res > 0) {\n\t\t\thunk->delta +=\n\t\t\t\trecount_edited_hunk(s, hunk,\n\t\t\t\t\t\t    backup.header.old_count,\n\t\t\t\t\t\t    backup.header.new_count);\n\t\t\tif (!run_apply_check(s, file_diff))\n\t\t\t\treturn 0;\n\t\t}\n\n\t\t/* Drop edits (they were appended to s->plain) */\n\t\tstrbuf_setlen(&s->plain, plain_len);\n\t\tstrbuf_setlen(&s->colored, colored_len);\n\t\t*hunk = backup;\n\n\t\t/*\n\t\t * TRANSLATORS: do not translate [y/n]\n\t\t * The program will only accept that input at this point.\n\t\t * Consider translating (saying \"no\" discards!) as\n\t\t * (saying \"n\" for \"no\" discards!) if the translation\n\t\t * of the word \"no\" does not start with n.\n\t\t */\n\t\tres = prompt_yesno(s, _(\"Your edited hunk does not apply. \"\n\t\t\t\t\t\"Edit again (saying \\\"no\\\" discards!) \"\n\t\t\t\t\t\"[y/n]? \"));\n\t\tif (res < 1)\n\t\t\treturn -1;\n\t}\n}\n\nstatic int apply_for_checkout(struct add_p_state *s, struct strbuf *diff,\n\t\t\t      int is_reverse)\n{\n\tconst char *reverse = is_reverse ? \"-R\" : NULL;\n\tstruct child_process check_index = CHILD_PROCESS_INIT;\n\tstruct child_process check_worktree = CHILD_PROCESS_INIT;\n\tstruct child_process apply_index = CHILD_PROCESS_INIT;\n\tstruct child_process apply_worktree = CHILD_PROCESS_INIT;\n\tint applies_index, applies_worktree;\n\n\tsetup_child_process(s, &check_index,\n\t\t\t    \"apply\", \"--cached\", \"--check\", reverse, NULL);\n\tapplies_index = !pipe_command(&check_index, diff->buf, diff->len,\n\t\t\t\t      NULL, 0, NULL, 0);\n\n\tsetup_child_process(s, &check_worktree,\n\t\t\t    \"apply\", \"--check\", reverse, NULL);\n\tapplies_worktree = !pipe_command(&check_worktree, diff->buf, diff->len,\n\t\t\t\t\t NULL, 0, NULL, 0);\n\n\tif (applies_worktree && applies_index) {\n\t\tsetup_child_process(s, &apply_index,\n\t\t\t\t    \"apply\", \"--cached\", reverse, NULL);\n\t\tpipe_command(&apply_index, diff->buf, diff->len,\n\t\t\t     NULL, 0, NULL, 0);\n\n\t\tsetup_child_process(s, &apply_worktree,\n\t\t\t\t    \"apply\", reverse, NULL);\n\t\tpipe_command(&apply_worktree, diff->buf, diff->len,\n\t\t\t     NULL, 0, NULL, 0);\n\n\t\treturn 1;\n\t}\n\n\tif (!applies_index) {\n\t\terr(s, _(\"The selected hunks do not apply to the index!\"));\n\t\tif (prompt_yesno(s, _(\"Apply them to the worktree \"\n\t\t\t\t\t  \"anyway? \")) > 0) {\n\t\t\tsetup_child_process(s, &apply_worktree,\n\t\t\t\t\t    \"apply\", reverse, NULL);\n\t\t\treturn pipe_command(&apply_worktree, diff->buf,\n\t\t\t\t\t    diff->len, NULL, 0, NULL, 0);\n\t\t}\n\t\terr(s, _(\"Nothing was applied.\\n\"));\n\t} else\n\t\t/* As a last resort, show the diff to the user */\n\t\tfwrite(diff->buf, diff->len, 1, stdout);\n\n\treturn 0;\n}\n\n#define SUMMARY_HEADER_WIDTH 20\n#define SUMMARY_LINE_WIDTH 80\nstatic void summarize_hunk(struct add_p_state *s, struct hunk *hunk,\n\t\t\t   struct strbuf *out)\n{\n\tstruct hunk_header *header = &hunk->header;\n\tstruct strbuf *plain = &s->plain;\n\tsize_t len = out->len, i;\n\n\tstrbuf_addf(out, \" -%lu,%lu +%lu,%lu \",\n\t\t    header->old_offset, header->old_count,\n\t\t    header->new_offset, header->new_count);\n\tif (out->len - len < SUMMARY_HEADER_WIDTH)\n\t\tstrbuf_addchars(out, ' ',\n\t\t\t\tSUMMARY_HEADER_WIDTH + len - out->len);\n\tfor (i = hunk->start; i < hunk->end; i = find_next_line(plain, i))\n\t\tif (plain->buf[i] != ' ')\n\t\t\tbreak;\n\tif (i < hunk->end)\n\t\tstrbuf_add(out, plain->buf + i, find_next_line(plain, i) - i);\n\tif (out->len - len > SUMMARY_LINE_WIDTH)\n\t\tstrbuf_setlen(out, len + SUMMARY_LINE_WIDTH);\n\tstrbuf_complete_line(out);\n}\n\n#define DISPLAY_HUNKS_LINES 20\nstatic size_t display_hunks(struct add_p_state *s,\n\t\t\t    struct file_diff *file_diff, size_t start_index)\n{\n\tsize_t end_index = start_index + DISPLAY_HUNKS_LINES;\n\n\tif (end_index > file_diff->hunk_nr)\n\t\tend_index = file_diff->hunk_nr;\n\n\twhile (start_index < end_index) {\n\t\tstruct hunk *hunk = file_diff->hunk + start_index++;\n\n\t\tstrbuf_reset(&s->buf);\n\t\tstrbuf_addf(&s->buf, \"%c%2d: \", hunk->use == USE_HUNK ? '+'\n\t\t\t    : hunk->use == SKIP_HUNK ? '-' : ' ',\n\t\t\t    (int)start_index);\n\t\tsummarize_hunk(s, hunk, &s->buf);\n\t\tfputs(s->buf.buf, stdout);\n\t}\n\n\treturn end_index;\n}\n\nstatic const char help_patch_remainder[] =\nN_(\"j - leave this hunk undecided, see next undecided hunk\\n\"\n   \"J - leave this hunk undecided, see next hunk\\n\"\n   \"k - leave this hunk undecided, see previous undecided hunk\\n\"\n   \"K - leave this hunk undecided, see previous hunk\\n\"\n   \"g - select a hunk to go to\\n\"\n   \"/ - search for a hunk matching the given regex\\n\"\n   \"s - split the current hunk into smaller hunks\\n\"\n   \"e - manually edit the current hunk\\n\"\n   \"p - print the current hunk, 'P' to use the pager\\n\"\n   \"? - print help\\n\");\n\nstatic int patch_update_file(struct add_p_state *s,\n\t\t\t     struct file_diff *file_diff)\n{\n\tsize_t hunk_index = 0;\n\tssize_t i, undecided_previous, undecided_next, rendered_hunk_index = -1;\n\tstruct hunk *hunk;\n\tchar ch;\n\tstruct child_process cp = CHILD_PROCESS_INIT;\n\tint colored = !!s->colored.len, quit = 0, use_pager = 0;\n\tenum prompt_mode_type prompt_mode_type;\n\tenum {\n\t\tALLOW_GOTO_PREVIOUS_HUNK = 1 << 0,\n\t\tALLOW_GOTO_PREVIOUS_UNDECIDED_HUNK = 1 << 1,\n\t\tALLOW_GOTO_NEXT_HUNK = 1 << 2,\n\t\tALLOW_GOTO_NEXT_UNDECIDED_HUNK = 1 << 3,\n\t\tALLOW_SEARCH_AND_GOTO = 1 << 4,\n\t\tALLOW_SPLIT = 1 << 5,\n\t\tALLOW_EDIT = 1 << 6\n\t} permitted = 0;\n\n\t/* Empty added files have no hunks */\n\tif (!file_diff->hunk_nr && !file_diff->added)\n\t\treturn 0;\n\n\tstrbuf_reset(&s->buf);\n\trender_diff_header(s, file_diff, colored, &s->buf);\n\tfputs(s->buf.buf, stdout);\n\tfor (;;) {\n\t\tif (hunk_index >= file_diff->hunk_nr)\n\t\t\thunk_index = 0;\n\t\thunk = file_diff->hunk_nr\n\t\t\t\t? file_diff->hunk + hunk_index\n\t\t\t\t: &file_diff->head;\n\t\tundecided_previous = -1;\n\t\tundecided_next = -1;\n\n\t\tif (file_diff->hunk_nr) {\n\t\t\tfor (i = hunk_index - 1; i >= 0; i--)\n\t\t\t\tif (file_diff->hunk[i].use == UNDECIDED_HUNK) {\n\t\t\t\t\tundecided_previous = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\tfor (i = hunk_index + 1; i < file_diff->hunk_nr; i++)\n\t\t\t\tif (file_diff->hunk[i].use == UNDECIDED_HUNK) {\n\t\t\t\t\tundecided_next = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t}\n\n\t\t/* Everything decided? */\n\t\tif (undecided_previous < 0 && undecided_next < 0 &&\n\t\t    hunk->use != UNDECIDED_HUNK)\n\t\t\tbreak;\n\n\t\tstrbuf_reset(&s->buf);\n\t\tif (file_diff->hunk_nr) {\n\t\t\tif (rendered_hunk_index != hunk_index) {\n\t\t\t\tif (use_pager) {\n\t\t\t\t\tsetup_pager(the_repository);\n\t\t\t\t\tsigchain_push(SIGPIPE, SIG_IGN);\n\t\t\t\t}\n\t\t\t\trender_hunk(s, hunk, 0, colored, &s->buf);\n\t\t\t\tfputs(s->buf.buf, stdout);\n\t\t\t\trendered_hunk_index = hunk_index;\n\t\t\t\tif (use_pager) {\n\t\t\t\t\tsigchain_pop(SIGPIPE);\n\t\t\t\t\twait_for_pager();\n\t\t\t\t\tuse_pager = 0;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstrbuf_reset(&s->buf);\n\t\t\tif (undecided_previous >= 0) {\n\t\t\t\tpermitted |= ALLOW_GOTO_PREVIOUS_UNDECIDED_HUNK;\n\t\t\t\tstrbuf_addstr(&s->buf, \",k\");\n\t\t\t}\n\t\t\tif (hunk_index) {\n\t\t\t\tpermitted |= ALLOW_GOTO_PREVIOUS_HUNK;\n\t\t\t\tstrbuf_addstr(&s->buf, \",K\");\n\t\t\t}\n\t\t\tif (undecided_next >= 0) {\n\t\t\t\tpermitted |= ALLOW_GOTO_NEXT_UNDECIDED_HUNK;\n\t\t\t\tstrbuf_addstr(&s->buf, \",j\");\n\t\t\t}\n\t\t\tif (hunk_index + 1 < file_diff->hunk_nr) {\n\t\t\t\tpermitted |= ALLOW_GOTO_NEXT_HUNK;\n\t\t\t\tstrbuf_addstr(&s->buf, \",J\");\n\t\t\t}\n\t\t\tif (file_diff->hunk_nr > 1) {\n\t\t\t\tpermitted |= ALLOW_SEARCH_AND_GOTO;\n\t\t\t\tstrbuf_addstr(&s->buf, \",g,/\");\n\t\t\t}\n\t\t\tif (hunk->splittable_into > 1) {\n\t\t\t\tpermitted |= ALLOW_SPLIT;\n\t\t\t\tstrbuf_addstr(&s->buf, \",s\");\n\t\t\t}\n\t\t\tif (hunk_index + 1 > file_diff->mode_change &&\n\t\t\t    !file_diff->deleted) {\n\t\t\t\tpermitted |= ALLOW_EDIT;\n\t\t\t\tstrbuf_addstr(&s->buf, \",e\");\n\t\t\t}\n\t\t\tstrbuf_addstr(&s->buf, \",p\");\n\t\t}\n\t\tif (file_diff->deleted)\n\t\t\tprompt_mode_type = PROMPT_DELETION;\n\t\telse if (file_diff->added)\n\t\t\tprompt_mode_type = PROMPT_ADDITION;\n\t\telse if (file_diff->mode_change && !hunk_index)\n\t\t\tprompt_mode_type = PROMPT_MODE_CHANGE;\n\t\telse\n\t\t\tprompt_mode_type = PROMPT_HUNK;\n\n\t\tprintf(\"%s(%\"PRIuMAX\"/%\"PRIuMAX\") \", s->s.prompt_color,\n\t\t\t      (uintmax_t)hunk_index + 1,\n\t\t\t      (uintmax_t)(file_diff->hunk_nr\n\t\t\t\t\t\t? file_diff->hunk_nr\n\t\t\t\t\t\t: 1));\n\t\tprintf(_(s->mode->prompt_mode[prompt_mode_type]),\n\t\t       s->buf.buf);\n\t\tif (*s->s.reset_color)\n\t\t\tfputs(s->s.reset_color, stdout);\n\t\tfflush(stdout);\n\t\tif (read_single_character(s) == EOF)\n\t\t\tbreak;\n\n\t\tif (!s->answer.len)\n\t\t\tcontinue;\n\t\tch = tolower(s->answer.buf[0]);\n\n\t\t/* 'g' takes a hunk number and '/' takes a regexp */\n\t\tif (s->answer.len != 1 && (ch != 'g' && ch != '/')) {\n\t\t\terr(s, _(\"Only one letter is expected, got '%s'\"), s->answer.buf);\n\t\t\tcontinue;\n\t\t}\n\t\tif (ch == 'y') {\n\t\t\thunk->use = USE_HUNK;\nsoft_increment:\n\t\t\thunk_index = undecided_next < 0 ?\n\t\t\t\tfile_diff->hunk_nr : undecided_next;\n\t\t} else if (ch == 'n') {\n\t\t\thunk->use = SKIP_HUNK;\n\t\t\tgoto soft_increment;\n\t\t} else if (ch == 'a') {\n\t\t\tif (file_diff->hunk_nr) {\n\t\t\t\tfor (; hunk_index < file_diff->hunk_nr; hunk_index++) {\n\t\t\t\t\thunk = file_diff->hunk + hunk_index;\n\t\t\t\t\tif (hunk->use == UNDECIDED_HUNK)\n\t\t\t\t\t\thunk->use = USE_HUNK;\n\t\t\t\t}\n\t\t\t} else if (hunk->use == UNDECIDED_HUNK) {\n\t\t\t\thunk->use = USE_HUNK;\n\t\t\t}\n\t\t} else if (ch == 'd' || ch == 'q') {\n\t\t\tif (file_diff->hunk_nr) {\n\t\t\t\tfor (; hunk_index < file_diff->hunk_nr; hunk_index++) {\n\t\t\t\t\thunk = file_diff->hunk + hunk_index;\n\t\t\t\t\tif (hunk->use == UNDECIDED_HUNK)\n\t\t\t\t\t\thunk->use = SKIP_HUNK;\n\t\t\t\t}\n\t\t\t} else if (hunk->use == UNDECIDED_HUNK) {\n\t\t\t\thunk->use = SKIP_HUNK;\n\t\t\t}\n\t\t\tif (ch == 'q') {\n\t\t\t\tquit = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else if (s->answer.buf[0] == 'K') {\n\t\t\tif (permitted & ALLOW_GOTO_PREVIOUS_HUNK)\n\t\t\t\thunk_index--;\n\t\t\telse\n\t\t\t\terr(s, _(\"No previous hunk\"));\n\t\t} else if (s->answer.buf[0] == 'J') {\n\t\t\tif (permitted & ALLOW_GOTO_NEXT_HUNK)\n\t\t\t\thunk_index++;\n\t\t\telse\n\t\t\t\terr(s, _(\"No next hunk\"));\n\t\t} else if (s->answer.buf[0] == 'k') {\n\t\t\tif (permitted & ALLOW_GOTO_PREVIOUS_UNDECIDED_HUNK)\n\t\t\t\thunk_index = undecided_previous;\n\t\t\telse\n\t\t\t\terr(s, _(\"No previous hunk\"));\n\t\t} else if (s->answer.buf[0] == 'j') {\n\t\t\tif (permitted & ALLOW_GOTO_NEXT_UNDECIDED_HUNK)\n\t\t\t\thunk_index = undecided_next;\n\t\t\telse\n\t\t\t\terr(s, _(\"No next hunk\"));\n\t\t} else if (s->answer.buf[0] == 'g') {\n\t\t\tchar *pend;\n\t\t\tunsigned long response;\n\n\t\t\tif (!(permitted & ALLOW_SEARCH_AND_GOTO)) {\n\t\t\t\terr(s, _(\"No other hunks to goto\"));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tstrbuf_remove(&s->answer, 0, 1);\n\t\t\tstrbuf_trim(&s->answer);\n\t\t\ti = hunk_index - DISPLAY_HUNKS_LINES / 2;\n\t\t\tif (i < (int)file_diff->mode_change)\n\t\t\t\ti = file_diff->mode_change;\n\t\t\twhile (s->answer.len == 0) {\n\t\t\t\ti = display_hunks(s, file_diff, i);\n\t\t\t\tprintf(\"%s\", i < file_diff->hunk_nr ?\n\t\t\t\t       _(\"go to which hunk (<ret> to see \"\n\t\t\t\t\t \"more)? \") : _(\"go to which hunk? \"));\n\t\t\t\tfflush(stdout);\n\t\t\t\tif (strbuf_getline(&s->answer,\n\t\t\t\t\t\t   stdin) == EOF)\n\t\t\t\t\tbreak;\n\t\t\t\tstrbuf_trim_trailing_newline(&s->answer);\n\t\t\t}\n\n\t\t\tstrbuf_trim(&s->answer);\n\t\t\tresponse = strtoul(s->answer.buf, &pend, 10);\n\t\t\tif (*pend || pend == s->answer.buf)\n\t\t\t\terr(s, _(\"Invalid number: '%s'\"),\n\t\t\t\t    s->answer.buf);\n\t\t\telse if (0 < response && response <= file_diff->hunk_nr)\n\t\t\t\thunk_index = response - 1;\n\t\t\telse\n\t\t\t\terr(s, Q_(\"Sorry, only %d hunk available.\",\n\t\t\t\t\t  \"Sorry, only %d hunks available.\",\n\t\t\t\t\t  file_diff->hunk_nr),\n\t\t\t\t    (int)file_diff->hunk_nr);\n\t\t} else if (s->answer.buf[0] == '/') {\n\t\t\tregex_t regex;\n\t\t\tint ret;\n\n\t\t\tif (!(permitted & ALLOW_SEARCH_AND_GOTO)) {\n\t\t\t\terr(s, _(\"No other hunks to search\"));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tstrbuf_remove(&s->answer, 0, 1);\n\t\t\tstrbuf_trim_trailing_newline(&s->answer);\n\t\t\tif (s->answer.len == 0) {\n\t\t\t\tprintf(\"%s\", _(\"search for regex? \"));\n\t\t\t\tfflush(stdout);\n\t\t\t\tif (strbuf_getline(&s->answer,\n\t\t\t\t\t\t   stdin) == EOF)\n\t\t\t\t\tbreak;\n\t\t\t\tstrbuf_trim_trailing_newline(&s->answer);\n\t\t\t\tif (s->answer.len == 0)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tret = regcomp(&regex, s->answer.buf,\n\t\t\t\t      REG_EXTENDED | REG_NOSUB | REG_NEWLINE);\n\t\t\tif (ret) {\n\t\t\t\tchar errbuf[1024];\n\n\t\t\t\tregerror(ret, &regex, errbuf, sizeof(errbuf));\n\t\t\t\terr(s, _(\"Malformed search regexp %s: %s\"),\n\t\t\t\t    s->answer.buf, errbuf);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ti = hunk_index;\n\t\t\tfor (;;) {\n\t\t\t\t/* render the hunk into a scratch buffer */\n\t\t\t\trender_hunk(s, file_diff->hunk + i, 0, 0,\n\t\t\t\t\t    &s->buf);\n\t\t\t\tif (regexec(&regex, s->buf.buf, 0, NULL, 0)\n\t\t\t\t    != REG_NOMATCH)\n\t\t\t\t\tbreak;\n\t\t\t\ti++;\n\t\t\t\tif (i == file_diff->hunk_nr)\n\t\t\t\t\ti = 0;\n\t\t\t\tif (i != hunk_index)\n\t\t\t\t\tcontinue;\n\t\t\t\terr(s, _(\"No hunk matches the given pattern\"));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tregfree(&regex);\n\t\t\thunk_index = i;\n\t\t} else if (s->answer.buf[0] == 's') {\n\t\t\tsize_t splittable_into = hunk->splittable_into;\n\t\t\tif (!(permitted & ALLOW_SPLIT)) {\n\t\t\t\terr(s, _(\"Sorry, cannot split this hunk\"));\n\t\t\t} else if (!split_hunk(s, file_diff,\n\t\t\t\t\t     hunk - file_diff->hunk)) {\n\t\t\t\tcolor_fprintf_ln(stdout, s->s.header_color,\n\t\t\t\t\t\t _(\"Split into %d hunks.\"),\n\t\t\t\t\t\t (int)splittable_into);\n\t\t\t\trendered_hunk_index = -1;\n\t\t\t}\n\t\t} else if (s->answer.buf[0] == 'e') {\n\t\t\tif (!(permitted & ALLOW_EDIT))\n\t\t\t\terr(s, _(\"Sorry, cannot edit this hunk\"));\n\t\t\telse if (edit_hunk_loop(s, file_diff, hunk) >= 0) {\n\t\t\t\thunk->use = USE_HUNK;\n\t\t\t\tgoto soft_increment;\n\t\t\t}\n\t\t} else if (ch == 'p') {\n\t\t\trendered_hunk_index = -1;\n\t\t\tuse_pager = (s->answer.buf[0] == 'P') ? 1 : 0;\n\t\t} else if (s->answer.buf[0] == '?') {\n\t\t\tconst char *p = _(help_patch_remainder), *eol = p;\n\n\t\t\tcolor_fprintf(stdout, s->s.help_color, \"%s\",\n\t\t\t\t      _(s->mode->help_patch_text));\n\n\t\t\t/*\n\t\t\t * Show only those lines of the remainder that are\n\t\t\t * actually applicable with the current hunk.\n\t\t\t */\n\t\t\tfor (; *p; p = eol + (*eol == '\\n')) {\n\t\t\t\teol = strchrnul(p, '\\n');\n\n\t\t\t\t/*\n\t\t\t\t * `s->buf` still contains the part of the\n\t\t\t\t * commands shown in the prompt that are not\n\t\t\t\t * always available.\n\t\t\t\t */\n\t\t\t\tif (*p != '?' && !strchr(s->buf.buf, *p))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tcolor_fprintf_ln(stdout, s->s.help_color,\n\t\t\t\t\t\t \"%.*s\", (int)(eol - p), p);\n\t\t\t}\n\t\t} else {\n\t\t\terr(s, _(\"Unknown command '%s' (use '?' for help)\"),\n\t\t\t    s->answer.buf);\n\t\t}\n\t}\n\n\t/* Any hunk to be used? */\n\tfor (i = 0; i < file_diff->hunk_nr; i++)\n\t\tif (file_diff->hunk[i].use == USE_HUNK)\n\t\t\tbreak;\n\n\tif (i < file_diff->hunk_nr ||\n\t    (!file_diff->hunk_nr && file_diff->head.use == USE_HUNK)) {\n\t\t/* At least one hunk selected: apply */\n\t\tstrbuf_reset(&s->buf);\n\t\treassemble_patch(s, file_diff, 0, &s->buf);\n\n\t\tdiscard_index(s->s.r->index);\n\t\tif (s->mode->apply_for_checkout)\n\t\t\tapply_for_checkout(s, &s->buf,\n\t\t\t\t\t   s->mode->is_reverse);\n\t\telse {\n\t\t\tsetup_child_process(s, &cp, \"apply\", NULL);\n\t\t\tstrvec_pushv(&cp.args, s->mode->apply_args);\n\t\t\tif (pipe_command(&cp, s->buf.buf, s->buf.len,\n\t\t\t\t\t NULL, 0, NULL, 0))\n\t\t\t\terror(_(\"'git apply' failed\"));\n\t\t}\n\t\tif (repo_read_index(s->s.r) >= 0)\n\t\t\trepo_refresh_and_write_index(s->s.r, REFRESH_QUIET, 0,\n\t\t\t\t\t\t     1, NULL, NULL, NULL);\n\t}\n\n\tputchar('\\n');\n\treturn quit;\n}\n\nint run_add_p(struct repository *r, enum add_p_mode mode,\n\t      const char *revision, const struct pathspec *ps)\n{\n\tstruct add_p_state s = {\n\t\t{ r }, STRBUF_INIT, STRBUF_INIT, STRBUF_INIT, STRBUF_INIT\n\t};\n\tsize_t i, binary_count = 0;\n\n\tinit_add_i_state(&s.s, r);\n\n\tif (mode == ADD_P_STASH)\n\t\ts.mode = &patch_mode_stash;\n\telse if (mode == ADD_P_RESET) {\n\t\tif (!revision || !strcmp(revision, \"HEAD\"))\n\t\t\ts.mode = &patch_mode_reset_head;\n\t\telse\n\t\t\ts.mode = &patch_mode_reset_nothead;\n\t} else if (mode == ADD_P_CHECKOUT) {\n\t\tif (!revision)\n\t\t\ts.mode = &patch_mode_checkout_index;\n\t\telse if (!strcmp(revision, \"HEAD\"))\n\t\t\ts.mode = &patch_mode_checkout_head;\n\t\telse\n\t\t\ts.mode = &patch_mode_checkout_nothead;\n\t} else if (mode == ADD_P_WORKTREE) {\n\t\tif (!revision)\n\t\t\ts.mode = &patch_mode_checkout_index;\n\t\telse if (!strcmp(revision, \"HEAD\"))\n\t\t\ts.mode = &patch_mode_worktree_head;\n\t\telse\n\t\t\ts.mode = &patch_mode_worktree_nothead;\n\t} else\n\t\ts.mode = &patch_mode_add;\n\ts.revision = revision;\n\n\tdiscard_index(r->index);\n\tif (repo_read_index(r) < 0 ||\n\t    (!s.mode->index_only &&\n\t     repo_refresh_and_write_index(r, REFRESH_QUIET, 0, 1,\n\t\t\t\t\t  NULL, NULL, NULL) < 0) ||\n\t    parse_diff(&s, ps) < 0) {\n\t\tadd_p_state_clear(&s);\n\t\treturn -1;\n\t}\n\n\tfor (i = 0; i < s.file_diff_nr; i++)\n\t\tif (s.file_diff[i].binary && !s.file_diff[i].hunk_nr)\n\t\t\tbinary_count++;\n\t\telse if (patch_update_file(&s, s.file_diff + i))\n\t\t\tbreak;\n\n\tif (s.file_diff_nr == 0)\n\t\terr(&s, _(\"No changes.\"));\n\telse if (binary_count == s.file_diff_nr)\n\t\terr(&s, _(\"Only binary files changed.\"));\n\n\tadd_p_state_clear(&s);\n\treturn 0;\n}\n",
        "imports": [
            "pathspec.h",
            "environment.h",
            "repository.h",
            "prompt.h",
            "sigchain.h",
            "color.h",
            "editor.h",
            "pager.h",
            "strvec.h",
            "advice.h",
            "strbuf.h",
            "gettext.h"
        ],
        "functions": [
            "if",
            "parse_diff",
            "parse_hunk_header",
            "add_p_state_clear",
            "is_octal",
            "normalize_marker",
            "find_next_line",
            "recolor_hunk",
            "read_single_character",
            "complete_file",
            "err",
            "prompt_yesno",
            "edit_hunk_manually"
        ],
        "variables": [
            "file_diff_alloc",
            "end",
            "algorithm",
            "mode",
            "UNDECIDED_HUNK",
            "mode_change",
            "res",
            "diff_algorithm",
            "use",
            "colored_cp",
            "use_shell",
            "len",
            "eol",
            "new_offset",
            "merged",
            "ch",
            "diff_filter",
            "response",
            "backup",
            "old_count",
            "git_cmd",
            "header",
            "start",
            "i",
            "delta",
            "context_line_count",
            "current",
            "check_index",
            "plain",
            "marker",
            "args",
            "save_len",
            "file_diff",
            "pend",
            "colored",
            "colored_start",
            "rendered_hunk_index",
            "plain_len",
            "overlap_end",
            "filter_cp",
            "binary",
            "colored_extra_start",
            "hunk_index",
            "colored_current",
            "undecided_previous",
            "offset",
            "permitted",
            "color_arg_index",
            "binary_count",
            "apply_worktree",
            "colored_end",
            "first",
            "quit",
            "splittable_into",
            "remaining",
            "revision",
            "j",
            "colored_extra_end",
            "overlap_start",
            "extra_end",
            "skip_mode_change",
            "added",
            "p",
            "check_worktree",
            "head",
            "end_index",
            "extra_start",
            "hunk",
            "undecided_next",
            "new_count",
            "next",
            "colored_p",
            "cp",
            "count",
            "deleted",
            "line",
            "reverse",
            "prompt_mode_type",
            "use_pager",
            "apply_index",
            "old_offset",
            "suppress_colored_line_range",
            "colored_pend"
        ]
    },
    {
        "file_name": "advice.c",
        "language": "cpp",
        "source_code": "#include \"git-compat-util.h\"\n#include \"advice.h\"\n#include \"config.h\"\n#include \"color.h\"\n#include \"environment.h\"\n#include \"gettext.h\"\n#include \"help.h\"\n#include \"string-list.h\"\n\nstatic int advice_use_color = -1;\nstatic char advice_colors[][COLOR_MAXLEN] = {\n\tGIT_COLOR_RESET,\n\tGIT_COLOR_YELLOW,\t/* HINT */\n};\n\nenum color_advice {\n\tADVICE_COLOR_RESET = 0,\n\tADVICE_COLOR_HINT = 1,\n};\n\nstatic int parse_advise_color_slot(const char *slot)\n{\n\tif (!strcasecmp(slot, \"reset\"))\n\t\treturn ADVICE_COLOR_RESET;\n\tif (!strcasecmp(slot, \"hint\"))\n\t\treturn ADVICE_COLOR_HINT;\n\treturn -1;\n}\n\nstatic const char *advise_get_color(enum color_advice ix)\n{\n\tif (want_color_stderr(advice_use_color))\n\t\treturn advice_colors[ix];\n\treturn \"\";\n}\n\nenum advice_level {\n\tADVICE_LEVEL_NONE = 0,\n\tADVICE_LEVEL_DISABLED,\n\tADVICE_LEVEL_ENABLED,\n};\n\nstatic struct {\n\tconst char *key;\n\tenum advice_level level;\n} advice_setting[] = {\n\t[ADVICE_ADD_EMBEDDED_REPO]\t\t\t= { \"addEmbeddedRepo\" },\n\t[ADVICE_ADD_EMPTY_PATHSPEC]\t\t\t= { \"addEmptyPathspec\" },\n\t[ADVICE_ADD_IGNORED_FILE]\t\t\t= { \"addIgnoredFile\" },\n\t[ADVICE_AMBIGUOUS_FETCH_REFSPEC]\t\t= { \"ambiguousFetchRefspec\" },\n\t[ADVICE_AM_WORK_DIR] \t\t\t\t= { \"amWorkDir\" },\n\t[ADVICE_CHECKOUT_AMBIGUOUS_REMOTE_BRANCH_NAME] \t= { \"checkoutAmbiguousRemoteBranchName\" },\n\t[ADVICE_COMMIT_BEFORE_MERGE]\t\t\t= { \"commitBeforeMerge\" },\n\t[ADVICE_DETACHED_HEAD]\t\t\t\t= { \"detachedHead\" },\n\t[ADVICE_DIVERGING]\t\t\t\t= { \"diverging\" },\n\t[ADVICE_FETCH_SET_HEAD_WARN]\t\t\t= { \"fetchRemoteHEADWarn\" },\n\t[ADVICE_FETCH_SHOW_FORCED_UPDATES]\t\t= { \"fetchShowForcedUpdates\" },\n\t[ADVICE_FORCE_DELETE_BRANCH]\t\t\t= { \"forceDeleteBranch\" },\n\t[ADVICE_GRAFT_FILE_DEPRECATED]\t\t\t= { \"graftFileDeprecated\" },\n\t[ADVICE_IGNORED_HOOK]\t\t\t\t= { \"ignoredHook\" },\n\t[ADVICE_IMPLICIT_IDENTITY]\t\t\t= { \"implicitIdentity\" },\n\t[ADVICE_MERGE_CONFLICT]\t\t\t\t= { \"mergeConflict\" },\n\t[ADVICE_NESTED_TAG]\t\t\t\t= { \"nestedTag\" },\n\t[ADVICE_OBJECT_NAME_WARNING]\t\t\t= { \"objectNameWarning\" },\n\t[ADVICE_PUSH_ALREADY_EXISTS]\t\t\t= { \"pushAlreadyExists\" },\n\t[ADVICE_PUSH_FETCH_FIRST]\t\t\t= { \"pushFetchFirst\" },\n\t[ADVICE_PUSH_NEEDS_FORCE]\t\t\t= { \"pushNeedsForce\" },\n\t[ADVICE_PUSH_NON_FF_CURRENT]\t\t\t= { \"pushNonFFCurrent\" },\n\t[ADVICE_PUSH_NON_FF_MATCHING]\t\t\t= { \"pushNonFFMatching\" },\n\t[ADVICE_PUSH_REF_NEEDS_UPDATE]\t\t\t= { \"pushRefNeedsUpdate\" },\n\t[ADVICE_PUSH_UNQUALIFIED_REF_NAME]\t\t= { \"pushUnqualifiedRefName\" },\n\t[ADVICE_PUSH_UPDATE_REJECTED]\t\t\t= { \"pushUpdateRejected\" },\n\t[ADVICE_PUSH_UPDATE_REJECTED_ALIAS]\t\t= { \"pushNonFastForward\" }, /* backwards compatibility */\n\t[ADVICE_REBASE_TODO_ERROR]\t\t\t= { \"rebaseTodoError\" },\n\t[ADVICE_REF_SYNTAX]\t\t\t\t= { \"refSyntax\" },\n\t[ADVICE_RESET_NO_REFRESH_WARNING]\t\t= { \"resetNoRefresh\" },\n\t[ADVICE_RESOLVE_CONFLICT]\t\t\t= { \"resolveConflict\" },\n\t[ADVICE_RM_HINTS]\t\t\t\t= { \"rmHints\" },\n\t[ADVICE_SEQUENCER_IN_USE]\t\t\t= { \"sequencerInUse\" },\n\t[ADVICE_SET_UPSTREAM_FAILURE]\t\t\t= { \"setUpstreamFailure\" },\n\t[ADVICE_SKIPPED_CHERRY_PICKS]\t\t\t= { \"skippedCherryPicks\" },\n\t[ADVICE_SPARSE_INDEX_EXPANDED]\t\t\t= { \"sparseIndexExpanded\" },\n\t[ADVICE_STATUS_AHEAD_BEHIND_WARNING]\t\t= { \"statusAheadBehindWarning\" },\n\t[ADVICE_STATUS_HINTS]\t\t\t\t= { \"statusHints\" },\n\t[ADVICE_STATUS_U_OPTION]\t\t\t= { \"statusUoption\" },\n\t[ADVICE_SUBMODULES_NOT_UPDATED] \t\t= { \"submodulesNotUpdated\" },\n\t[ADVICE_SUBMODULE_ALTERNATE_ERROR_STRATEGY_DIE] = { \"submoduleAlternateErrorStrategyDie\" },\n\t[ADVICE_SUBMODULE_MERGE_CONFLICT]               = { \"submoduleMergeConflict\" },\n\t[ADVICE_SUGGEST_DETACHING_HEAD]\t\t\t= { \"suggestDetachingHead\" },\n\t[ADVICE_UPDATE_SPARSE_PATH]\t\t\t= { \"updateSparsePath\" },\n\t[ADVICE_WAITING_FOR_EDITOR]\t\t\t= { \"waitingForEditor\" },\n\t[ADVICE_WORKTREE_ADD_ORPHAN]\t\t\t= { \"worktreeAddOrphan\" },\n};\n\nstatic const char turn_off_instructions[] =\nN_(\"\\n\"\n   \"Disable this message with \\\"git config set advice.%s false\\\"\");\n\nstatic void vadvise(const char *advice, int display_instructions,\n\t\t    const char *key, va_list params)\n{\n\tstruct strbuf buf = STRBUF_INIT;\n\tconst char *cp, *np;\n\n\tstrbuf_vaddf(&buf, advice, params);\n\n\tif (display_instructions)\n\t\tstrbuf_addf(&buf, turn_off_instructions, key);\n\n\tfor (cp = buf.buf; *cp; cp = np) {\n\t\tnp = strchrnul(cp, '\\n');\n\t\tfprintf(stderr,\t_(\"%shint:%s%.*s%s\\n\"),\n\t\t\tadvise_get_color(ADVICE_COLOR_HINT),\n\t\t\t(np == cp) ? \"\" : \" \",\n\t\t\t(int)(np - cp), cp,\n\t\t\tadvise_get_color(ADVICE_COLOR_RESET));\n\t\tif (*np)\n\t\t\tnp++;\n\t}\n\tstrbuf_release(&buf);\n}\n\nvoid advise(const char *advice, ...)\n{\n\tva_list params;\n\tva_start(params, advice);\n\tvadvise(advice, 0, \"\", params);\n\tva_end(params);\n}\n\nint advice_enabled(enum advice_type type)\n{\n\tint enabled = advice_setting[type].level != ADVICE_LEVEL_DISABLED;\n\tstatic int globally_enabled = -1;\n\n\tif (globally_enabled < 0)\n\t\tglobally_enabled = git_env_bool(GIT_ADVICE_ENVIRONMENT, 1);\n\tif (!globally_enabled)\n\t\treturn 0;\n\n\tif (type == ADVICE_PUSH_UPDATE_REJECTED)\n\t\treturn enabled &&\n\t\t       advice_enabled(ADVICE_PUSH_UPDATE_REJECTED_ALIAS);\n\n\treturn enabled;\n}\n\nvoid advise_if_enabled(enum advice_type type, const char *advice, ...)\n{\n\tva_list params;\n\n\tif (!advice_enabled(type))\n\t\treturn;\n\n\tva_start(params, advice);\n\tvadvise(advice, !advice_setting[type].level, advice_setting[type].key,\n\t\tparams);\n\tva_end(params);\n}\n\nint git_default_advice_config(const char *var, const char *value)\n{\n\tconst char *k, *slot_name;\n\n\tif (!strcmp(var, \"color.advice\")) {\n\t\tadvice_use_color = git_config_colorbool(var, value);\n\t\treturn 0;\n\t}\n\n\tif (skip_prefix(var, \"color.advice.\", &slot_name)) {\n\t\tint slot = parse_advise_color_slot(slot_name);\n\t\tif (slot < 0)\n\t\t\treturn 0;\n\t\tif (!value)\n\t\t\treturn config_error_nonbool(var);\n\t\treturn color_parse(value, advice_colors[slot]);\n\t}\n\n\tif (!skip_prefix(var, \"advice.\", &k))\n\t\treturn 0;\n\n\tfor (size_t i = 0; i < ARRAY_SIZE(advice_setting); i++) {\n\t\tif (strcasecmp(k, advice_setting[i].key))\n\t\t\tcontinue;\n\t\tadvice_setting[i].level = git_config_bool(var, value)\n\t\t\t\t\t  ? ADVICE_LEVEL_ENABLED\n\t\t\t\t\t  : ADVICE_LEVEL_DISABLED;\n\t\treturn 0;\n\t}\n\n\treturn 0;\n}\n\nvoid list_config_advices(struct string_list *list, const char *prefix)\n{\n\tfor (size_t i = 0; i < ARRAY_SIZE(advice_setting); i++)\n\t\tlist_config_item(list, prefix, advice_setting[i].key);\n}\n\nint error_resolve_conflict(const char *me)\n{\n\tif (!strcmp(me, \"cherry-pick\"))\n\t\terror(_(\"Cherry-picking is not possible because you have unmerged files.\"));\n\telse if (!strcmp(me, \"commit\"))\n\t\terror(_(\"Committing is not possible because you have unmerged files.\"));\n\telse if (!strcmp(me, \"merge\"))\n\t\terror(_(\"Merging is not possible because you have unmerged files.\"));\n\telse if (!strcmp(me, \"pull\"))\n\t\terror(_(\"Pulling is not possible because you have unmerged files.\"));\n\telse if (!strcmp(me, \"revert\"))\n\t\terror(_(\"Reverting is not possible because you have unmerged files.\"));\n\telse if (!strcmp(me, \"rebase\"))\n\t\terror(_(\"Rebasing is not possible because you have unmerged files.\"));\n\telse\n\t\tBUG(\"Unhandled conflict reason '%s'\", me);\n\n\tif (advice_enabled(ADVICE_RESOLVE_CONFLICT))\n\t\t/*\n\t\t * Message used both when 'git commit' fails and when\n\t\t * other commands doing a merge do.\n\t\t */\n\t\tadvise(_(\"Fix them up in the work tree, and then use 'git add/rm <file>'\\n\"\n\t\t\t \"as appropriate to mark resolution and make a commit.\"));\n\treturn -1;\n}\n\nvoid NORETURN die_resolve_conflict(const char *me)\n{\n\terror_resolve_conflict(me);\n\tdie(_(\"Exiting because of an unresolved conflict.\"));\n}\n\nvoid NORETURN die_conclude_merge(void)\n{\n\terror(_(\"You have not concluded your merge (MERGE_HEAD exists).\"));\n\tif (advice_enabled(ADVICE_RESOLVE_CONFLICT))\n\t\tadvise(_(\"Please, commit your changes before merging.\"));\n\tdie(_(\"Exiting because of unfinished merge.\"));\n}\n\nvoid NORETURN die_ff_impossible(void)\n{\n\tadvise_if_enabled(ADVICE_DIVERGING,\n\t\t_(\"Diverging branches can't be fast-forwarded, you need to either:\\n\"\n\t\t\"\\n\"\n\t\t\"\\tgit merge --no-ff\\n\"\n\t\t\"\\n\"\n\t\t\"or:\\n\"\n\t\t\"\\n\"\n\t\t\"\\tgit rebase\\n\"));\n\tdie(_(\"Not possible to fast-forward, aborting.\"));\n}\n\nvoid advise_on_updating_sparse_paths(struct string_list *pathspec_list)\n{\n\tstruct string_list_item *item;\n\n\tif (!pathspec_list->nr)\n\t\treturn;\n\n\tfprintf(stderr, _(\"The following paths and/or pathspecs matched paths that exist\\n\"\n\t\t\t  \"outside of your sparse-checkout definition, so will not be\\n\"\n\t\t\t  \"updated in the index:\\n\"));\n\tfor_each_string_list_item(item, pathspec_list)\n\t\tfprintf(stderr, \"%s\\n\", item->string);\n\n\tadvise_if_enabled(ADVICE_UPDATE_SPARSE_PATH,\n\t\t\t  _(\"If you intend to update such entries, try one of the following:\\n\"\n\t\t\t    \"* Use the --sparse option.\\n\"\n\t\t\t    \"* Disable or modify the sparsity rules.\"));\n}\n\nvoid detach_advice(const char *new_name)\n{\n\tconst char *fmt =\n\t_(\"Note: switching to '%s'.\\n\"\n\t\"\\n\"\n\t\"You are in 'detached HEAD' state. You can look around, make experimental\\n\"\n\t\"changes and commit them, and you can discard any commits you make in this\\n\"\n\t\"state without impacting any branches by switching back to a branch.\\n\"\n\t\"\\n\"\n\t\"If you want to create a new branch to retain commits you create, you may\\n\"\n\t\"do so (now or later) by using -c with the switch command. Example:\\n\"\n\t\"\\n\"\n\t\"  git switch -c <new-branch-name>\\n\"\n\t\"\\n\"\n\t\"Or undo this operation with:\\n\"\n\t\"\\n\"\n\t\"  git switch -\\n\"\n\t\"\\n\"\n\t\"Turn off this advice by setting config variable advice.detachedHead to false\\n\\n\");\n\n\tfprintf(stderr, fmt, new_name);\n}\n\nvoid advise_on_moving_dirty_path(struct string_list *pathspec_list)\n{\n\tstruct string_list_item *item;\n\n\tif (!pathspec_list->nr)\n\t\treturn;\n\n\tfprintf(stderr, _(\"The following paths have been moved outside the\\n\"\n\t\t\t  \"sparse-checkout definition but are not sparse due to local\\n\"\n\t\t\t  \"modifications.\\n\"));\n\tfor_each_string_list_item(item, pathspec_list)\n\t\tfprintf(stderr, \"%s\\n\", item->string);\n\n\tadvise_if_enabled(ADVICE_UPDATE_SPARSE_PATH,\n\t\t\t  _(\"To correct the sparsity of these paths, do the following:\\n\"\n\t\t\t    \"* Use \\\"git add --sparse <paths>\\\" to update the index\\n\"\n\t\t\t    \"* Use \\\"git sparse-checkout reapply\\\" to apply the sparsity rules\"));\n}\n",
        "imports": [
            "environment.h",
            "config.h",
            "color.h",
            "help.h",
            "advice.h",
            "gettext.h"
        ],
        "functions": [
            "advise_if_enabled",
            "advise_on_updating_sparse_paths",
            "die_resolve_conflict",
            "detach_advice",
            "die_ff_impossible",
            "parse_advise_color_slot",
            "list_config_advices",
            "advice_enabled",
            "git_default_advice_config",
            "die_conclude_merge",
            "advise",
            "advise_on_moving_dirty_path",
            "error_resolve_conflict"
        ],
        "variables": [
            "slot",
            "cp",
            "buf",
            "globally_enabled",
            "enabled",
            "advice_use_color",
            "np",
            "i"
        ]
    }
]